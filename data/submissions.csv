authors,date_downloaded,date_modified,date_published,date_submitted,image_url,incident_date,incident_id,language,mongodb_id,source_domain,submitters,text,title,url
"[""Ryan Francis""]",2020-10-30,2020-10-30,2017-05-03,2020-10-30,,2013-11-27,0,en,,www.csoonline.com,"[""Ingrid Dickinson (CSET)""]","""It is commonly referred to as information overload. An infosec professional throws out a wide net in hopes of stopping malware before it gets too deep into the network, but like a motion-sensor light, sometimes the alert catches a squirrel instead of a burglar.

Rob Kerr, chief technology officer at Haystax Technology, cited the 2013 breach at Target, as an example in which thieves stole some 40 million Target credit cards by accessing data on point of sale (POS) systems. Target later revised that number to include theft of private data for 70 million customers.

“There were many missteps before the breach happened, but a big one was that Target missed internal alerts -- only finding out about the breach when they were contacted by the Department of Justice,” he said.

Kerr said there were two different issues relating to the alert problem: While the attack was in progress, monitoring software (FireEye) alerted staff in Bangalore, India, who in turn notified Target staff in Minneapolis. No action was taken because these alerts were included with many other likely false alerts. Kerr recalls that it also appeared that at least some of the company's network infiltration alerting systems were turned off to reduce false positives.


A survey by FireEye polled C-level security executives at large enterprises worldwide and found that 37 percent of respondents receive more than 10,000 alerts each month. Of those alerts, 52 percent were false positives, and 64 percent were redundant alerts.

“This represents a huge burden on companies, as around 40 percent of them manually review each alert,” Kerr said.

In most enterprises, various monitoring and detection solutions are constantly combing through network and user activity data looking for anomalies that may indicate a malicious event is taking place. Each time the system gets a hit, an alert is generated that typically requires a human analyst to either verify it is a bona-fide threat, or clear as not applicable or too minor.





The problem this creates is analyst overload, Kerr notes. “In other words, the system is unable to provide sufficient context up front to filter out the anomaly before it generates an alert, so it falls to the analyst to do that manually. This is a big problem because there are thousands of pieces of data on network logins, printer activity and building access logs. So there will be an alert when Bob -- who typically works 9 to 5 every day -- reenters the office at 7:30 one evening and prints a large file on a Sunday, accessing a file server that is normally off-limits to him.” 






The Cisco 2017 Security Capabilities Benchmark Study found that, due to various constraints, organizations can investigate only 56 percent of the security alerts they receive on a given day. Half of the investigated alerts (28 percent) are deemed legitimate; less than half (46 percent) of legitimate alerts are remediated. In addition, 44 percent of security operations managers see more than 5000 security alerts per day.

Kerr added that the combination of filtering out minor activity and highlighting the highest-priority risks has the net effect of providing enough context to drastically diminish false positives and the burdens they place on overworked analyst teams.

One of the key takeaways from a recent Rapid7 report was that reducing alert fatigue should always be a goal, but there’s more to it. A better signal-to-noise ratio means responders and analysts are more likely to see meaningful trends. One trend is that attackers still heavily rely on user interaction. For instance, on Monday holidays, alerts dipped significantly, which Rapid7's analysts attributed to a lack of employees interacting with malicious emails, attachments, etc.

Rapid 7’s report also notes that if you design indicators based only on currently available information, rather than seeking out additional intelligence or adding industry- and company-specific context, the result will be low-quality alerts. In other words: while most alerts are triggered from known, malicious activity, the quality of these alerts is entirely dependent on the established indicators.


As the entire process shifts, this is how to address remote learning — quickly

Rebekah Brown, intelligence lead at Rapid7, said it is difficult to compile a list of common alerts that will be good for everyone, all the time. “Even some that seem like obvious choices--i.e., alerting on hashes associated with ransomware--may not be universally applicable. For example, if they are hashes that encrypt Windows-based systems, they wouldn’t be relevant in an organization that only uses Macs,” she said.

She added that a customer needs to identify what threats are relevant to them because of the systems they use, the data they have, and their threat profile. For example, a retail company that deals with both online and in-store sales would want to alert on:

Threats to their e-commerce platform, which can come from a threat feed that gathers data on brute force and other attacks against that specific platform
Threats to POS systems 
Threats reported by an industry-specific information sharing group, such as the R-CISC
General threats to Windows systems including ransomware, credential theft or malware 
Custom alerts based on things that they have seen in their environment before
Brown said there are three primary sources for detections: threat feeds (primarily from honeypots and network sensors), threat reports (primarily from IR investigations or research initiatives), and internal detections (from previous incidents).

Staple lists Brown likes to use include Facebook threat exchange, Openbl_1d (open blocklist 1 day), Ransomware_feed by Abuse.ch and SSL blacklist by Abuse.ch.

Common mistakes
Kerr shared common security analytics mistakes that trigger false positives. They are as follows: 

1. More alerts than you can process. Security analytics systems present a challenge for security organizations. They can alert on just about any event, and most of the time those alerts are for benign events. The natural response is to turn off the noisiest offenders. Turn off too many and you can miss events that are important. Good security analytics tools use context to give you the best of both worlds -- fewer, higher quality alerts.

2. Only alerting for things that are happening right now. Security analytics systems are most often configured to alert when something obviously malicious is occurring. But waiting until you see malicious activity puts your security team into response mode before they even start. Good security analytics tools are like spotlights and let you see negative behavior before it starts so you can take proactive action to stop it.

3. Only looking at network data. Security analytics systems are often configured to only see network data. This data is readily available, and connecting it to your system is generally painless. However, many potentially malicious network events are more likely benign events that triggered your security analytics system. Additional information is essential to clear the alerts and your security team spends its time gathering data to clear alerts. Good security analytics tools integrate with other internal and external data sources to eliminate specious alerts based on context.

4. Not prioritizing alerts. Time is our most precious commodity. Security analytics systems that don't effectively prioritize alerts waste your team's time by asking them to clear low-value alerts when highly important alerts linger at the bottom of their queue. Good security analytics systems drive your team to the events that most need attention via strong prioritization.

5. Alerts without context. When your security team processes an alert, the first thing they will do is look for additional information that provides the context they need to clear it. They will use a variety of on and off network tools to evaluate the alert and take the appropriate action. Good security analytics tools will assist that workflow by integrating with other internal and external data sources to minimize false alerts and prioritize real alerts more effectively.

Setting up good alerts
Brown said companies should look for reports that are specific to your industry, sector, or geographic region. Reports are better at this than feeds. Here are a few she recommends.

US-CERT reports usually contain at least some contextual information, along with network based indicators that can be used to build detections for network traffic (primarily c2 nodes--if you see this then you already have malware on your system) as well as host-based detections, which are usually malware hashes or file names. These can be blocked or monitored for. If they are blocked you should still have a way to check those alerts, it is important to see who has attempted to attack you. 

DHS/FBI and other .gov reports are often not as timely as one would hope, but if the government is taking the time to put something out, it is worth your time to read. Look for the timing on when the attack/data was from. Most of the time with these reports it is good to look back in time for indicators rather than setting up alerts. By the time the government has put something out the attackers have likely changed infrastructure or moved onto a new evolution of malware samples. 

Commercial threat reports are often more timely than government reports, though they are also usually coming at the tail end of an investigation, so it may not be worth setting up alerts looking for future activity, especially when it comes to network-based indicators like IP addresses and domains that are often not active for long. Instead, look for attacker activities or behaviors to build detections – for example, did an attacker use a phishing email to get an attacker to download a file, which then launched an executable (like we saw with the recent exploitation of CVE-2017-1099)? In that case, blocking or alerting on executables launched from word documents would be a good detection. The more a detection can look for an attacker behavior rather than just a discreet indicator associated with a piece of malware the more effective that detection will be.

Researcher blogs are some of the best sources of intelligence on threats. They are usually timely and have good technical information, though they don’t always provide higher level details such as which industries are targeted or what the impact of an attack would be. Still, they are some of the best places to get information on how attacks are being carried out.

“When building detections from blogs, I look for what can be alerted on (IPs, domains, hashes or behaviors such as where a malware sample executes), the timing and what the threat is related to. When you have these pieces you can understand what the threat is and what to do if you see something alert,” Brown said.

Rapid 7’s threat report also found that attacks increase in volume during the afternoon and evening hours. This may correlate to when most users are active, but regardless, analysts should be aware of the hours of the day when the highest volume of threats are generated.

The report also showed that most alerts come from known, bad activity, like multiple concurrent logins or malware. However, data showed a large increase in custom alerts relevant to only some organizations. This can be helpful for tracking new threats, but can also be misleading. When there are high volumes of alerts generated from a single rule in a short time period, it either means that you have a big problem with your network, or you have a big problem with the custom alert.

■ RELATED: Threat intelligence overload
Rapid 7 said attackers will always be faster, but understanding the threat landscape will go a long way toward shortening the time to respond and remediate critical issues. Knowing when you need to act fast and when to stick with a slow and steady (and reliable) remediation plan is the key.""",False positives still cause threat alert fatigue,"https://www.csoonline.com/article/3191379/false-positives-still-cause-alert-fatigue.html#:~:text=Rob%20Kerr%2C%20chief%20technology%20officer,data%20for%2070%20million%20customers."
"[""Anand Tamboli""]",2020-10-30,2020-10-30,2020-09-11,2020-10-30,,2018-02-01,0,en,,medium.com,"[""Ingrid Dickinson (CSET)""]","""In early 2018, an Australian telecommunications company bit the bullet and rolled out an AI program for its incident management process. The telco expected to save more than 25% of the operational costs from this implementation. Unfortunately, the plan backfired.
The bot was designed to intercept all of the network incidents, 100% of them. Once intercepted, it would follow a series of checks based on the problem as reported by the users. It was programmed to take one of the three pre-defined actions based on the tests it would perform.
Firstly, it would remotely resolve the incident by fixing the issue programmatically. If that did not work, it would assume that a technician’s visit is required to customer premises. Accordingly, it would issue a work order to send someone directly. If none of that were apparent, it would present the case to the human operator for further investigation and decision.
At the outset, this approach was seemingly sound and appeared quite logical. Within a few weeks, after the rollout company realized, the bot was sending an awful lot of technicians in the field. Of course, sending out technicians for the field visit was a costly affair, and it was always the last choice for fixing an issue. The bot, however, maximized on that choice.
Later, the team found out that there were a few incident scenarios only a human operator could understand (and invariably join the dots). Apparently, for the bot, they were not clear enough. In all such cases, a human operator would have taken a different decision than the bot.
Now, here was the kicker. Despite finding out the flaw in logic, the automation team was unable to turn off the bot (much like what Microsoft did with Tay in 2016). They had implemented the bot in all or nothing fashion, and it was sitting right in the middle of the user and operator interface. Which meant there were only two possibilities. Either all the incidents would go through the bot and get incorrectly handled more often. Or none of them would go through the bot and thereby getting handled manually.
But the telco was not ready to handle such a workload — they had already released the staff for saving costs (oops!).
Eventually, the telco set up another project to fix the bot while it was in operation and wasted several million dollars in the process.
They spent the money on two things, for continuing the service with an artificially stupid bot, and for running a massive fix-up project that lasted for more than a year.
Eventually, the endowment effect kicked in, and the company had no plans to go back and fix the problem from its roots. Instead, it kept pushing through and wasting an enormous amount of money, allegedly circa $11 million in operational costs.
The crucial question remains — who eventually paid for this?
Every link that joins two heterogeneous systems is a weak link!
I saw this fiasco up close and personal. In my view, this implementation went wrong on several levels, right from system design to its implementation and fixing of the problems.
But the first and foremost question is: why there was no plan B, a kill switch of some sort to stop this bot. The bot development and rollout were not thoroughly tested for all the potential scenarios and thus lacked testing rigor that could have identified problems early on. While the time required to fix the situation was too long, detecting the failure of bot took considerably longer.
This story (or case study, as some would call it) highlights many weak spots in AI and its development. It guides us to focus on specific risks. It may be merely a drop in the ocean, but an accurate representation of a few common aspects.
What went wrong?
A few things in the above story failed, and it is not the technology!
Creators of AI and the business that deployed it have not been careful enough. They did not follow the fundamental tenet of handling something as powerful as AI, responsibly.
We often say, “With great power comes great responsibility.” And yet, in this case, responsible design or deployment did not occur in full spirit.
Responsible behavior is necessary for the deployment and use of AI as well as all other stages from conception to design, testing to implementation, and ongoing management and governance.
There is also a level of weakness in the solution conception stage, which directly seeped into their development.
Emphasis on solution quality was not enough. There might have been a few testing routines. Just enough to meet the requirements of IT development frameworks, but not enough to meet the AI development framework — which does not exist!
Creators lacked thoughtfulness in the design of the solution.
Three things you should learn from this
If you are planning to implement an AI solution, or in the midst of it, then you must learn from this fiasco. It will not only save your money and resources but also give you peace of mind in the long run.
1. Rigorous testing is of utmost importance: Firstly, you must understand that narrow AI is all about the relation between input and outputs. You provide input X and get output Y, or there is input X to do output Y. Either way, the nature of input affects the output. Indiscriminate input can lead to adverse outcomes. And this is just one good reason why rigorous testing is so important. We must note that in the case of AI systems, general IT system testing mechanisms are usually not enough.
2. Always keep humans in the loop: When discretion and exceptions are required, use automated systems only as a tool to assist humans — or do not use them at all. There are still several applications and use cases that we cannot define as clearly as a game of chess. The majority of the AI systems are still kids, and they need a responsible adult to be in charge. Most importantly, ensuring enough human resources are available to handle the likely workload is always a good idea.
3. Good governance and risk management are critical: As AI systems become more powerful, managing risk is going to be even more critical. Having robust governance in place is not only an umbrella requirement for the industry but also is a good idea for every business to have in-house.
You do not always need to lose millions or face challenges to learn. When you see a failure, do not fail to learn the lesson!""",A Lesson Worth $11 Million,https://medium.com/tomorrow-plus-plus/a-lesson-worth-11-million-7851be19921f
"[""Paul Mozur"","" Raymond Zhong"","" Aaron Krolik""]",2020-11-25,2020-11-25,2020-03-01,2020-11-25,https://static01.nyt.com/images/2020/02/29/business/29china-surveillance-1/29china-surveillance-1-superJumbo.jpg?quality=90&auto=webp,2020-02-15,0,en,,www.nytimes.com,"[""Chang Liu""]","Leon Lei, 29, signed up for an Alipay code before leaving his hometown, Anqing, to return to work in Hangzhou. At first, his code was green. But a day before he departed, it turned red, and he didn’t know why. Anqing has not been especially hard hit by the virus, though it neighbors Hubei Province, the center of the outbreak.

On the road to Hangzhou, officers at two highway exits saw his digital scarlet letter and stopped him from taking the exit. Only at a third exit was he allowed to pass.

“The broad rules aren’t public,” Mr. Lei said. “How it assigns red or yellow codes isn’t public. And there’s no clear way to make your code turn green.”

Both Alibaba and Ant Financial have their headquarters in Hangzhou, and as the system expands nationwide, other places may not enforce it as stringently. According to the Xinhua news agency, 100 Chinese cities were using the system within a week of its introduction in Hangzhou on Feb. 11.

Complaints began flooding social media almost as quickly.

Vanessa Wong, 25, works in Hangzhou but has been stuck for weeks in her hometown in Hubei Province. She has no symptoms. But her health code is red, and both her employer and her housing complex in Hangzhou require people to have a green code to be allowed back.

So far, she has heard nothing from the authorities about when she might expect her code to change color. Her best guess is that it’s red simply because she is in Hubei.

Hangzhou officials have acknowledged the unease the system has caused. At a recent news conference, they urged citizens to report glitches and inaccuracies to the authorities.

“Even if a yellow code or a red code appears, don’t be nervous,” said Tu Dongshan, the deputy secretary-general of the city’s Communist Party committee.

Holed up at home and unable to concentrate on her work, Ms. Wong is feeling helpless. She cannot help noticing that the system encourages a kind of regional prejudice.

“It divides people up based on where they’re from,” she said. “Isn’t that discrimination?”

With fear of the virus still acute, many in China take comfort in high-tech precautions, even if they are at times impractical and dysfunctional. Doo Wang, 26, said her code was red for a day before it inexplicably changed to green. Calling a support hotline yielded no answers. Yet she still approves of the system.

“If we had to use it indefinitely, that would be crazy — just way too big a pain,” Ms. Wang said. “But for the epidemic, it makes sense.”",China's Health Code misjudge citizen's COVID-19 exposure risk,"https://www.nytimes.com/2020/03/01/business/china-coronavirus-surveillance.html#:~:text=A%20green%20code%20enables%20its,means%20a%20two%2Dweek%20quarantine."
"[""Jason Koebler""]",2020-10-29,2020-12-17,2020-06-29,2020-12-17,,2020-01-16,74,en,,www.vice.com,"[""Patrick Hall (BNH.ai)"",""CSET annotators""]","Detroit police have used highly unreliable facial recognition technology almost exclusively against Black people so far in 2020, according to the Detroit Police Department’s own statistics. The department’s use of the technology gained national attention last week after the American Civil Liberties Union and New York Times brought to light the case of Robert Julian-Borchak Williams, a man who was wrongfully arrested because of the technology.

In a public meeting Monday, Detroit Police Chief James Craig admitted that the technology, developed by a company called DataWorks Plus, almost never brings back a direct match and almost always misidentifies people.

“If we would use the software only [to identify subjects], we would not solve the case 95-97 percent of the time,” Craig said. “That’s if we relied totally on the software, which would be against our current policy … If we were just to use the technology by itself, to identify someone, I would say 96 percent of the time it would misidentify.""

Todd Pastorini, a general manager at DataWorks Plus, told Motherboard that it does not keep statistics on the software's accuracy in real-world use, and it does not specifically instruct law enforcement how to use the software.

""There's no statistics for that,"" Pastorini said. ""The matter is the quality of the probes used. I’m very reluctant based on the last New York Times article I was misquoted or slightly misrepresented based on the context that was used. You might know how a shovel works—you stick it in the ground to pick up dirt and you might use it as a weapon. Facial recognition has been weaponized by the media to some degree. I understand the chief’s comment, but unfortunately many people don’t.""

Pastorini likened DataWorks Plus' software to automated fingerprint identification systems, where dozens or hundreds of potential matches are returned. It ""does not bring back a single candidate,"" he said. ""It's hundreds. They are weighted just like a fingerprint system based on the probe [and what's in the database].""

The result of this, according to Detroit's own police officers, is that they are ultimately making the decision to question and investigate people based on what the software returns and a detective's judgment. This means that people who may have had nothing to do with a crime are ultimately questioned and investigated by police. In Detroit, this means, almost exclusively, Black people.

So far this year (through June 22), the technology had been used 70 times, according to publicly released data by the Detroit Police Department. In 68 of those cases, the photo fed into the software was of a Black person; in two of the cases, the race was listed as 'U,' which likely means unidentified (in other reports from the police, U stands for unidentified); the Detroit Police Department did not respond to a request to clarify. These photos were largely pulled from social media (31 of 70 cases), or a security camera (18 of 70 cases).

Several cities have banned police from using facial recognition software, which has well-known racial bias issues (and many false-positive issues as well). Detroit, however, has a very public debate in 2019 about the use of facial recognition, and instead decided to regulate its use rather than ban it altogether. Late last year, the city adopted a policy, which bans the use of facial recognition to “surveil the public through any camera or video device,” bans its use on livestream and recorded videos, and restricts (but does not ban) its use at protests. According to the policy, the software must be used only “on a still image of an individual,” and can only be used as part of an ongoing criminal investigation. The software checks images across a state database of photos, which include mugshot images. As part of these regulations, the police department is required to release weekly reports about the use of the technology, which show that it has been almost exclusively used on Black people.

Williams was arrested before the policy went into practice. Craig said during the meeting that the media it ran through DataWorks’ facial recognition system was “a horrible video. It was grainy … it would have never made it under the new policy … if we can’t obtain a good picture, we’re not going to push it through to the detective.”

Craig and his colleague, Captain Aric Tosqui, said that they want to continue using facial recognition because they say it can be a tool to assist investigators even if it doesn’t often lead to arrest. But even when someone isn’t falsely arrested, their misidentification through facial recognition can often lead to an investigator questioning them, which is an inconvenience at best and a potentially deadly situation at worst. According to Tosqui, the technology has been used on a total of 185 cases throughout the years. “The majority of the cases the detective reported back that [the match] was not useful.”

Despite these problems, DataWorks Plus said that it does not guide law enforcement on how to best use the software. ""We don't tell our customers how to use the system,"" Pastorini said. ""There’s already law enforcement policies. It is my experience the clearer the image, clearly is going to affect the likelihood of a more solid result.""

The Detroit Police Department did not respond to a request for further comment. In recent months, there has been a new movement by city council members to ban the use of the technology.",Detroit Police Chief: Facial Recognition Software Misidentifies 96% of the Time,https://www.vice.com/en/article/dyzykz/detroit-police-chief-facial-recognition-software-misidentifies-96-of-the-time
"[""Ben Geman""]",2020-11-17,2020-12-17,2020-01-29,2020-12-17,,2020-01-16,0,en,5fdaf7b4c549550761c63f79,www.axios.com,"[""Charlie Pownall"",""Ingrid Dickinson (CSET)""]","A report this month by the activist group Avaaz alleges YouTube is ""driving millions of people to watch climate misinformation"" daily.

What they found: One finding is that when users search for ""global warming,"" 16% of the top 100 ""related videos"" in the ""up next"" feature had climate disinformation. Another is that major brands are often unaware that their ads run on these videos.

The big picture: Democratic Rep. Kathy Castor, who heads the Select Committee on the Climate Crisis, this week urged Google to curb false climate information on YouTube. She called for steps including removing climate ""denial"" and ""disinformation"" from YouTube's recommendation algorithm.

No longer allowing users to monetize videos that ""promote harmful misinformation and falsehoods"" about climate.
The other side: A YouTube spokesperson said the company has ""significantly invested in reducing recommendations of borderline content and harmful misinformation, and raising up authoritative voices.""

Their policies give advertisers the ""tools to opt out of content that doesn’t align with their brand,"" the spokesperson added.",YouTube faces criticism over climate misinformation,https://www.axios.com/youtube-climate-change-misinformation-8ff85b76-285b-4b40-b8ca-2b7211246fb2.html
"[""Anonymous""]",2021-04-28,2021-04-29,2021-04-09,2021-04-29,,2021-04-08,0,en,,poliisi.fi,"[""Roman Lutz""]","The National Bureau of Investigation (NBI) has discovered within its activities a possible information security incident involving personal data and has made a report on the incident to the National Police Board. The alleged incident took place during the processing of personal data in a service for which information security or compliance with data protection legislation may not have been ensured in advance in a sufficient manner.

The incident was discovered by the NBI on Thursday 8 April. On the basis of initial information from the NBI, the National Police Board decided to report the matter to the Office of the Data Protection Ombudsman and submitted the report on Friday 9 April.

The information security incident is related to the testing of facial recognition technology carried out in early 2020 by the NBI's unit responsible for combating the sexual exploitation of children. The duties of this unit include pre-screening child sexual abuse material received from international stakeholders.

The police have a legal right to process biometric facial images under the conditions set out by law if the processing is necessary to perform a statutory police duty. 

– The unit tested a US service called Clearview AI for the identification of possible victims of sexual abuse to control the increased workload of the unit by means of artificial intelligence and automation. The identification of the victims depicted in the images and videos examined by the unit is crucial to safeguarding the victims' rights, and to interrupting any crime in progress or detecting an offence already committed, says Head of Cybercrime Centre, Senior Detective Superintendent Mikko Rauhamaa of the NBI.

During the trial period, the unit made around 120 searches in the software, which makes use of pictures of persons on social media. The pictures entered into this software had been edited by the unit to only depict the face of those who needed to be identified. There was one 'hit' during the trial period, and it led to cooperation with social authorities. No criminal investigation has been started or other actual police action taken as a result of the hit.

After using the software for the duration of the trial period, it was concluded that Clearview AI was not suitable in Finland for that particular cooperation between authorities. 

– One of the statutory duties of the NBI is to further develop methods of combating crime. The unit concerned was exploring tools for reducing the workload and enhancing the effectiveness of police processes to enable immediate intervention in any exploitation in progress. In this particular case, the software was not suitable for the police, Mikko Rauhamaa says.

The NBI continues to investigate the matter.",Testing of facial recognition software by NBI reported to Data Protection Ombudsman,https://poliisi.fi/sv/-/ckp-s-testanvanding-av-ett-program-avsett-for-ansiktsigenkanning-har-anmalts-till-dataombudsmannen?languageId=en_US
"[""Anonymous""]",2021-04-28,2021-04-29,2021-04-08,2021-04-29,https://i.guim.co.uk/img/media/239bf7b02c03bad84bbdd6c1cabc3f88b8886a25/0_0_4500_2700/master/4500.jpg?width=620&quality=85&auto=format&fit=max&s=22b4a7df7643a6e76a9e458ab8cbd051,2020-07-21,0,en,608a32da74c838bcc7fd552b,www.theguardian.com,"[""Roman Lutz""]","A software mistake caused a Tui flight to take off heavier than expected as female passengers using the title “Miss” were classified as children, an investigation has found.

The departure from Birmingham airport to Majorca with 187 passengers on board was described as a “serious incident” by the Air Accidents Investigation Branch (AAIB).

An update to the airline’s reservation system while its planes were grounded due to the coronavirus pandemic led to 38 passengers on the flight being allocated a child’s “standard weight” of 35kg as opposed to the adult figure of 69kg.

This caused the load sheet – produced for the captain to calculate what inputs are needed for take-off – to state that the Boeing 737 was more than 1,200kg lighter than it actually was.

Despite the issue, the thrust used for the departure from Birmingham on 21 July 2020 was only “marginally less” than it should have been, and the “safe operation of the aircraft was not compromised”, the AAIB said.

The same fault caused two other Tui flights to take off from the UK with inaccurate load sheets later that day.

The system was adapted when the problem was first identified 11 days earlier, but this did not correct the weight entries for the 21 July flights.

The operator subsequently introduced manual checks to ensure adult females were referred to as Ms on relevant documentation.

Tui said in a statement: “The health and safety of our customers and crew is always our primary concern. Following this isolated incident, we corrected a fault identified in our IT system. As stated in the report, the safe operation of the flight was not compromised.”",Tui plane in ‘serious incident’ after every ‘Miss’ on board was assigned child’s weight,https://www.theguardian.com/world/2021/apr/09/tui-plane-serious-incident-every-miss-on-board-child-weight-birmingham-majorca
"[""Patrick Howell O'Neill""]",2021-05-03,2021-05-04,2020-02-19,2021-05-04,https://cdn.technologyreview.com/i/images/screen-shot-2020-02-18-at-5.43.55-pm.png?sw=700&cx=19&cy=9&cw=812&ch=457,2020-02-19,0,en,,www.technologyreview.com,"[""Roman Lutz""]","A two inch piece of tape fooled the Tesla’s cameras and made the car quickly and mistakenly speed up.

Don’t believe your car’s lying eyes. 

Hackers have manipulated multiple Tesla cars into speeding up by 50 miles per hour. The researchers fooled the car’s Mobileye EyeQ3 camera system by subtly altering a speed limit sign on the side of a road in a way that a person driving by would almost never notice.

This demonstration from the cybersecurity firm McAfee is the latest indication that adversarial machine learning can potentially wreck autonomous driving systems, presenting a security challenge to those hoping to commercialize the technology.

Mobileye EyeQ3 camera systems read speed limit signs and feed that information into autonomous driving features like Tesla’s automatic cruise control, said Steve Povolny and Shivangee Trivedi from McAfee’s Advanced Threat Research team.

The researchers stuck a tiny and nearly imperceptible sticker on a speed limit sign. The camera read the sign as 85 instead of 35, and in testing, both the 2016 Tesla Model X and that year’s Model S sped up 50 miles per hour.

This is the latest in an increasing mountain of research showing how machine-learning systems can be attacked and fooled in life-threatening situations. 

In an 18-month-long research process, Trivedi and Povolny replicated and expanded upon a host of adversarial machine-learning attacks including a study from UC Berkeley professor Dawn Song that used stickers to trick a self-driving car into believing a stop sign was a 45-mile-per-hour speed limit sign. Last year, hackers tricked a Tesla into veering into the wrong lane in traffic by placing stickers on the road in an adversarial attack meant to manipulate the car’s machine-learning algorithms.

“Why we’re studying this in advance is because you have intelligent systems that at some point in the future are going to be doing tasks that are now handled by humans,” Povolny said. “If we are not very prescient about what the attacks are and very careful about how the systems are designed, you then have a rolling fleet of interconnected computers which are one of the most impactful and enticing attack surfaces out there.”

As autonomous systems proliferate, the issue extends to machine-learning algorithms far beyond vehicles: A March 2019 study showed medical machine-learning systems fooled into giving bad diagnoses.

The McAfee research was disclosed to both Tesla and Mobileye EyeQ3 last year. Tesla did not respond to a request for comment from MIT Technology Review but did acknowledge McAfee’s findings and say the issues would not be fixed in that generation of hardware. A Mobileye spokesperson downplayed the research by suggesting that the modified sign would fool even a human into reading 85 instead of 35. The company doesn't consider tricking the camera to be an attack, and despite the role the camera plays in Tesla’s cruise control, it wasn’t designed for autonomous driving. 

“Autonomous vehicle technology will not rely on sensing alone, but will also be supported by various other technologies and data, such as crowdsourced mapping, to ensure the reliability of the information received from the camera sensors and offer more robust redundancies and safety,” the Mobileye spokesperson said in a statement.

Tesla has since moved to proprietary cameras on newer models, and Mobileye EyeQ3 has released several new versions of its cameras that in preliminary testing were not susceptible to this exact attack.

There are still a sizable number of Tesla cars operating with the vulnerable hardware, Povolny said. He pointed out that Teslas with the first version of hardware cannot be upgraded to newer hardware. 

“What we’re trying to do is we're really trying to raise awareness for both consumers and vendors of the types of flaws that are possible,” Povolny said “We are not trying to spread fear and say that if you drive this car, it will accelerate into through a barrier, or to sensationalize it.”",Hackers can trick a Tesla into accelerating by 50 miles per hour,https://www.technologyreview.com/2020/02/19/868188/hackers-can-trick-a-tesla-into-accelerating-by-50-miles-per-hour/
"[""Haroon Siddique"",""Ben Quinn""]",2021-05-04,2021-05-04,2021-04-23,2021-05-04,https://i.guim.co.uk/img/media/392df3fb7c2f3092939176f1aaa5f75f3ac84966/0_0_6720_4032/master/6720.jpg?width=620&quality=85&auto=format&fit=max&s=14e9b5783016615997d959ddc5446a5c,2021-04-21,0,en,,www.theguardian.com,"[""Roman Lutz""]","Dozens of former Post Office workers had their convictions for theft, fraud and false accounting quashed by the court of appeal on Friday after one of the biggest miscarriages of justice in British legal history.

The decision to clear 39 subpostmasters led to immediate calls for a full public inquiry and for them, and the hundreds caught up in the scandal, to be properly compensated.

Some of the convicted workers were sent to prison, others lost their livelihoods and their homes. Many went bankrupt – and some died before their names were cleared.

So far, those who have been offered compensation are to receive less than £22,000 each after legal fees.

As those cleared left the Royal Courts of Justice, some weeping, they were cheered by supporters and other former Post Office workers.

Vijay Parekh, 62, spent six months in prison after he was advised by his barrister to plead guilty when accused of theft of about £78,000.

“It was intended to be the business that we would work through towards a comfortable retirement,” he said, flanked by relatives. “The whole family suffered. I was inside, but outside my father was in his 70s and it had an impact on everyone. It was impossible to sleep, you found yourself crying every day. Because of that CRB [criminal history] check you really can’t work anywhere at all. Now it will have been cleared and I could look for a job but I have reached retirement age.”

Campaigners believe that as many as 900 operators, often known as subpostmasters, may have been prosecuted and convicted between 2000 and 2014 after the Horizon IT system installed by the Post Office and supplied by Fujitsu falsely suggested there were cash shortfalls.

In his damning written judgment, Lord Justice Holroyde, sitting with Mr Justice Picken and Mrs Justice Farbey, said the Post Office, which brought the prosecutions itself, “knew that there were serious issues about the reliability of Horizon”.

He wrote: “The failures of investigation and disclosure were in our judgment so egregious as to make the prosecution of any of the ‘Horizon cases’ an affront to the conscience of the court.

“By representing Horizon as reliable, and refusing to countenance any suggestion to the contrary, POL [Post Office Limited] effectively sought to reverse the burden of proof: it treated what was no more than a shortfall shown by an unreliable accounting system as an incontrovertible loss, and proceeded as if it were for the accused to prove that no such loss had occurred.

“Denied any disclosure of material capable of undermining the prosecution case, defendants were inevitably unable to discharge that improper burden. As each prosecution proceeded to its successful conclusion the asserted reliability of Horizon was, on the face of it, reinforced. Defendants were prosecuted, convicted and sentenced on the basis that the Horizon data must be correct, and cash must therefore be missing, when in fact there could be no confidence as to that foundation.”

The court of appeal considered 42 cases, which were referred last year by the Criminal Cases Review Commission after a landmark civil case brought by the Justice for Subpostmasters Alliance against the Post Office.

The Post Office settled the civil claim brought by 555 claimants for £57.75m – amounting to £12m after legal costs – without admitting liability, in December 2019. This month, the Post Office chief executive, Nick Read, who took up his post in September 2019, called on the government to fund “meaningful compensation” to those wrongfully convicted, saying the Post Office “simply does not have the financial resources” to do so.

No one has ever been held accountable for the scandal and while a government inquiry was launched last year, campaigners say it does not go far enough.

David Enright, from Howe + Co solicitors, who acts for the alliance, said the inquiry “has been set up by the very organisation that owns the Post Office (the Department for Business, Energy and Industrial Strategy), and there’s a clear financial conflict of interest so that’s inappropriate. Secondly, it’s non-statutory so therefore it cannot hear evidence under oath, it can’t compel witnesses, it can’t compel evidence, so it’s toothless.

“And then, most importantly, its published terms of references specifically exclude it and prevent it from looking at the Post Office’s role in criminal prosecutions. It’s quite rightly been reported as the largest miscarriage of justice in British legal history. In what circumstances would we not have a statutory inquiry into that?”

He warned that if the government did not agree within seven days to re-establish it as a statutory inquiry and consult on the terms of reference, his client would be launching a judicial review.

Labour also called for “a proper inquiry with teeth” while the Communication Workers Union called for criminal investigations into senior Post Office figures who “oversaw the criminalisation of hundreds of postmasters”.

In an earlier ruling at the high court, Mr Justice Fraser found the Fujitsu-developed Horizon system contained “bugs, errors and defects” and that there was a “material risk” that shortfalls in branch accounts were caused by the system.

The 42 argued their convictions were unsafe because in light of the evidence, including Fraser’s findings, the trial process must have been unfair and it was an affront to the public conscience for them to face prosecution.

The Post Office conceded the first ground in relation to the 39 who were cleared but only conceded the second in relation to four of them. The court cleared all 39 on both grounds but rejected three other appeals, which the Post Office had fully opposed, the judges concluding that the Horizon data was not central to those cases.

Speaking on a visit to a farm in Stoney Middleton, Derbyshire, Boris Johnson said: “I know the distress many subpostmasters and their families have felt for a very long time now through the Horizon scandal and I’m pleased that we’ve got the right judgment.

“Our thoughts are very much with the victims and we’ll have to make sure that people get properly looked after because it’s clear that an appalling injustice has been done.”

The Post Office chairman, Tim Parker, expressed his sorrow for the impact on all affected and said it had “supported the overturning of the vast majority of convictions. We are contacting other postmasters and Post Office workers with criminal convictions from past private Post Office prosecutions that may be affected, to assist them to appeal should they wish.”",Court clears 39 post office operators convicted due to ‘corrupt data’,https://www.theguardian.com/uk-news/2021/apr/23/court-clears-39-post-office-staff-convicted-due-to-corrupt-data
"[""@moo_hax; @monoxgas""]",2019-09-08,2021-05-13,2019-09-08,2021-05-13,,2019-09-08,0,en,,github.com,"[""Anonymous""]","An issue was discovered in Proofpoint Email Protection through 2019-09-08. By collecting scores from Proofpoint email headers, it is possible to build a copy-cat Machine Learning Classification model and extract insights from this model. The insights gathered allow an attacker to craft emails that receive preferable scores, with a goal of delivering malicious emails.",Proofpoint Functional Extraction,https://github.com/moohax/Proof-Pudding
"[""@moo_hax; @monoxgas""]",2020-09-08,2021-05-13,2020-09-08,2021-05-13,,2020-09-08,0,en,,github.com,"[""Anonymous""]","An issue was discovered in Windows Defender for PowerShell. By collecting 0 or 1 from AMSI, it is possible to build a copy-cat Machine Learning Classification model and extract insights from this model. The insights gathered allow an attacker to craft PowerShell scripts that bypass AMSI protections, with the goal of malicious code execution. ",Windows Defender Functional Extraction,https://github.com/moohax/aiv_workshop_20
"[""Olivia Solon and Cyrus Farivar""]",2021-07-27,2021-07-27,2021-06-06,2021-07-27,"https://media-cldnry.s-nbcnews.com/image/upload/t_fit-2000w,f_auto,q_auto:best/newscms/2021_22/3480433/210604-robert-jones-mn-1500.jpg",2015-09-01,0,en,,www.nbcnews.com,"[""Kate Perkins""]","June 6, 2021, 3:00 AM MST
By Olivia Solon and Cyrus Farivar
Five months after Robert Jones, a 44-year-old aerospace process auditor, moved to what he described as the “really nice” neighborhood of Gulf Harbors in Pasco County, Florida, with his wife and four kids, “seven or eight” police cars showed up at his door.

Officers said they had heard about his then-16-year-old son Bobby’s school delinquency from colleagues in Pinellas County, where the family previously lived, and wanted to make sure he understood that the Pasco Sheriff’s Office did things a little differently, Jones recalled.

Bobby had been expelled from his last school after he was caught smoking pot and then got into a fight with another student. But both he and his dad had hoped the move to Pasco County would provide a fresh start.

“Truthfully, I thought it was one of these ‘scared straight’ moments,” said Jones, referring to the sheriff’s office's intimidating welcome to the neighborhood.

Officers said they wanted to come in and talk to his kids, and Jones said he let them. Within seconds they had their flashlights on and were, Jones said, “running rampant” without a search warrant.

The police report stated that Jones consented to a search. But he disputes this.

They scoured the house and found empty zip-close bags in his son’s room that later tested positive for traces of marijuana. Bobby was arrested and spent three weeks in juvenile detention before his trial, where the judge dismissed the charges due to the lack of measurable marijuana. He had been at his new school for just over a week.

What Jones didn’t realize at the time was that his son had been identified as a target by the Pasco Sheriff’s Office's “intelligence-led” policing program. Police had gathered records of Bobby’s previous interactions with law enforcement and were using his history to predict that he would be a troublemaker in Pasco County.

After Bobby was released, a monthslong ordeal followed, which Jones described as a “horror story” of police showing up at the family home, sometimes multiple times a day or in the middle of the night, to inquire about Bobby or ask to enter the home. Any time there was a crime in the neighborhood, such as a burglary, Bobby was a suspect. On some occasions, described in a lawsuit filed in March by Jones and others targeted bythe Pasco Sheriff’s Office, as many as 18 officers would show up at the home, “banging on windows and yelling at his young daughters while they were hiding under the bed.”

Jones, who had studied to be a paralegal, said he tried to stand his ground and refused to allow officers to conduct any more warrantless searches of his property. But police interpreted this behavior, the lawsuit states, as uncooperative and he was repeatedly cited — and eventually arrested — for property code violations such as having overly long grass, missing numbers on his mailbox and a Jet Ski trailer on the property.

From October 2015 through April 2016, Jones, who had no previous criminal history, was arrested five times. None of the arrests resulted in a conviction. His home was ransacked, laptops and phones seized, and he ultimately fled their home in the middle of the night to avoid further harassment by police, the lawsuit alleges.

“My family will never be made whole from this atrocity,” he said. “Where does anyone in my family get their presumption of innocence back?”

The sheriff’s office disputed the idea that its intelligence-led policing was “predictive policing” in the sense characterized by the science fiction movie ""Minority Report,"" where people are arrested by a police department’s ""pre-crime"" division before they have the chance to carry out illegal deeds.

It said the Pasco Sheriff’s Office used historical data to “work with those who have shown a consistent pattern of offending to attempt to break the cycle of recidivism” but said Bobby was not added to its “prolific offender program,” which results in random visits from deputies, until 2017 — long after the period of harassment alleged by Jones.

A spokesman said Bobby had interacted with the sheriff’s office three times before officers showed up at the family home in September 2015. The spokesman said officers went to the house to discuss “involvement in criminal activity in the area” and searched his room to look for a stolen GPS device, which is how they found the empty baggies.

In a lengthy statement released by the agency in response to reporting on the program, a spokesman said the individuals accusing the sheriff’s office of harassment all had “lengthy criminal histories, often with a multitude of arrests and victims.”

Predictive analytics
Jones' family and civil liberties experts believe his case is emblematic of a broader effort by law enforcement agencies across the United States to predict criminality based on a wide range of data points that are crunched together and used to assign a risk score to individuals or places. While some law enforcement agencies say it can be a useful approach to efficient resource allocation and early intervention, critics say these programs can enter into the alarming realm of ""pre-crime,"" where the presumption of innocence is lost, and that they encode existing racial and social biases.

Pasco County’s approach to “intelligence-led” policing, developed over a decade, has drawn particular concern from civil liberties experts because of a data-sharing arrangement with the local school district, which was first reported by Tampa Bay Times. That partnership gave the police access to data relating to students’ grades, attendance and behavior as well as any history of abuse or other “adverse childhood experiences.”

School records were used to allocate students one of four labels: on track, at risk, off track or critical. Getting a D grade or having a parent or sibling go to prison could be enough to put a child in the “at risk” category, according to Pasco’s own 83-page “Intelligence-Led Policing Manual,” first obtained by the Tampa Bay Times.

The manual, last updated in January 2018, states that the data-sharing was designed to identify “at-risk youth who are destined to a life of crime” and intervene to “set them on the right path.”

The sheriff’s office took the list of 20,000 students determined to be at risk, according to school data, and cross-referenced it with its own records of law enforcement interactions to come up with a smaller list of a few hundred students to be monitored closely and offered “positive mentorship and support” by school resource officers — sheriff’s office deputies contracted to work at the school. The agency does not notify parents of children added to the list but said parents can file a public records request to find out.

Bobby and his dad say they were never offered any kind of diversion program or support at school or home, only harassment and punishment. The sheriff’s office said the program to identify at-risk students was entirely separate from the ""prolific offender"" program but that Bobby and other adolescent offenders were given a “resource card” featuring details about “opportunities in our community” relating to mental health and substance abuse.

Criminal justice advocates say programs like these targeting adolescents, and the broader trend to increase surveillance in schools under the guise of school safety, fuel the so-called school-to-prison pipeline. This is where instead of letting children and teenagers make and learn from their mistakes, they are marked as criminals at an early age — even if they are only interacting with school resource officers. Once they are in the criminal justice system, it’s almost impossible to escape.

“This idea that you can predict criminality is very worrisome and extremely troubling. You cannot,” said Jason Nance, professor of law at the University of Florida Levin College of Law. “These are real kids with real lives. If you have extra scrutiny on a child and that child does something minor or untoward, you may set that student on a pathway in which that student will become more involved in the criminal justice system later on.”

After the Tampa Bay Times disclosed the existence of the program in late 2020, the program received widespread criticism from civil liberties and legal experts.

“It is really, as someone who has studied this, it is jaw-droppingly bad in all aspects,” said Andrew Ferguson, a law professor at American University. “They basically built this system as a justification to chase the bad kids out of town, to monitor them in over-aggressive ways with no intention to help them but to make their lives so miserable that they would leave.”

The intelligence-led policing program, which is the subject of the lawsuit in which Jones and three other parents are named as plaintiffs, alleges that the sheriff’s office repeatedly violated property rights with its unwarranted, suspicionless visits to targets’ homes.

“Having a policy of harassment and intimidation and constitutional violations of a county’s own residents is not a legitimate way to do police work,” said Ari Bargil, an attorney at the Institute for Justice, the legal nonprofit bringing the case.

Federal notice
Pasco’s program has also attracted the attention of the U.S. Department of Education, which in April opened an investigation into the data-sharing arrangement between the school district and the sheriff’s office.

The Department of Education investigation focuses on whether the data-sharing between the school district and law enforcement violates the Family Educational Rights and Privacy Act, a federal law that governs access to children’s education records. Under FERPA, a school is not allowed to disclose personally identifiable information from a student’s education records without consent, although there are some exceptions for emergency situations.

“The unlawful sharing of information between school districts and law enforcement is not unusual,” said Harold Jordan, senior policy advocate at the American Civil Liberties Union of Pennsylvania. “What makes Pasco County an outlier is the notion that they scan large groups of kids’ records. They aren’t even claiming the kids have made any threats. These aren’t situations where there is any legitimate law enforcement investigation happening, just that the school has identified some kids as disruptive.”

Alberto Betancourt, a Department of Education spokesman, said the agency is not able to provide any further details about the investigation until it has concluded — a process that experts say could take years.

If Pasco County Schools is found to be in violation of FERPA, it could lose its federal funding. However, privacy expert Linnette Attai, author of a book about FERPA compliance, said this has never happened before and is “an extremely unlikely outcome.”

In early May, weeks after the Department of Education launched its investigation, the Pasco Sheriff’s Office and the school board revised their data-sharing agreement so that police officers contracted as school resource officers would no longer have access to student grades or discipline histories, nor the school district’s early warning system, which categorizes students as on track, off track or at risk.

In a statement, Sheriff Chris Nocco said the agency was “voluntarily making this update” to “ease any anxiety that parents may have as a result of misinformation perpetuated by media reports.”

Some civil liberties and criminal justice organizations say this doesn’t go far enough, noting that the new wording still allows ""criminal intelligence analysts"" from the sheriff’s office to access school data.

A group of 30 advocacy groups called the PASCO Coalition (People Against Surveillance of Children and Overpolicing​), which includes the Southern Poverty Law Center, Electronic Frontier Foundation and Color of Change, issued a statement saying it was “extremely disappointed” by the announced revisions to the data-sharing agreement because they did not go far enough. It called for the immediate termination of “the student data-sharing arrangement for any collaboration, involvement, or participation in school-based predictive policing” operated by the sheriff’s office.

National problem
School districts and law enforcement officials nationwide are struggling with similar problems. In 2015, in St. Paul, Minnesota, the Ramsey County Attorney's Office, the St. Paul school board, the Ramsey County Sheriff's Office and the city of St. Paul, among others, started collaborating and trying to come up with a new way to help troubled youth and families.

Like the program in Pasco County, this one also was designed to help children avoid any interactions with the juvenile justice system by sharing data across agencies.

In a slide deck from April 2018, the agencies stated that they wanted to “transform the way our public systems work together by developing a new triage referral system that uses data-driven decision-making” and “prevent interaction with the juvenile justice system” by creating a “statewide system” to share information across agencies.

This plan was quickly met with resistance from local advocacy groups, which banded together to form the Coalition to Stop the Cradle to Prison Algorithm. Local activists argued that such data could be used to racially profile and make inaccurate predictions about students of color, which could have led to outcomes that local authorities were trying to avoid.

Marika Pfefferkorn, co-founder of the coalition, pointed to the fact that the Minnesota Department of Human Rights found in March 2018 that students of color comprise two-thirds of all school suspensions across the state, despite the fact that such students constitute only 31 percent of the student population.

So, she argued, if a predictive model is using implicitly biased data like school suspensions, it is unlikely to be a useful way to determine future criminality.

”What they were proposing to use as indicators would not actually be a sound understanding of who was in need,” she said.

The program was halted in less than a year.

Just a few years before St. Paul began to think about how to integrate data-sharing as a way to predict possible criminal behavior, Rochester, Minnesota, explored a related idea.

Rochester was one of several cities — including Memphis, Tennessee, and Richmond, Virginia — to buy an “advanced analytics software” tool called InfoSphere Identity Insight that would enable local police to forecast crime “hot spots.”

In the Minnesota city, which is about 100 miles southeast of the Twin Cities, the goal was to not only target adult criminals but also approximately 30 juveniles, combining data about criminal history and their real-world social network.

But this program didn’t have great results, according to Capt. John Sherwin, a 20-year veteran of the Rochester Police Department. The reality was that sometimes “predictions” produced by the IBM system were things that veteran officers had already figured out. For example, Sherwin said, juveniles who have a probation violation are slightly more likely to commit a violent felony offense as an adult than the general population.

“The idea was novel: We are going to disrupt these trends that have plagued juveniles going to prison,” he said. “The results were really underwhelming.”

In 2019, after being sued by the ACLU, California’s Riverside County stopped using a voluntary probation program that designated young people as at risk of criminality for noncriminal behavior. Signals that a child was at risk included truancy, talking back to school officials or poor academic performance. Adolescents who were signed up to the program faced unannounced home visits, restrictions on who they could speak to and curfews.

“Young people are supposed to be allowed to do stupid things. That’s how they learn,” said Corey Jackson, CEO of student mentoring program Sigma Beta Xi, which partnered with the ACLU in the lawsuit. “But with this program, if they made a mistake, they could be punished for the rest of their life.”

The Pasco Sheriff’s Office’s manual described this type of intelligence-led policing as a way for law enforcement to operate more efficiently and focus resources on the “most serious and prolific offenders.”

Intelligence-led policing ""emphasizes analysis and intelligence as pivotal to an objective, decision-making framework that prioritizes crime hot spots, repeat victims, prolific offenders, and criminal groups. It facilitates crime and harm reduction, disruption, and prevention through strategic and tactical management, deployment and enforcement,” the manual states.

Lasting impact
For Robert Jones, the damage is already done.

“I have friends who are police officers and have seen plenty of good police. But my kids will never see them like that,” said Jones. “They don’t think they are out to help at all, but to hurt, and that’s really scary.”

His son Bobby, now 21, who is currently in the Pinellas County jail awaiting trial for an assault charge, is struggling to deal with the fallout.

""I was still a child,"" he said. ""Rather than targeting me, they should have offered some kind of redirection or supervision. It would have helped my mental state.""

CORRECTION (June 7, 2021 12:02a.m.ET): A previous version of this article misidentified Robert Jones' wife. They are married; she is not his girlfriend.",Predictive policing strategies for children face pushback,https://www.nbcnews.com/tech/tech-news/predictive-policing-strategies-children-face-pushback-n1269674
"[""Joe Henderson""]",2021-07-27,2021-07-27,2020-09-09,2021-07-27,https://s3.amazonaws.com/ledejs/resized/s2020-pasco-ilp/480/taylor1.jpg,2015-09-01,0,en,,floridapolitics.com,"[""Kate Perkins""]","The county's Intelligence Led Policing sounds like an Orwellian nightmare.
You live in Pasco County and have one or two run-ins with the Sheriff’s Office on your record. Maybe it wasn’t a major crime, but that doesn’t matter. It’s late, and you’re headed to bed, when … BAM! BAM!

Someone is pounding on the door, and without looking, you know who it is because they’ve been there many times before. Pasco County deputies are just checking to see what you’re doing.

They know where you live and want you to remember that. They are always watching.

This sounds like an Orwellian nightmare. According to a scalding Tampa Bay Times investigative report, though, it’s a reality for those caught in the net of the Intelligence-Led Policing program incorporated by Sheriff Chris Nocco. It’s designed to identify people likely to commit a crime.

The concept is not new. Police departments in several cities around the country use computer models to predict where crimes might occur and who the guilty parties might be. In 2001, Tampa posted three dozen facial recognition cameras in Ybor City but took them down a couple of years later after no results.

Cameras have been used in Super Bowl host cities, including Tampa, to monitor possible terrorists.

And Tampa Mayor Jane Castor admitted a program that targeted Black bicyclists when she was the city’s Police Chief was wrong.

That said, the Pasco program, according to the Times report, takes this idea to a different level. After the computer identifies people to target, deputies take it from there.

“They swarm homes in the middle of the night, waking families and embarrassing people in front of their neighbors,” the Times reported. “They write tickets for missing mailbox numbers and overgrown grass, saddling residents with court dates and fines.”

And, the Times concluded, “They come again and again, making arrests for any reason they can.”

One former deputy described the directive like this to the Times: “Make their lives miserable until they move or sue.”

The Times had bodycam footage of some of the interactions. Imagine this happening at your house.

The Sheriff’s response is that the ends justify the means.

“We again would like to reiterate our firm stance that Intelligence-Led Policing has worked to reduce property crimes in Pasco County and continues to work in agencies in our area that also use this model such as Hillsborough County,” the Office said in a lengthy retort.

“This reduction in property crimes includes a 74.4% reduction in residential burglaries and a 20.7% reduction in auto thefts, along with an overall reduction of property crimes of 35.6%.”

That data was from 2011 when Nocco became Sheriff through 2019.

From dispatch logs, the Times reported deputies executed more than 12,500 of these “visits” since 2015. And about 10% of those trips were aimed at individuals under 18. To be fair, Nocco is popular with most Pasco residents. Supporters believe he is doing what he must to protect them.

But harassment on this scale as a police tactic goes over the line. Deputies don’t bring warrants and don’t have probable cause. They just show up, repeatedly.

I’m not a lawyer, but an argument can be made this violates the Fourth Amendment.

It states, “The right of the people to be secure in their persons, houses, papers, and effects, against unreasonable searches and seizures, shall not be violated …”

Pasco has moved to the edge of unreasonable, and maybe a step or two over.",Joe Henderson: Do Pasco Sheriff’s Office surveillance tactics go over the line?,https://floridapolitics.com/archives/365879-joe-henderson-do-pasco-sheriffs-office-surveillance-tactics-go-over-the-line/
"[""Kathleen McGrory and Neil Bedi""]",2021-07-27,2021-07-27,2020-09-03,2021-07-27,https://s3.amazonaws.com/ledejs/resized/s2020-pasco-ilp/480/dotson3.jpg,2015-09-01,0,en,,projects.tampabay.com,"[""Kate Perkins""]","Pasco County Sheriff Chris Nocco took office in 2011 with a bold plan: to create a cutting-edge intelligence program that could stop crime before it happened.

What he actually built was a system to continuously monitor and harass Pasco County residents, a Tampa Bay Times investigation has found.

First the Sheriff’s Office generates lists of people it considers likely to break the law, based on arrest histories, unspecified intelligence and arbitrary decisions by police analysts.

Then it sends deputies to find and interrogate anyone whose name appears, often without probable cause, a search warrant or evidence of a specific crime.

They swarm homes in the middle of the night, waking families and embarrassing people in front of their neighbors. They write tickets for missing mailbox numbers and overgrown grass, saddling residents with court dates and fines. They come again and again, making arrests for any reason they can.

One former deputy described the directive like this: “Make their lives miserable until they move or sue.”

In just five years, Nocco’s signature program has ensnared almost 1,000 people.

At least 1 in 10 were younger than 18, the Times found.

Some of the young people were labeled targets despite having only one or two arrests.

Rio Wojtecki, 15, became a target in September 2019, almost a year after he was arrested for sneaking into carports with a friend and stealing motorized bicycles.

Those were the only charges against Rio, and he already had a state-issued juvenile probation officer checking on him. Yet from September 2019 to January 2020, Pasco Sheriff’s deputies went to his home at least 21 times, dispatch logs show.

They showed up at the car dealership where his mom worked, looked for him at a friend’s house and checked his gym to see if he had signed in.

More than once, the deputies acknowledged that Rio wasn’t getting into trouble. They mostly grilled him about his friends, according to body-camera video of the interactions. But he had been identified as a target, they said, so they had to keep checking on him.

Since September 2015, the Sheriff’s Office has sent deputies on checks like those more than 12,500 times, dispatch logs show.

Deputies gave the mother of one teenage target a $2,500 fine because she had five chickens in her backyard. They arrested another target’s father after peering through a window in his house and noticing a 17-year-old friend of his son smoking a cigarette.

As they make checks, deputies feed information back into the system, not just on the people they target, but on family members, friends and anyone else in the target’s orbit.

In the past two years alone, two of the nation’s largest law enforcement agencies have scrapped similar programs following public outcries and reports documenting serious flaws.

In Pasco, however, the initiative has expanded. Last summer, the Sheriff’s Office announced plans to begin keeping tabs on people who have been repeatedly committed to psychiatric hospitals.

The Times shared its findings with the Sheriff’s Office six weeks before this story published. Nocco declined multiple interview requests.

In statements that spanned more than 30 pages, the agency said it stands behind its program — part of a larger initiative it calls intelligence-led policing. It said other local departments use similar techniques and accused the Times of cherry-picking examples and painting “basic law enforcement functions” as harassment.

The Sheriff’s Office said its program was designed to reduce bias in policing by using objective data. And it provided statistics showing a decline in burglaries, larcenies and auto thefts since the program began in 2011.

“This reduction in property crime has a direct, positive impact on the lives of the citizens of Pasco County and, for that, we will not apologize,” one of the statements said. “Our first and primary mission is to serve and protect our community and the Intelligence Led Policing philosophy assists us in achieving that mission.”

But Pasco’s drop in property crimes was similar to the decline in the seven-largest nearby police jurisdictions. Over the same time period, violent crime increased only in Pasco.

Criminal justice experts said they were stunned by the agency’s practices. They compared the tactics to child abuse, mafia harassment and surveillance that could be expected under an authoritarian regime.

“Morally repugnant,” said Matthew Barge, an expert in police practices and civil rights who oversaw court-ordered agreements to address police misconduct in Cleveland and Baltimore.

“One of the worst manifestations of the intersection of junk science and bad policing — and an absolute absence of common sense and humanity — that I have seen in my career,"" said David Kennedy, a renowned criminologist at the John Jay College of Criminal Justice, whose research on crime prevention is referenced in Pasco’s policies.

The Times’ examination of Pasco’s intelligence program comes amid a national debate over the role of police in society and calls to reduce funding for law enforcement or replace entire departments.

For years, the program’s inner workings have remained largely out of public view, even as Nocco has touted its merits during debates and community forums. Times reporters combed through thousands of pages of documents, watched hours of body-camera footage and spent months obtaining and analyzing the target list, which had not been previously released.

Pasco is an overwhelmingly white county, and the program did not appear to disproportionately target people based on race.

But juvenile offenders, regardless of race, were an outsized priority for the intelligence program, according to former deputies and a Times data analysis.

Of the 20 addresses visited most by its dedicated enforcement teams, more than half were home to middle- or high-schoolers who were identified as targets.

BUILDING THE MACHINE
Nocco took over the Pasco Sheriff’s Office in 2011 when his predecessor retired early and then-Gov. Rick Scott appointed him to finish the term.

Nocco was 35 and a newly promoted major who had joined the Sheriff’s Office two years earlier. He had deep ties to Republican politics but far less experience in law enforcement than the outgoing sheriff.

He quickly rolled out a plan to remake the department that sounded like a pitch for a Hollywood blockbuster: Moneyball meets Minority Report.

The intent was to reduce property crime. The agency, which has 650 sworn law enforcement officers and covers a county of roughly 500,000 residents, would use data to predict where future crimes were likely to take place and who was likely to commit them, Nocco told reporters. Then deputies would find those people and “take them out” — thwarting criminal activity before it happened.

“Instead of being reactive,” he said, “we are going to be proactive.”

He later said the approach was not unlike the way the federal government goes after terrorists.

The Pasco Sheriff’s Office wasn’t the only local law enforcement agency trying to predict crime. The Hillsborough County Sheriff’s Office had already started using crime statistics to pinpoint high-crime areas and identify repeat offenders. The two departments discussed techniques, the Pasco agency said in one of its statements.

The Pasco Sheriff’s Office won a $95,000 federal grant to upgrade its computer systems and hired a small team of civilian analysts. At first, the analysts focused on identifying geographic crime trends and gathering information from people in jail, said former Lt. Brian Prescott, who oversaw the team and retired in 2014.

But Nocco wanted to make proactive strategies and intelligence gathering his agency’s central philosophy. All employees were required to take a two-hour course on intelligence-led policing, Prescott said. Supervisors got additional training.

Nocco referenced the program often as he ran for election for the first time in 2012. Some residents appreciated it so much, he boasted in one campaign appearance, they threw deputies a block party.

He won the race and continued building his intelligence machine.

Today, the Sheriff’s Office has a 30-person intelligence-led policing section with a $2.8 million budget, run by a former senior counterterrorism analyst who was assigned to the National Counterterrorism Center. The No. 2 is a former Army intelligence officer.

Twenty analysts scour police reports, property records, Facebook pages, bank statements and surveillance photos to help deputies across the agency investigate crimes, according to the agency’s latest intelligence-led policing manual.

Since September 2015, they have also decided who goes on the list of people deemed likely to break the law.

The people on the list are what the department calls “prolific offenders.” The manual describes them as individuals who have “taken to a career of crime” and are “not likely to reform.”

Potential prolific offenders are first identified using an algorithm the department invented that gives people scores based on their criminal records. People get points each time they’re arrested, even when the charges are dropped. They get points for merely being a suspect.

The manual says people’s scores are “enhanced” — it does not say by how much — if they miss court dates, violate their probation or appear in five or more police reports, even if they were listed as a witness or the victim.

The Sheriff’s Office told the Times that a computer generates the scores and creates an initial pool of offenders every three months. But the analysts go through the list by hand and make a determination about which 100 people should be on the list.

The analysts also work with the command staff to pick “Top 5” offenders, who are thought to be key players in criminal networks, and “district targets,” who the department has enough evidence to charge with a crime. The manual does not say what criteria they use.

Deputies visit the prolific offenders and the other targets as part of their daily responsibilities.

Nocco described the practice as “bothering criminals” to the Council of Neighborhood Associations in 2012.

The manual describes the goal in aggressive terms.

“If the offender does not feel the pressure, if the offender is not arrested when they commit their next crime, or if the offender is left to feel their punishment is menial,” the manual says, “the strategy will have no impact.”

‘ONE WAY OR ANOTHER’
Inside the agency, keeping the machine humming was a top priority, six former deputies and department leaders told the Times.

“At the end of every shift, they’d want to know how many prolific-offender checks your squad did,” said Chris Starnes, a former lieutenant who oversaw patrol and narcotics units.

Former Capt. James Steffens, who was previously chief of the New Port Richey Police Department, said deputies who didn’t visit enough targets could be removed from special assignments or sent to work in districts far from their homes. Their supervisors could too.

Both Starnes and Steffens resigned from the Sheriff’s Office. Starnes is a plaintiff in an ongoing federal lawsuit that accuses the agency of pushing out employees who criticized specific policies, including the intelligence program. Steffens is also suing the agency, alleging racial discrimination, retaliation and defamation. The Sheriff’s Office denies the claims.

Some deputies — those on Strategic Targeted Area Response teams, or STAR teams — were dedicated to the program’s objectives. Among their assignments: to “hunt down” the targets, according to a post the Sheriff’s Office made on its Facebook page in 2017.

Later in the post, then-STAR team Deputy John Riyad described the allure of being on the team: “I want to go out and find people to arrest so we can prevent those crimes from happening.”

The job included “intensive monitoring,” as the agency’s strategic plan described it. Email reports recount STAR deputies driving by targets’ homes, hunting for intel. They spotted an orange mountain bike outside one young offender’s house and checked to see if any bicycles matching that description had been reported stolen. (None had.) They found another young offender riding his scooter in front of his residence on the county’s east side.

“He has cut his hair, which is now short,” a deputy wrote in an undated report. “He advised after the summer break he will be going to 9th grade at Schwettman (Education Center). He claimed not to be associating with any of his old friends.”

It also involved “directed harassment,” former STAR team Cpl. Royce Rodgers said in an interview with the Times.

Rodgers, who also resigned from the Sheriff’s Office and is a plaintiff in the lawsuit with Starnes, said his captain ordered him to make the contacts aggressive enough that targets would want to move.

Rodgers and his team would show up at people’s homes just to make them uncomfortable, he said. They didn’t always log the contacts in the agency’s official records. He recalled parking five patrol cars outside one target’s home all night and visiting some as many as six times in a single day.

They would do the same to targets’ friends, relatives and other “associates,” he said.

“Those associates might have nothing to do with the offender,” Rodgers said. But as long as the analysts listed them in the system, “we’d harass them too,” he said.

If the targets, their family members or associates wouldn’t speak to deputies or answer questions, STAR team deputies were told to look for code enforcement violations like faded mailbox numbers, a forgotten bag of trash or overgrown grass, Rodgers said.

“We would literally go out there and take a tape measure and measure the grass if somebody didn’t want to cooperate with us,” he said.

Rodgers said people sometimes would fail to pay the fine, which would result in a warrant being issued for their arrest.

“We’d get them one way or another,” he said.

Rodgers said the tactics made him and many of his colleagues uneasy. He thought the strategy was both ineffective and unethical, he said. But when he raised concerns, he said, a supervisor threatened to strip him of his rank and send him back to patrol.

LATE-NIGHT VISITS AND CODE CITATIONS
In interviews with the Times, 21 families targeted by the program described deputies pounding on their doors at all hours of the day and night.

Nearly half said deputies sometimes surrounded their homes, lined their streets with patrol cars or shined flashlights into their windows.

Nine said they were threatened with or received code enforcement citations.

Four said they seriously considered moving. One did.

Two adults whose teenage children were targeted had no complaints about how the Sheriff’s Office had treated their families. Both said they were having trouble with their children and appreciated deputies stepping in. Another father said he was surprised but not bothered that deputies checked on his teenage daughter.

All of the others called the tactics unhelpful or unbearable.

Sheila Smith was among them. Deputies showed up at her home in Land O’ Lakes over and over in 2017 and 2018 looking for her teenage son, even though he was under court-ordered house arrest at his grandmother’s home in Hillsborough County, she said.

Their fifth visit was on Jan. 11, 2018, at 10:32 p.m. Smith stepped outside in a bathrobe and explained the situation. “He’s already under supervision,” she told the deputies politely, according to body-camera video of the encounter. “It’s not necessary for y’all to come here anymore.”

Deputies came by looking for her son at least three more times after that, the dispatch log shows. Another time, they put her husband, Vaughn Sr., in handcuffs and loaded him into the back of a cruiser, she said. They later said they had mistaken him for his brother and let him go.

In one of its statements to the Times, the Sheriff’s Office said that the incident had nothing to do with intelligence-led policing and the deputy had apologized for the confusion. But Vaughn Smith Sr. said the visit had started as so many others had: with the deputy asking about his son.

The Smiths said it was obviously harassment. They called a lawyer and considered moving out of the county, they said. They stayed only because they own their home.

The deputies didn’t only go looking for the targets themselves.

They grilled a 25-year-old woman at the Dunkin’ Donuts where she worked in September 2019 and watched her as she sat outside the building two days later, Sheriff’s Office records show.

The woman had no criminal history beyond traffic offenses. But her boyfriend was a target, and the deputies were trying to find him.

When deputies returned a third time that week, the woman said she and her boyfriend had broken up and complained that the deputies were harassing her, according to their notes. The deputies later confirmed the man they were looking for had left the state with a different woman.

People who were targeted said the checks lasted for months.

Dalanea Taylor was arrested 14 times before turning 17, mostly for burglaries and stealing cars. She went to prison, was released and stopped breaking the law, she said. But deputies kept showing up at her home. They’d ask who she was hanging out with, what she knew about certain people, if she was in a relationship.

Taylor, now 20, wouldn’t answer, she said. It felt inappropriate.

Once, after Taylor posted a photo with a male friend on Facebook, deputies asked about the friend. Later, she said, a deputy followed her in a patrol car as she walked down her street.

When deputies knocked on her door at 7:32 a.m. on New Year’s Day 2018, a family friend implored them to ease up. By then, Taylor had been out of prison for nine months and had not been re-arrested. The deputies said they would not stop monitoring her for a “couple of years,” according to their notes on the conversation.

“She advised she’s staying out of trouble,” they wrote. “She is pregnant and is expecting in June.”

Rio Wojtecki, the 15-year-old who deputies checked on 21 times, said the constant visits made him anxious. One night in January, a few hours after deputies had visited, Rio had trouble breathing and collapsed on the bathroom floor. His mother called an ambulance. Later, an emergency room doctor said anxiety was likely to blame.

In one of its statements to the Times, the Sheriff’s Office said Rio had been named a “Top 5” offender because of his “criminal network and associations.” The agency also said he is in a gang, citing criminal intelligence, but would not elaborate. He and his mother denied the allegation.

Rio wasn’t the only person in the family who felt harassed.

One night, deputies showed up at the house when Rio’s older sisters were home alone. His 19-year-old sister, KayLee, explained that Rio was with their mother at her office and went back inside.

Deputy Thomas Garmon knocked on the window and pounded on the door.

“KayLee!” he yelled, according to his body-camera video. “You’re about to have some issues.”

When she opened the door, Garmon threatened to write her a code enforcement citation for not having numbers posted on the house or mailbox unless she let them search the home for Rio. She insisted there were numbers on the mailbox but ultimately let a deputy in.

A few months later, deputies gave Rio’s mother two tickets: one for not having numbers on her house and one for a broken-down car in the driveway. She had to go to court and pay $100 in fines.

‘HOW CAN WE GET THIS DUDE?’
Many of the visits were polite, according to interviews with the program’s targets and body-camera footage of the interactions. But as deputies came back repeatedly, some of the interactions turned combative — and had serious consequences.

Rodgers, the former STAR team corporal, said he and his team would look for reasons to make arrests. Once, they spotted a teenage target through the window of his home. Another teenager was there, too, smoking a cigarette. Both refused to come outside, and the target’s father, Robert A. Jones III, wouldn’t make them.

“We couldn’t get the kids,” Rodgers recalled. “So we arrested the dad.”

Deputies charged Jones with contributing to the delinquency of a minor and resisting an officer.

The charges were dropped. But nine days later, deputies arrested Jones again, this time for missing a court hearing for a code enforcement citation he said he never received. Deputies arrested Jones a third time less than three months later, saying they found a small amount of marijuana in his house and truck.

“It was like a gang,” Jones told the Times. “They were like, ‘How can we get this dude?’ ”

The new charges against Jones — marijuana possession and child neglect — were also dropped, but not before the Sheriff’s Office posted the details of the arrest on its Facebook page.

Jones moved his family to a motel to get away from the harassment, he said. They later moved to Pinellas County.

Other families had similar experiences.

Deputies went to 14-year-old target Da’Marion Allen’s house before school one day last October to ask about a car theft they thought he was involved in. While they were there, they arrested his 53-year-old grandmother, his 28-year-old uncle and a 20-year-old female relative.

The grandmother, Michelle Dotson, was standing outside when the deputies first arrived. She said she asked them to call Da’Marion’s lawyer. But when Da’Marion came out, she said, one of the deputies tried taking him into custody.

A police report says Dotson grabbed the deputy by his wrist and refused to let go. Dotson denies the allegations. She said the only person she touched was her grandson, who has developmental disabilities and functions at the level of a young child.

Deputies said the 20-year-old relative tried to hit one of them in the head with a decorative vase. Dotson said that when deputies started crowding the foyer, she asked the relative to move the vase so it wouldn’t break.

None of the adults had been arrested before, they said. They all denied touching or threatening any deputies. Their cases are pending.

Tammy Heilman had the Sheriff’s Office policy explained bluntly to her in September 2016.

Earlier in the day, STAR team Deputy Andrew Denbo had stopped by her house asking questions about a dirt bike he thought her 16-year-old son — a Sheriff’s Office target — bought with stolen money. Heilman was taking her 7-year-old daughter to Girl Scouts. She told Denbo she wouldn’t speak without an attorney present and drove off.

Denbo noticed Heilman and her daughter were not wearing seat belts, according to the police report. He told her to stop, then followed her down the street and pulled her over.

In the report, Denbo wrote that he opened Heilman’s car door and ordered her to get out. She stayed put and called 9-1-1, saying a deputy had hurt her and she needed help, body-camera video shows.

Heilman told the Times she was scared and confused. She said her daughter had been wearing a seat belt until Denbo opened the door and the two adults began yelling at each other.

The video shows a group of deputies yanking Heilman from the car.

Heilman was arrested on charges of resisting an officer, battery on a law enforcement officer and providing false information in a prior conversation about the dirt bike. The police report says she scratched and kicked the deputies who arrested her.

Before she was taken to jail, during a conversation captured on the tape, Heilman asked why she had been arrested. “Because I told you to stop back there and you drove away,” Denbo replied.

On the way to jail, he continued: “Here’s the policy of the agency. I’ll explain it to you so it makes sense. If people themselves or people that live at a house are committing crimes and victimizing the community, then the direction we receive from our Sheriff’s Office, from the top down, is to go out there and for every single violation that person commits, to come down and enforce it upon them.”

Two years later, deputies arrested Heilman a second time, after she opened her screen door into a deputy’s chest. Heilman said it wasn’t intentional. She had a child in her arms and said the door sometimes jams. Video shows her angrily shoving the door open, but then holding it open and telling the deputies they could come inside.

Because Heilman was on probation, she wasn’t offered bail. She spent 76 days in jail. When she was offered a plea deal that sentenced her to one-year probation plus time served, she took it.

She wanted to spend Christmas with her children, she said. But the decision had lasting consequences. She is now a convicted felon. In the two years since, she said, she has been unable to find work.

‘EVERYTHING THAT’S WRONG ABOUT POLICING’
Fifteen experts on policing reviewed aspects of Pasco’s program for the Times. Five of them reviewed versions of the program’s manual.

They identified some portions of the program that are based on well-established law enforcement philosophies, including problem-oriented and community policing.

But they also pointed to what they described as serious flaws.

They noted that Pasco’s scoring system awards points based on arrests, which can reflect racially biased policing practices and doesn’t take into account whether charges were dropped or the person was acquitted.

Some experts were concerned that people can get points for having been suspected of a crime. There are no rules for what makes someone a suspect. It can boil down to who they know or how an individual detective investigates, said Sarah Brayne, a sociology professor at the University of Texas at Austin and author of a new book on big-data policing.

Ana Muñiz, a University of California, Irvine criminologist who studies gang databases, noted that the manuals don’t include a way for residents to check if they’ve been targeted or to file an appeal.

The system also lets the Sheriff’s Office collect an extraordinary amount of information on people who may not have committed a crime, said Andrew Guthrie Ferguson, a law professor at American University and national expert in predictive policing.

After reviewing the most recent manual, Ferguson said: “It feels like everything that’s wrong about policing in one document.”

Other experts said the agency’s tactics were unlikely to deter people from breaking the law and added that the program provides little extra help or social services to the people it targets.

The closest it comes is a palm-sized card with a list of 20 local health care providers, nonprofits and government agencies that deputies are supposed to hand out. The cards contain names, addresses, phone numbers and nothing else.

Initially, a half-dozen of the program’s targets said deputies never gave them even that much information. That changed last month. After the Times presented its findings to the Sheriff’s Office, both Rio’s mother and Heilman said deputies came to their houses with printouts of a community resource guide from the local office of the Florida Department of Health.

Ferguson said programs like Pasco’s were popular a decade or so ago. But in recent years, he said, the concept had been largely discredited.

The Los Angeles Police Department used to have a scoring system to identify violent offenders. But critics attacked the program as biased and invasive, and the department’s Inspector General found that half of the 637 people in the database had one or no violent-crime arrests. The department discontinued the program in August 2018.

The Chicago Police Department had its own system that sought to identify people who were likely to be involved in shootings, either as the perpetrator or victim. But the city’s Inspector General found the program was unfair and based on outdated and inaccurate data. The agency quietly ended the program in November 2019.

The Pasco Sheriff’s Office said it developed its scoring system with the help of a top expert. The agency said it created the rubric “in concert with the recommendations of Dr. Jerry Ratcliffe, who we continue to partner with on this program.”

Ratcliffe, a national expert on intelligence-led policing, told the Times he hadn’t spoken with anyone at the Pasco Sheriff’s Office “in years and years.” He said his involvement in the program was limited to a two- or three-day training he provided in 2013.

Told this by the Times, the Sheriff’s Office responded that Ratcliffe’s books are required reading, that a Pasco captain contributed to Ratcliffe’s most recent book and that several members of the agency attended a training Ratcliffe conducted this year in St. Petersburg.

TEENAGERS AS TARGETS
Young people were a major focus of the program, according to records and interviews.

Rodgers, the former STAR team corporal, said his squad “chased almost exclusively juveniles.” Denbo told Heilman, the mother who was arrested twice, that he spent most of his time dealing with kids and their families, according to body-camera footage.

The number of teenagers who were targeted is likely larger than the Times was able to identify.

The agency wouldn’t provide a list that specified when people were added, so the Times started out by excluding anyone who had been arrested after turning 18. That left 88 people. Through interviews, the reporters identified another seven who were targeted as minors and later arrested, raising the total to 10 percent of the list.

About 7.5 percent of people arrested in Pasco County are 17 or younger.

In its statements to the Times, the Sheriff’s Office said the program was designed to address types of property crimes that teenagers often commit. It pointed specifically to a number of auto thefts by young people in neighboring Pinellas County that the Times has reported on extensively.

The statements included an extensive recounting of the criminal records of the juveniles featured in this story. “Just because an individual is 12 does not make him or her incapable of committing crime,” it said of one of the program’s youngest targets.

Kennedy, the John Jay criminologist, called the agency’s tactics “child abuse.”

“There is nothing that justifies terrorizing school kids,” he said.

Other experts pointed to studies showing aggressive policing makes juvenile offenders more likely to reoffend, not less. They said the criminal justice system treats young people more leniently than adults because their brains are not fully developed and they are more likely to be rehabilitated.

The Sheriff’s Office uses juvenile records the same as adult records in its score calculation. Its latest manual encourages deputies to make sure young prolific offenders don’t get the benefits of the juvenile justice system: It recommends they be charged as adults instead of children.

Pasco isn’t the only local law enforcement agency that pays extra attention to young offenders. Several Pinellas County agencies have a joint program to monitor teenagers on court-ordered home detention or probation. But teens must have at least five felony arrests in one year to qualify. It is run in partnership with the state Department of Juvenile Justice and brings social workers and counselors on visits to the teenagers’ homes, Pinellas Sheriff Bob Gualtieri told the Times.

State Department of Juvenile Justice spokeswoman Amanda Slama said her agency had limited knowledge of Pasco’s program and was not involved. She declined to comment further.

Some of the minors who were targeted were especially vulnerable.

Twenty of the targets were 15 or younger when the list was provided to the Times. Two are 13 today, including Jahheen Winters, who has autism and post-traumatic stress disorder from childhood abuse, his mother said.

At least three of the targets had developmental disabilities: Jahheen, Da’Marion and Lorenzo Gary, a 17-year-old with autism and mental health conditions, his mother said. Lorenzo was twice found incompetent to stand trial, meaning he couldn’t be prosecuted because a judge found he didn’t understand the gravity of the charges or the potential penalties.

The targeting was sometimes taking place as troubled teenagers worked to get their lives back on track.

Matthew Lott was put on the prolific offender list at 14. He was arrested at least six times in 2016 and 2017, mostly for breaking into unoccupied homes and cars. Deputies checked on him constantly, his mother recalled, sometimes interrupting family movie nights.

But by 2018, after several months at a residential program for at-risk kids outside of Orlando, Matthew started to turn his life around. He returned to Pasco, earned his GED, got a maintenance job at his church and stayed out of trouble, records show.

Still, deputies showed up at his door. They came one evening that September, when he was supposed to be resting after having his tonsils removed. They came again in October.

“He’s still labeled in our system as a prolific offender, which means he’s going to keep getting checked on,” a deputy told his mom, according to video of the encounter.

Three of Matthew’s close friends said he was afraid the department would find a reason to send him back to jail.

Six weeks after the October visit, Matthew’s body was found behind a vacant building on U.S. 19. His death was ruled a suicide by prescription drug overdose. He had left a short note on his laptop, apologizing and thanking his family and friends.

Matthew’s mother said she wasn’t sure why Matthew took his life. Experts say suicide rarely has a single cause. But two psychologists and a social worker who were not involved in Matthew’s case said the way the deputies treated him could put tremendous psychological pressure on any young person and contribute to a feeling of hopelessness.

Officials at the Department of Juvenile Justice knew Matthew was struggling. They noted in his file at least seven times that he had been cutting himself or had suicidal thoughts.

The Sheriff’s Office acknowledged it had access to a portion of the file that labeled Matthew at risk of suicide. But the department said it would be irresponsible to blame Matthew’s death on its program. It said the program is based on crime data alone, and Matthew qualified.

“Despite our best effort with providing resources, Mr. Lott continued to offend,” the agency said.

Asked what resources it provided, the Sheriff’s Office said it gave Matthew a copy of the resource card, listing 20 other organizations he could turn to for help.",Targeted,https://projects.tampabay.com/projects/2020/investigations/police-pasco-sheriff-targeted/intelligence-led-policing/
"[""Neil Bedi"",""Kathleen McGrory"",""and Jennifer Glenfield""]",2021-07-27,2021-07-27,2020-09-03,2021-07-27,https://s3.amazonaws.com/ledejs/resized/s2020-pasco-ilp/1500/handcuffs.jpg,2015-09-01,0,en,,projects.tampabay.com,"[""Kate Perkins""]","By NEIL BEDI, KATHLEEN McGRORY and JENNIFER GLENFIELD
Photos by DOUGLAS R. CLIFFORD
Times staff
Sept. 3, 2020
Warning: Some of the videos contain explicit language

Over the past five years, nearly 1,000 Pasco County residents have been swept up in the Pasco Sheriff’s Office’s data-driven policing program. The program aims to use analytics to identify people who the department thinks are most likely to commit future crimes.

Deputies create a list and “check on” anyone whose name appears. They knock on doors at all hours of the day and night. They look for reasons to write code enforcement citations or arrest the targets and their friends and family.

“We’re bothering criminals,” Sheriff Chris Nocco said in 2011. “That’s what we do.”

At least 1 in 10 of the program’s targets have been 17 years old or younger. Some had been arrested only once or twice. And the people deputies are bothering are often friends and family members.

Many of the interactions were captured on body-camera footage, obtained by the Tampa Bay Times through public records requests. Here’s what the video shows.

In 2019, 15-year-old Rio Wojtecki was labeled a “Top 5” criminal — a category intended for key players in criminal networks. Deputies started checking on him around the clock.

Rio and his family didn’t understand why. Even the deputies conceded he only had one charge on his record, from when he and some friends stole two motorized bikes. He already had a probation officer checking on him for that.

The deputies told him to stop hanging out with his “bad” friends. The department has told the Times that Rio was in a gang. Rio says he was not.


In 2019, 15-year-old Rio Wojtecki was labeled a “Top 5” criminal — a category intended for key players in criminal networks. Deputies started checking on him around the clock.

Rio and his family didn’t understand why. Even the deputies conceded he only had one charge on his record, from when he and some friends stole two motorized bikes. He already had a probation officer checking on him for that.

The deputies told him to stop hanging out with his “bad” friends. The department has told the Times that Rio was in a gang. Rio says he was not.

When Rio was doing homework at his mother’s office in September 2019, the deputies hassled two of his sisters, who were 19 and 22 at the time. They threatened to write unrelated code citation fines unless the sisters let the deputies search the house for Rio.

Deputies also visited the car dealership where Rio’s mom worked. They even checked his gym. During the coronavirus pandemic, deputies continued to visit and question Rio.

When Sheila Smith’s son began getting into trouble, she sent him to another county to live with his grandmother, hoping to get him away from bad influences. The Sheriff’s Office signed off on the arrangement, she said. But deputies started visiting her and her husband, anyway, asking to check on their son.


In 2019, 15-year-old Rio Wojtecki was labeled a “Top 5” criminal — a category intended for key players in criminal networks. Deputies started checking on him around the clock.

Rio and his family didn’t understand why. Even the deputies conceded he only had one charge on his record, from when he and some friends stole two motorized bikes. He already had a probation officer checking on him for that.

The deputies told him to stop hanging out with his “bad” friends. The department has told the Times that Rio was in a gang. Rio says he was not.

When Rio was doing homework at his mother’s office in September 2019, the deputies hassled two of his sisters, who were 19 and 22 at the time. They threatened to write unrelated code citation fines unless the sisters let the deputies search the house for Rio.

Deputies also visited the car dealership where Rio’s mom worked. They even checked his gym. During the coronavirus pandemic, deputies continued to visit and question Rio.

When Sheila Smith’s son began getting into trouble, she sent him to another county to live with his grandmother, hoping to get him away from bad influences. The Sheriff’s Office signed off on the arrangement, she said. But deputies started visiting her and her husband, anyway, asking to check on their son.

Each time, Smith calmly explained that her son had moved away.

They came again after that, department records show. Smith said there were more checks that weren’t recorded. Once, deputies handcuffed Smith’s husband and put him in the back of their squad car. After some time, they released him. They said they mistook him for his brother.

Families who did not react so patiently could face life-changing consequences.

After multiple visits and more than $2,500 in code enforcement citations, Tammy Heilman told a deputy asking about her son — one of the program’s targets — to call her attorney. Late for her 7-year-old daughter’s Girl Scout meeting, she drove away in a rush. The deputy yelled that they weren’t wearing seat belts. He followed her down the block, pulled her over and arrested her.

Heilman was charged with resisting arrest and battery on an officer. She also was charged with providing false information about her son. Deputies told her family that she was arrested for driving away with the seatbelt violation. On the ride to jail, a deputy said she was arrested because she didn’t stop to speak with him.

Heilman was released on bail. But two years later, while Heilman was fighting the charges in court, the deputies were back at her house.

They charged her with felony battery on a law enforcement officer for hitting the deputy with the screen door. Because she was on probation for the previous arrest, she was ineligible for bail. She stayed in jail for 76 days before finally agreeing to a plea deal so she could be home for Christmas.


In 2019, 15-year-old Rio Wojtecki was labeled a “Top 5” criminal — a category intended for key players in criminal networks. Deputies started checking on him around the clock.

Rio and his family didn’t understand why. Even the deputies conceded he only had one charge on his record, from when he and some friends stole two motorized bikes. He already had a probation officer checking on him for that.

The deputies told him to stop hanging out with his “bad” friends. The department has told the Times that Rio was in a gang. Rio says he was not.

When Rio was doing homework at his mother’s office in September 2019, the deputies hassled two of his sisters, who were 19 and 22 at the time. They threatened to write unrelated code citation fines unless the sisters let the deputies search the house for Rio.

Deputies also visited the car dealership where Rio’s mom worked. They even checked his gym. During the coronavirus pandemic, deputies continued to visit and question Rio.

When Sheila Smith’s son began getting into trouble, she sent him to another county to live with his grandmother, hoping to get him away from bad influences. The Sheriff’s Office signed off on the arrangement, she said. But deputies started visiting her and her husband, anyway, asking to check on their son.

Each time, Smith calmly explained that her son had moved away.

They came again after that, department records show. Smith said there were more checks that weren’t recorded. Once, deputies handcuffed Smith’s husband and put him in the back of their squad car. After some time, they released him. They said they mistook him for his brother.

Families who did not react so patiently could face life-changing consequences.

After multiple visits and more than $2,500 in code enforcement citations, Tammy Heilman told a deputy asking about her son — one of the program’s targets — to call her attorney. Late for her 7-year-old daughter’s Girl Scout meeting, she drove away in a rush. The deputy yelled that they weren’t wearing seat belts. He followed her down the block, pulled her over and arrested her.

Heilman was charged with resisting arrest and battery on an officer. She also was charged with providing false information about her son. Deputies told her family that she was arrested for driving away with the seatbelt violation. On the ride to jail, a deputy said she was arrested because she didn’t stop to speak with him.

Heilman was released on bail. But two years later, while Heilman was fighting the charges in court, the deputies were back at her house.

They charged her with felony battery on a law enforcement officer for hitting the deputy with the screen door. Because she was on probation for the previous arrest, she was ineligible for bail. She stayed in jail for 76 days before finally agreeing to a plea deal so she could be home for Christmas.

Deputies asked to speak with Michelle Dotson’s developmentally disabled grandson, Da’Marion, about a car theft. She asked them to leave and contact his attorney, she said. They waited on the street and when the teenager came outside for school, a deputy stepped toward him. Dotson grabbed her grandson by the wrist. The Sheriff’s Office didn’t provide footage of this part of the encounter, but the police report said Dotson grabbed a deputy and refused to let go. She denies it.


In 2019, 15-year-old Rio Wojtecki was labeled a “Top 5” criminal — a category intended for key players in criminal networks. Deputies started checking on him around the clock.

Rio and his family didn’t understand why. Even the deputies conceded he only had one charge on his record, from when he and some friends stole two motorized bikes. He already had a probation officer checking on him for that.

The deputies told him to stop hanging out with his “bad” friends. The department has told the Times that Rio was in a gang. Rio says he was not.

When Rio was doing homework at his mother’s office in September 2019, the deputies hassled two of his sisters, who were 19 and 22 at the time. They threatened to write unrelated code citation fines unless the sisters let the deputies search the house for Rio.

Deputies also visited the car dealership where Rio’s mom worked. They even checked his gym. During the coronavirus pandemic, deputies continued to visit and question Rio.

When Sheila Smith’s son began getting into trouble, she sent him to another county to live with his grandmother, hoping to get him away from bad influences. The Sheriff’s Office signed off on the arrangement, she said. But deputies started visiting her and her husband, anyway, asking to check on their son.

Each time, Smith calmly explained that her son had moved away.

They came again after that, department records show. Smith said there were more checks that weren’t recorded. Once, deputies handcuffed Smith’s husband and put him in the back of their squad car. After some time, they released him. They said they mistook him for his brother.

Families who did not react so patiently could face life-changing consequences.

After multiple visits and more than $2,500 in code enforcement citations, Tammy Heilman told a deputy asking about her son — one of the program’s targets — to call her attorney. Late for her 7-year-old daughter’s Girl Scout meeting, she drove away in a rush. The deputy yelled that they weren’t wearing seat belts. He followed her down the block, pulled her over and arrested her.

Heilman was charged with resisting arrest and battery on an officer. She also was charged with providing false information about her son. Deputies told her family that she was arrested for driving away with the seatbelt violation. On the ride to jail, a deputy said she was arrested because she didn’t stop to speak with him.

Heilman was released on bail. But two years later, while Heilman was fighting the charges in court, the deputies were back at her house.

They charged her with felony battery on a law enforcement officer for hitting the deputy with the screen door. Because she was on probation for the previous arrest, she was ineligible for bail. She stayed in jail for 76 days before finally agreeing to a plea deal so she could be home for Christmas.

Deputies asked to speak with Michelle Dotson’s developmentally disabled grandson, Da’Marion, about a car theft. She asked them to leave and contact his attorney, she said. They waited on the street and when the teenager came outside for school, a deputy stepped toward him. Dotson grabbed her grandson by the wrist. The Sheriff’s Office didn’t provide footage of this part of the encounter, but the police report said Dotson grabbed a deputy and refused to let go. She denies it.

Deputies arrested Dotson and two other family members who tried to help. One was a 20-year-old relative, who tried to move a decorative vase out of the way. Deputies said they were worried the woman was going to attack them with the vase. The other was Dotson’s son, who deputies said tried pulling them off her. None of them had been arrested before, they said. They all deny the allegations.


In 2019, 15-year-old Rio Wojtecki was labeled a “Top 5” criminal — a category intended for key players in criminal networks. Deputies started checking on him around the clock.

Rio and his family didn’t understand why. Even the deputies conceded he only had one charge on his record, from when he and some friends stole two motorized bikes. He already had a probation officer checking on him for that.

The deputies told him to stop hanging out with his “bad” friends. The department has told the Times that Rio was in a gang. Rio says he was not.

When Rio was doing homework at his mother’s office in September 2019, the deputies hassled two of his sisters, who were 19 and 22 at the time. They threatened to write unrelated code citation fines unless the sisters let the deputies search the house for Rio.

Deputies also visited the car dealership where Rio’s mom worked. They even checked his gym. During the coronavirus pandemic, deputies continued to visit and question Rio.

When Sheila Smith’s son began getting into trouble, she sent him to another county to live with his grandmother, hoping to get him away from bad influences. The Sheriff’s Office signed off on the arrangement, she said. But deputies started visiting her and her husband, anyway, asking to check on their son.

Each time, Smith calmly explained that her son had moved away.

They came again after that, department records show. Smith said there were more checks that weren’t recorded. Once, deputies handcuffed Smith’s husband and put him in the back of their squad car. After some time, they released him. They said they mistook him for his brother.

Families who did not react so patiently could face life-changing consequences.

After multiple visits and more than $2,500 in code enforcement citations, Tammy Heilman told a deputy asking about her son — one of the program’s targets — to call her attorney. Late for her 7-year-old daughter’s Girl Scout meeting, she drove away in a rush. The deputy yelled that they weren’t wearing seat belts. He followed her down the block, pulled her over and arrested her.

Heilman was charged with resisting arrest and battery on an officer. She also was charged with providing false information about her son. Deputies told her family that she was arrested for driving away with the seatbelt violation. On the ride to jail, a deputy said she was arrested because she didn’t stop to speak with him.

Heilman was released on bail. But two years later, while Heilman was fighting the charges in court, the deputies were back at her house.

They charged her with felony battery on a law enforcement officer for hitting the deputy with the screen door. Because she was on probation for the previous arrest, she was ineligible for bail. She stayed in jail for 76 days before finally agreeing to a plea deal so she could be home for Christmas.

Deputies asked to speak with Michelle Dotson’s developmentally disabled grandson, Da’Marion, about a car theft. She asked them to leave and contact his attorney, she said. They waited on the street and when the teenager came outside for school, a deputy stepped toward him. Dotson grabbed her grandson by the wrist. The Sheriff’s Office didn’t provide footage of this part of the encounter, but the police report said Dotson grabbed a deputy and refused to let go. She denies it.

Deputies arrested Dotson and two other family members who tried to help. One was a 20-year-old relative, who tried to move a decorative vase out of the way. Deputies said they were worried the woman was going to attack them with the vase. The other was Dotson’s son, who deputies said tried pulling them off her. None of them had been arrested before, they said. They all deny the allegations.

Multiple deputies detained Da’Marion, even as Dotson explained that he is sensitive to touch. The teenager had a meltdown.


In 2019, 15-year-old Rio Wojtecki was labeled a “Top 5” criminal — a category intended for key players in criminal networks. Deputies started checking on him around the clock.

Rio and his family didn’t understand why. Even the deputies conceded he only had one charge on his record, from when he and some friends stole two motorized bikes. He already had a probation officer checking on him for that.

The deputies told him to stop hanging out with his “bad” friends. The department has told the Times that Rio was in a gang. Rio says he was not.

When Rio was doing homework at his mother’s office in September 2019, the deputies hassled two of his sisters, who were 19 and 22 at the time. They threatened to write unrelated code citation fines unless the sisters let the deputies search the house for Rio.

Deputies also visited the car dealership where Rio’s mom worked. They even checked his gym. During the coronavirus pandemic, deputies continued to visit and question Rio.

When Sheila Smith’s son began getting into trouble, she sent him to another county to live with his grandmother, hoping to get him away from bad influences. The Sheriff’s Office signed off on the arrangement, she said. But deputies started visiting her and her husband, anyway, asking to check on their son.

Each time, Smith calmly explained that her son had moved away.

They came again after that, department records show. Smith said there were more checks that weren’t recorded. Once, deputies handcuffed Smith’s husband and put him in the back of their squad car. After some time, they released him. They said they mistook him for his brother.

Families who did not react so patiently could face life-changing consequences.

After multiple visits and more than $2,500 in code enforcement citations, Tammy Heilman told a deputy asking about her son — one of the program’s targets — to call her attorney. Late for her 7-year-old daughter’s Girl Scout meeting, she drove away in a rush. The deputy yelled that they weren’t wearing seat belts. He followed her down the block, pulled her over and arrested her.

Heilman was charged with resisting arrest and battery on an officer. She also was charged with providing false information about her son. Deputies told her family that she was arrested for driving away with the seatbelt violation. On the ride to jail, a deputy said she was arrested because she didn’t stop to speak with him.

Heilman was released on bail. But two years later, while Heilman was fighting the charges in court, the deputies were back at her house.

They charged her with felony battery on a law enforcement officer for hitting the deputy with the screen door. Because she was on probation for the previous arrest, she was ineligible for bail. She stayed in jail for 76 days before finally agreeing to a plea deal so she could be home for Christmas.

Deputies asked to speak with Michelle Dotson’s developmentally disabled grandson, Da’Marion, about a car theft. She asked them to leave and contact his attorney, she said. They waited on the street and when the teenager came outside for school, a deputy stepped toward him. Dotson grabbed her grandson by the wrist. The Sheriff’s Office didn’t provide footage of this part of the encounter, but the police report said Dotson grabbed a deputy and refused to let go. She denies it.

Deputies arrested Dotson and two other family members who tried to help. One was a 20-year-old relative, who tried to move a decorative vase out of the way. Deputies said they were worried the woman was going to attack them with the vase. The other was Dotson’s son, who deputies said tried pulling them off her. None of them had been arrested before, they said. They all deny the allegations.

Multiple deputies detained Da’Marion, even as Dotson explained that he is sensitive to touch. The teenager had a meltdown.

The Sheriff’s Office called an ambulance for Da’Marion and had him taken for a psychiatric evaluation under Florida’s Baker Act. He was later arrested on auto theft charges.

Months later, Dotson still feels unsafe. “Everywhere we go, they follow us around,” she said. “They sit here on the corner in unmarked cars like we don’t know their faces.”

In the last five years, Pasco County sheriff’s deputies checked on people on the list and their families more than 12,500 times.",How a Florida Sheriff harasses families: Watch the body-cam video,https://projects.tampabay.com/projects/2020/investigations/police-pasco-sheriff-targeted/body-cam-footage/
"[""Nedi Bedi and Kathleen McGrory""]",2021-07-27,2021-07-27,2020-11-19,2021-07-27,https://s3.amazonaws.com/ledejs/resized/s2020-pasco-ilp/600/nocco5.jpg,2015-09-01,0,en,,projects.tampabay.com,"[""Kate Perkins""]","By NEIL BEDI and KATHLEEN McGRORY
Times staff writers
Nov. 19, 2020
The Pasco Sheriff’s Office keeps a secret list of kids it thinks could “fall into a life of crime” based on factors like whether they’ve been abused or gotten a D or an F in school, according to the agency's internal intelligence manual.

The Sheriff’s Office assembles the list by combining the rosters for most middle and high schools in the county with records so sensitive, they’re protected by state and federal law.

School district data shows which children are struggling academically, miss too many classes or are sent to the office for discipline. Records from the state Department of Children and Families flag kids who have witnessed household violence or experienced it themselves.

According to the manual, any one of those factors makes a child more likely to become a criminal.

Four hundred and twenty kids are on the list, the Sheriff’s Office said.

The process largely plays out in secret. The Sheriff’s Office doesn’t tell the kids or their parents about the designation. In an interview, schools superintendent Kurt Browning said he was unaware the Sheriff’s Office was using school data to identify kids who might become criminals. So were the principals of two high schools.

The Department of Children and Families didn’t answer when asked if it knew its data was being fed into such a system.

Sheriff Chris Nocco declined requests to be interviewed, and his agency did not make anyone from its intelligence-led policing or school resource divisions available for comment.

In a series of written statements, the Sheriff’s Office said the list is used only to help the deputies assigned to middle and high schools offer “mentorship” and “resources” to students.

Asked for specifics, it pointed to one program where school resource officers take children fishing and another where they give clothes to kids in need.

Ten experts in law enforcement and student privacy questioned the justification for combing through thousands of students’ education and child-welfare records.

They called the program highly unusual. Many said it was a clear misuse of children’s confidential information that stretched the limits of the law.

“Can you imagine having your kid in that county and they might be on a list that says they may become a criminal?” said Linnette Attai, a consultant who helps companies and schools comply with student privacy laws.

“And you have no way of finding out if they are on that list?”

The Sheriff’s Office said its data sharing practices with the school district date back 20 years and are crucial to keeping campuses safe.

It added that only a juvenile intelligence analyst and the school resource officers have access to the list and the underlying data.

The agency also objected to the characterization of the list as potential future criminals, saying it was also designed to identify students at risk for victimization, truancy, self-harm and substance abuse.

But the intelligence manual — an 82-page document that school resource officers and other deputies are required to read — doesn’t mention those other risks. Instead, in five separate places, it describes efforts to pinpoint kids who are likely to become criminals.

The office could not provide any documents instructing school resource officers to interpret the list another way.

The list of school kids isn’t the agency’s only effort to identify and target people it considers likely to commit crimes. In September, a Tampa Bay Times investigation revealed that the department’s intelligence arm also uses people’s criminal histories and social networks to predict if they will break the law.

The Sheriff’s Office pursues those people even when there’s no evidence of a new crime. Former deputies told the Times they were ordered to harass people on the target list by visiting their homes repeatedly and looking for reasons to write tickets and make arrests. One in 10 of the people targeted have been teenagers.

The ways the agency has extended its intelligence effort into mining education and child-welfare records have not previously been reported.

Because the children themselves don’t know if they’ve been flagged, it is difficult to say how it affects interactions between students and school resource officers or other deputies. The Sheriff’s Office declined to release a copy of its list of students to the Times.

When a reporter described the effort to Browning, he said he did not find it concerning.

“We have an agreement with the Sheriff’s Office,” the superintendent said. “The agreement requires them to use (the data) for official law enforcement purposes. I have to assume that’s exactly what they are using it for.”

Later, in a written statement, he added: “If there is any need to revisit any aspect of our relationship, we will do so in a thoughtful manner with the goal of keeping our students and staff safe.”

Two members of the Pasco School Board, Megan Harding and Alison Crumbley, described the district’s relationship with the Sheriff’s Office as strong and referenced safeguards to protect students’ privacy. The agreement between the two institutions says the Sheriff’s Office must keep the records confidential and use them in legal ways.

The three other School Board members did not return calls or declined to comment.

Experts said having school resource officers single out children could be harmful, especially if the kids were struggling at home or in school, or if they didn’t trust police.

They also said the effort was based on flawed science and likely biased against children of color and children with disabilities.

“It is a recipe for violating people’s rights and civil liberties,” said Harold Jordan, a senior policy advocate for the American Civil Liberties Union of Pennsylvania.

Elsewhere in the country, scandals have erupted when law enforcement agencies were found to have access to children’s private data, said Andrew Guthrie Ferguson, a law professor at American University and national expert in predictive policing.

Sensitive information about kids, Ferguson said, should remain “in the hands of people who can offer help.”

“Police are not in the business of offering help to juveniles,” he said. “They are in the business of policing.”

BAD GRADES AND CHILDHOOD TRAUMA
In its intelligence manual, the Pasco Sheriff’s Office says most police departments have no way of knowing if kids have “low intelligence” or come from “broken homes” — factors that can predict whether they’ll break the law.

“Fortunately,” it continues, “these records are available to us.”

The manual says the Sheriff’s Office has access to the information through partnerships with the Pasco school district and the state Department of Children and Families.

The district pays the sheriff $2.3 million annually to place 32 deputies in middle and high schools. It also provides access to its Early Warning System, which tracks all students’ grades, attendance and behavior.

Separately, the Department of Children and Families allows law enforcement agencies across Florida to use to its child welfare database so they can investigate child abuse and find missing children. The database, known as the Florida Safe Families Network, contains detailed case notes and kids’ abuse histories.

The Sheriff’s Office has its own records, too, which indicate if children have been the subject of custody disputes, have run away from home, have violated the county’s curfew for young people or have been caught with drugs or alcohol. The office also keeps track of who is friends with whom.

It feeds information from all three datasets into a system that scores kids in 16 different categories. In each, children are assigned one of four labels: on track, at risk, off track or critical.

It doesn’t take much to be designated “at risk.”

Getting a D on your report card is enough, the manual says. So is missing school three or more times in a quarter.

Kids are also labeled “at risk” if they’ve experienced a childhood trauma. That includes witnessing household violence, being the victim of abuse or neglect, or having a parent go to prison.

Internal emails show the list was last updated in October for the new school year.

The agency said it only looks at data in schools where it provides school resource officers — the vast majority of the district’s middle and high schools. In total, those schools have more than 30,000 students.

Elementary schools have armed security guards and are not included, the agency said.

The Sheriff’s Office has been identifying and monitoring at-risk children as part of its intelligence operation since at least 2011, when Nocco first became sheriff.

That summer, school resource officers made hundreds of home visits to at-risk kids, according to news reports. They offered support to the children and their families, they told reporters at the time. But they also questioned them about local crimes and arrested kids who violated probation or curfew orders.

In one of its statements to the Times, the Sheriff’s Office said it looks for alternatives to arrest “when possible” and that supporting struggling kids is an important part of any school resource officer’s job.

Internal documents, however, show that those officers do more than mentor.

The intelligence manual encourages them to work their relationships with students to find “the seeds of criminal activity” and to collect information that can help with investigations.

“Often SROs will hear about past, present or future crimes well before others in the law enforcement community,” the manual says.

One school resource officer’s annual performance review, obtained by the Times through a public records request, noted he contributed to intelligence briefings. It also praised him for filing nearly two dozen “field interview reports” based on interactions with at-risk kids.

A ‘CIRCULAR EFFECT’
Law enforcement and privacy experts found many aspects of the Sheriff’s Office’s formula for identifying kids alarming.

Some metrics, they said, were completely outside of kids’ control. Others were likely biased.

Take school discipline.

In Pasco County, Black students and students with disabilities are twice as likely to be suspended or referred to law enforcement, according to federal data.

Bacardi Jackson, a senior supervising attorney for children’s rights for the Southern Poverty Law Center, said designating those kids as potential criminals could have a “circular effect.” They would likely receive even more attention from school resource officers and as a result, face additional discipline.

Leah Plunkett, an expert in digital privacy law and lecturer at Harvard Law School, said the program also appeared to be discriminatory.

Singling out kids based on whether they had been involved in custody disputes, for example, could be considered differential treatment based on family status, she said.

The Sheriff’s Office says its program is based on research. It points to a 2015 study that found young people who had experienced multiple childhood traumas were at a higher risk of becoming serious, violent criminals than those who hadn’t.

But David Kennedy, a renowned criminologist and professor at the John Jay College of Criminal Justice whose research is referenced in Pasco’s manual, said the associations between childhood trauma and criminal behavior are “extremely weak.” He said using them to make predictions about individuals “flies in the face of the science.”

The methodology used by the Sheriff’s Office, he added, was likely to generate a large pool of children, the vast majority of whom would never get in serious trouble.

“There’s nothing — absolutely nothing — that can be fed into even the most sophisticated algorithm or risk-assessment tool based on information available when someone is a child that can say this person is going to be a criminal later on, much less a serious prolific criminal,” he said.

LEGAL CONCERNS
After the Times started asking questions about the use of data to target young people, the Sheriff’s Office seems to have started revamping elements of its program.

Emails show the agency has been drafting a policy for how school resource officers should interact with at-risk students that focuses more on offering support and building positive relationships.

Even with those changes, experts questioned whether using student data this way was legal.

Under federal law, education records can only be released to outside parties in certain circumstances.

Law enforcement agencies can use the information to help thwart school shootings and offer support to students who are in the juvenile justice system. But in cases like those, experts said, they can only look at records relating to a specific student or situation.

“You can’t just give every student’s record out,” said LeRoy Rooker, who led the federal Department of Education’s oversight of student privacy for more than two decades.

The law does say school resource officers can access education records because they can be considered “school officials.” But under most circumstances, they can’t share the records with the rest of the department, said Amelia Vance, a member of the Maryland Department of Education’s Student Data Privacy Council who works for the nonprofit Future of Privacy Forum.

And they can’t use them in a law enforcement investigation without permission from a parent, unless there is a court order or a health and safety emergency, Vance said.

In its statement, the Sheriff’s Office said it had access to the data “lawfully” and noted that school resource officers receive annual training on the federal student privacy law.

The school district has recently been criticized for lax privacy practices. Last year, a state audit found that too many district employees had access to current and former students’ sensitive data, including social security numbers. The district later revoked privileges for 570 employees.

Attai, the student privacy consultant, said the school district should take action in this case, too, and reconsider its arrangement with the Sheriff’s Office.

“This is a district that is sending millions of dollars to the sheriff of Pasco County to target its students as criminals,” she said.",Pasco’s sheriff uses grades and abuse histories to label schoolchildren potential criminals. The kids and their parents don’t know.,https://projects.tampabay.com/projects/2020/investigations/police-pasco-sheriff-targeted/school-data/
"[""Peter Schorsch""]",2021-07-27,2021-07-27,2020-09-11,2021-07-27,https://s3.amazonaws.com/ledejs/resized/s2020-pasco-ilp/480/heilman4.jpg,2015-09-01,0,en,,floridapolitics.com,"[""Kate Perkins""]","Office says it's using data, not reading tea leaves.
The Pasco County Sheriff’s Office is pushing back against a recent Tampa Bay Times report that casts its “Intelligence-Led Policing” model as a bludgeon used to harass residents.

The Times report says the ILP system implemented by Pasco County Sheriff Chris Nocco employs “arrest histories, unspecified intelligence and arbitrary decisions by police analysts” to single out and pick on Pasco County residents.

But the Sheriff’s Office says the Times went one for three on the input and struck out entirely on how the department uses the output.

The Sheriff’s Office says Times is conflating Intelligence-Led Policing, a decades old method used by law enforcement agencies across the globe, with “Predictive Policing,” a term for guessing what crimes will be committed and by whom.

Some law enforcement agencies have experimented with Predictive Policing, but the Pasco County Sheriff’s Office says it does not — and has not. ILP, it says, is an entirely different method.

“Contrary to the report, ILP is not a futuristic, predictive model where people are arrested for crimes they have not yet committed. Instead, the system is based SOLELY on an individual’s criminal history. Multiple studies have shown that 6% of the population commit 60% of the crime and that is what this model focuses on,” the office wrote in a lengthy rebuttal posted on its Facebook page.

“Let us, again, be profusely clear that this model is based SOLELY on an individual’s criminal history. It is nameless, faceless, ageless, genderless and removes ALL identifying factors of an individual, EXCEPT for their criminal history. This philosophy removes any chance of bias in law enforcement, which is something that should be celebrated.”

The Sheriff’s Office also takes issue with other aspects of the article, namely the ex-officers who spoke with the Times.

All resigned or were dismissed for cause, the office says.

One was under investigation for engaging in sexual relations with a confidential informant; another had numerous, documented accounts of insubordination and failing to follow proper policies and procedures; and a third abandoned an extra duty detail without permission and went to a female subordinate’s personal residence while she was off duty without invitation.

But the Sheriff’s Office took particular umbrage with the Times insinuations of racial bias.

As highlighted in a recent report from WTSP, the Pasco County Sheriff’s Office is the most diverse in the Tampa Bay region and among the most diverse in the state. WTSP found nearly one in four full-time officers at the Pasco Sheriff’s Office are Black — more than double the statewide average of 11%.","Pasco Sheriff’s Office pushes back against allegations of harassment, targeting",https://floridapolitics.com/archives/366255-pasco-sheriffs-office-pushes-back-against-allegations-of-harassment-targeting/
"[""Kathleen McGrory and Neil Bedi""]",2021-07-27,2021-07-27,2020-12-24,2021-07-27,https://s3.amazonaws.com/ledejs/resized/s2020-pasco-ilp/480/nocco-scott.jpg,2015-09-01,0,en,,projects.tampabay.com,"[""Kate Perkins""]","Pasco Sheriff Chris Nocco built a controversial data-driven approach to policing. He also built a wide circle of powerful friends who don’t question his tactics.

Pasco Sheriff Chris Nocco was once asked under oath how he had landed two high-level posts in state government.

“It was the connections that I had made,” he said bluntly.

“I mean, you didn't have to like go and interview along with a hundred other people to get the job or anything?” an attorney pressed.

“No, ma’am,” he said.

Later that month, he told a reporter: “I’m very blessed to have friends that are in high offices.”

Today, Nocco himself is in a high place. A force in local GOP politics, he has twice been elected sheriff without opposition — something that hadn’t happened in Pasco County since World War II. His wife is one of the state’s most prominent Republican fundraisers. Their ties have reached the highest levels of government, including President Donald Trump’s administration.

Since becoming sheriff a decade ago, Nocco has used his connections and clout to grow the department and expand its reach.

He’s also taken the agency in directions that have appalled experienced cops and nationally recognized law enforcement experts.

A Tampa Bay Times investigation in September found that Nocco’s signature initiative — a sprawling intelligence program — uses an algorithm to identify people who might break the law, based on their criminal histories and social networks. The agency sends deputies to their homes, even if there is no evidence of a crime. Former deputies told the Times they had been ordered to make targets’ lives miserable.

The Times later revealed that the Sheriff’s Office starts trying to predict future criminals even earlier in life. It keeps a list of schoolchildren who might “fall into a life of crime” that’s built with data such as grades and child welfare records, agency documents show.

The department says the list currently includes more than 400 kids and is only used by school resource officers to provide support and “mentorship.” The kids and their parents are not made aware of the designation.

National experts who reviewed the programs for the Times called them “morally repugnant” and “everything that’s wrong about policing.” Some civil liberties groups — including the ACLU of Florida, Southern Poverty Law Center and Institute for Justice — are considering lawsuits and public advocacy campaigns. Tens of thousands of people have signed a petition demanding the school district stop sharing sensitive student data with the Sheriff’s Office.

Yet leaders in the Republican county and statehouse have been reluctant to weigh in. More than a dozen elected officials did not return phone calls or declined to be interviewed for this report.

Nocco became Pasco’s sheriff at 35, with eight years of law-enforcement experience. He set out to remake the agency: building an intelligence arm, hiring retired military officers and giving deputies a new and confrontational rallying cry, “We fight as one.”

The Sheriff’s Office takes pride in those efforts and touts its reliance on data, early adoption of body cameras, and recent work to address mental health and create a cutting-edge research institute as proof of its forward-thinking attitude.

But it has also faced criticism that its deputies have become too aggressive and its leadership too reliant on misapplied intelligence tactics.

Internally, the department has broken into open conflict. Some deputies have resisted the changes and been pushed out amid what they call retaliation. The schism has led to multiple lawsuits, including one that described agency leaders as “intoxicated with power” and willing to “physically abuse, intimidate, incarcerate, extort, and defame in order to ensure their absolute control.”

The Sheriff’s Office has denied the claims.

Nocco (pronounced knock-oh) declined to be interviewed for this story or any others related to the Times’ reporting on his intelligence programs. Instead, the Sheriff’s Office provided a copy of his biography and a 53-page report on how the agency has innovated during his tenure. Nine pages are devoted to intelligence programs. Seven highlight the agency’s partnership with the schools and its programs for kids.

“I am extremely proud of the bond we have created at the Pasco Sheriff's Office with our community over the last 10 years,” Nocco said in a statement. “This bond has allowed us to best serve our community, reducing crime as our population has grown rapidly and focusing on serving the needs of our community.”

In previous memos to the Times, the Sheriff’s Office has said it won’t back down from its intelligence strategies. It accused the news organization of “yellow journalism” and bias against law enforcement.

Even though sheriffs nationwide operate with vast autonomy and little oversight, Nocco stands out as particularly powerful.

His supporters say he’s a born leader whose humble attitude and natural charisma have made him popular among rank-and-file deputies and Pasco residents alike.

“People like him,” said longtime friend Mike Fasano, now the county tax collector. “It’s because of the way he comes across as a likeable person who cares about our community and keeping our community safe.”

Then, there are his connections.

He served as a top aide to Marco Rubio, now Florida’s senior U.S. senator.

His wife, Bridget, helped spearhead fundraising for Rick Scott, the state’s former governor and now its junior U.S. senator.

She’s been the finance director for one of the most important lobbying and public relations firms in the country. Her boss, Brian Ballard, has a long-standing relationship with the president. In 2018, Politico called Ballard the most powerful lobbyist in Trump’s Washington.

That proximity to the upper rungs of the Republican Party has made Nocco untouchable, said Bill Dumas, president of a local organization called Citizens Against Discrimination and Social Injustice.

“He does whatever he wants and there’s no arguing with him — or even sitting down and reasoning with him,” Dumas said.

A POLITICAL STAR
In reliably Republican, pro-cop Pasco County, people know Chris Nocco.

He’s the plain-spoken, tough-on-crime lawman with a Philly accent. The former college football player and father of three who tweets Bible verses and motivational quotes.

He wears a patrolman’s uniform, not a suit and tie like some of his predecessors, and he drives the same type of car as his deputies, Fasano said.

Politicians running for office vie for his endorsement. He typically gets what he wants from the County Commission without a fight.

Nocco, 44, wasn’t always on the fast track. After earning his master’s in public administration from the University of Delaware, he spent the first few years of his career hopscotching from one police agency to another.

His first job, as a Delaware State Police trainee, lasted eight months. He resigned because a lieutenant made “derogatory” remarks about his family and faith, he said during a 2011 deposition.

His next stint, with the Philadelphia Public Schools Police, lasted six months.

He went to the Fairfax County Police Department in northern Virginia and stayed almost 3.5 years. Then he moved to Florida, following Bridget back to her home state, where she had a job opportunity.

He quit his next job, as a patrol deputy with the Broward Sheriff’s Office, after ten months. He said during the deposition that he didn’t want to work for Sheriff Ken Jenne, a powerful Democrat who would later plead guilty to fraud and tax evasion charges.

Nocco said he believed Jenne’s refusal to pay overtime for units like the SWAT team had put lives in danger. “Even as a patrol officer or deputy on the road, you knew he was just not a good sheriff to work for,” he said.

From there, Bridget helped him segue into politics. Just two months before the 2004 election, he became field director for Republican President George W. Bush’s campaign in Broward County. (The liberal county went overwhelmingly for Bush’s Democratic challenger, John Kerry.)

Nocco spent the next four years as an aide in the Florida House of Representatives, working his way up to deputy chief of staff for Rubio, who was then House speaker.

He forged close ties with many members of Pasco’s political class. That included Fasano, then an influential state senator; Rep. Will Weatherford, who would later ascend to House speaker; and Pasco Sheriff Bob White, who travelled to Tallahassee with the Florida Sheriffs Association.

Nocco’s immediate supervisor, Richard Corcoran, was also from Pasco and would later be elected to the House and serve as speaker. Corcoran is now state education commissioner.

As Rubio’s tenure in the state House was coming to an end, Nocco landed a $100,000-a-year civilian position with the Florida Highway Patrol.

Seven months later, White asked him to be a captain overseeing administrative areas for the Pasco Sheriff’s Office.

In a recent interview, White said he was looking for candidates with leadership potential. He described Nocco as tireless and intelligent. “He had no enemies in Pasco County,” White said. “He seemed like the right fit.”

Bridget’s career had also taken off. She had lobbied for powerful business interests, including HCA Healthcare and U.S. Sugar. And in 2010, she played a key role in the fundraising that helped land Scott, then an unknown businessman, in the governor’s mansion.

She did not return calls or emails from the Times.

In his statement, Nocco said: “I am very proud of my wife and the career she has built for herself.”

In the spring of 2011, White announced that he would be retiring before his term ended to spend more time with his family. As governor, Scott got to choose his replacement.

Some observers speculated White retired early so Republicans could handpick Pasco’s next sheriff. White told the Times that was just “a conspiracy.” Nocco has said he was unaware of White’s plans and came to Pasco because he missed being a sworn officer.

Regardless, just two years into his tenure at the Sheriff’s Office, Nocco was named the county’s top law enforcement officer.

Since then, his connections have continued paying off.

In 2012, Weatherford helped Nocco secure an additional $1 million in state funding for Pasco's child protective investigators while money for other counties stayed flat.

When Corcoran became speaker, he appointed Nocco to a high-profile board tasked with proposing updates to the state Constitution. Nocco used the post to introduce Marsy’s Law — a victim’s rights measure that police agencies, including his own, have employed to withhold the names of officers who use force.

His national profile only grew after 2016 when Ballard, already a powerful lobbyist in Florida, became influential in Washington.

This spring, Nocco was named to a 32-member panel that advises the secretary of Homeland Security on federal policy.

In July, he had a speaking role at a Trump re-election rally at Tampa International Airport that was carried on TV.

Ballard called Nocco a “political star.”

“Wherever he’s gone, he’s made friends, he’s served well and people think highly of him,” Ballard said.

Even after the Trump administration ends, Nocco will have powerful friends in Washington. Rubio is acting chairman of the Senate Select Committee on Intelligence and sits on the Appropriations and Foreign Relations committees. Scott serves on the Committee on Homeland Security and Governmental Affairs.

Both are considered 2024 presidential contenders.

‘WE FIGHT AS ONE’
Despite his limited experience in law enforcement, Nocco didn’t hesitate to reimagine the department after becoming sheriff.

The intelligence machine was just one piece of his vision.

He had the new motto — we fight as one — emblazoned outside headquarters, on police vehicles and on the patches on deputies’ uniforms.

He reshaped the command staff, making a series of hires with specialized military backgrounds, including a retired Army Special Forces colonel, a former Air Force intelligence analyst and a retired Navy captain who led SEAL teams and worked at U.S. Special Operations Command.

And he made a concerted push into code enforcement, doubling the number of tickets deputies gave for violations like overgrown grass, a Times analysis found.

The office also built a national following online. Today, it posts nearly every daylight hour, using a 26-page social media plan and hashtag glossary. The agency’s hundreds of thousands of followers get safety tips, cute dog pictures and nightly reminders to lock their homes at 9 p.m.

Some of the strategies have been more controversial. In 2016, the Sheriff’s Office made headlines for posting a photo of a man mid-arrest to its social media accounts under the heading “SAD CRIMINAL OF THE DAY.” Critics accused the department of ridiculing someone who had not been convicted of a crime and marginalizing people of color.

“This criminal is not different than any criminal we post about every single day,” a spokeswoman said of the man, who was photographed in tears as two deputies grabbed his dreadlocks. “He was a threat to the community before. It's important the community know he is in custody and no longer a threat to them.”

In 2017, Nocco allowed the agency to be featured on the popular A&E reality series Live PD. Some deputies became overnight celebrities. But like its predecessor COPS, the show was criticized for glorifying aggressive policing.

Pasco ended its run on Live PD in 2019 after two seasons. The show was cancelled this year amid civil unrest over police brutality.

Over time, the Sheriff’s Office has grown. Although the number of sworn law-enforcement officers has increased about 15 percent since Nocco took office, the agency’s budget has risen 65 percent, to $142 million. (The department says its per-citizen cost of $294 still pales in comparison to the nearby Hillsborough and Pinellas sheriff’s offices.)

Nocco says property crime has gone down. But the reduction — which his office credits to its intelligence efforts — is similar to the decline in the seven-largest nearby police jurisdictions. Violent crime has gone up only in Pasco.

For years, community activists have raised concerns about what they consider to be over-aggressive policing.

Most recently, in June, a 2014 incident in which deputies shot and killed an unarmed Black man received new scrutiny when the Times obtained body-camera footage that contradicted the agency’s justification for opening fire.

That same month, Marques Johnson, a hip-hop artist who performs under the name Andre Roxx and is Black, filed a federal lawsuit alleging deputies violated his civil and constitutional rights by arresting him without probable cause in 2018. A Live PD crew was in tow, according to the suit.

At the time of his arrest, Johnson was a passenger in a car driven by his father. Deputies stopped the vehicle because it had an obstructed license plate. When Johnson declined to identify himself, citing his Fourth Amendment rights, the deputies arrested him for resisting an officer and obstructing without violence. A judge later dropped the charge.

“The default approach at that agency is an aggressive, hostile approach,” said Johnson’s lawyer, Ryan Barack.

The Sheriff’s Office is seeking to have the suit dismissed. It has said that it determined the deputies acted appropriately.

Internally, deep divisions have opened up within the department.

In April 2019, two former deputies and a civilian manager sued Nocco in federal court, alleging they had faced retaliation after reporting misconduct. Since then, another 16 former deputies have sued with additional claims of corruption.

The department has fought the allegations publicly and accused the plaintiffs of protocol violations ranging from insubordination to having sexual relations with a confidential informant.

Some of the allegations and counter-allegations are near-impossible to unravel and cast both the plaintiffs and the Sheriff’s Office in a negative light.

But some are jarring, coming from high-ranking officials who left the agency.

Former Intelligence Led Policing Manager Anthony Pearn, who holds a doctorate and once worked for the U.S. Department of Homeland Security, described in court papers a case in which agency leaders used the intelligence apparatus to try to arrest someone who posted a mugshot of a deputy on Facebook.

The Times reviewed the Internal Affairs investigation into the incident, which found that the intelligence arm looked into a woman — a known white supremacist — and her family after she posted an old booking mugshot and address of a deputy who had been featured on Live PD.

Agency leaders told Internal Affairs that they did subsequently target the woman and her family. But they said it was part of a separate drug investigation and called the timing coincidental. Internal Affairs determined there had been no wrongdoing.

Pearn declined to comment.

Former Capt. James Steffens, who Nocco wooed to the Sheriff’s Office from his previous post as New Port Richey police chief, said that he and most other command staff members were required to make big donations to the sheriff’s 2016 campaign: $1,000 in their own names and $1,000 in their spouses’.

The Sheriff’s Office said contributions were not mandatory. Campaign finance records from 2016 show that 13 out of 16 of Nocco’s captains, majors and colonels — including Steffens — gave $1,000, the maximum contribution in a countywide race. Many of their spouses also contributed $1,000. The pattern continued in 2020. In both years, Nocco was unopposed and gave the money he didn’t spend to charities or the Sheriff’s Office.

Steffens, a biracial Black man, also has a lawsuit against the department alleging he faced systemic racial discrimination at the agency. It claims Nocco forced Steffens’ resignation, then used the media to humiliate and defame him.

The agency has said that it accepted Steffens’ resignation over leadership lapses related to charges that one of his deputies tampered with evidence.

DECLINING TO COMMENT

For years, Nocco has touted his intelligence operation at community meetings and in campaign materials.

But when the details of how it actually worked became public, national experts were stunned. Two academics who the Sheriff’s Office said helped develop the program each disclaimed any responsibility for it. Others likened the tactics to harassment, child abuse and policing that could be expected under an authoritarian regime.

Meanwhile, state and county leaders — virtually all Republicans — have avoided saying anything of substance about the program.

Pasco County Commissioner Jack Mariano said he hadn’t read the Times’ investigation but fully backed the Sheriff’s Office. Commissioner Ron Oakley said he also hadn’t read the investigation and declined to comment.

The three other people on the commission in September — Mike Moore, Kathryn Starkey and Mike Wells Jr. — didn’t return reporters’ calls.

Neither did Christina Fitzpatrick, who joined in November.

Nor did the state’s legislative leaders: Senate President Wilton Simpson, who represents parts of Pasco, and House Speaker Chris Sprowls, a former prosecutor in the judicial circuit that covers the county.

Pinellas-Pasco State Attorney Bernie McCabe also didn’t return calls.

Asked about deputies harassing some of his clients, outgoing Public Defender Bob Dillinger said none had mentioned it.

Attorney General Ashley Moody’s office said it does not oversee independently elected sheriffs but noted that the governor could assign the Florida Department of Law Enforcement to review the program. The Florida Department of Law Enforcement said it does not have oversight over local agencies. Gov. Ron DeSantis did not answer questions.

U.S. Rep. Gus Bilirakis, who represents all of Pasco County, didn’t have time to read the Times’ investigation, his spokeswoman said in September. Asked again in December, he said in a statement that he “didn’t pretend to know enough about Pasco’s intelligence-led policing program.” He added that Nocco is “a good man and dedicated public servant.”

When the Times revealed the Sheriff’s Office uses confidential school-district data to identify kids at risk of becoming future criminals, schools Superintendent Kurt Browning called the sheriff “a great partner.” Two board members, elected in nonpartisan races, praised the district’s relationship with the Sheriff’s Office.

Browning — a Republican with a long history in state politics — later defended the school district’s practice of sharing its data with law enforcement, saying it helps keep schools safe. He didn’t address experts’ concerns that the arrangement could violate federal law, didn’t answer teachers’ questions about why the program was allowed to operate in secret and implied the Times’ reporting was misleading.

The only contrary view came from state Sen. Jeff Brandes, a St. Petersburg Republican and chairman of the Judiciary Committee. He asked on his Facebook page if children’s school grades are covered by the Fourth Amendment. “If not, they should be!” he wrote.

The Sheriff’s Office remains defiant.

“We will not apologize for continuing to keep our community safe,” it wrote on Facebook in September, after the Times' initial report.

“Misinformation has recently been shared about these important programs and facts matter,” it posted in December, after Pasco parents and teachers began demanding a review of its use of student data.

On his personal Twitter feed, Nocco has continued projecting tranquility.

On Dec. 7: “Stay firm.”

On Dec. 14: “You will never influence the world by trying to be like it.”

On Dec. 17: “As long as you know God is for you, it doesn’t matter who is against you.”",The man behind the machine,https://projects.tampabay.com/projects/2020/investigations/police-pasco-sheriff-targeted/chris-nocco/
"[""Pasco Sherriff Office""]",2021-07-27,2021-07-27,2020-09-08,2021-07-27,https://www.pascosheriff.com/assets/images/pascologo.png,2015-09-01,0,en,,www.facebook.com,"[""Kate Perkins""]","We are aware of a recent report by a media outlet in the Tampa Bay area that attempts to paint our Intelligence-Led Policing philosophy in a negative light. 
While the media outlet was provided over 30 pages of factual information to disprove their theory of so-called “harassment”, it was extremely disappointing to see this fact-based information take a backseat to supposed “reliable sources”, who have lengthy criminal histories, or who are former members of the Pasco Sheriff’s Office that were held accountable for their actions.
For example, the “reliable sources” named in this report who were held accountable for their actions by the Pasco Sheriff’s Office engaged in conduct such as:
* One “reliable source” quoted by the article, leading up to their resignation from the agency, was under investigation for engaging in sexual relations with a confidential informant
* One “reliable source” quoted by the article had numerous, documented accounts of insubordination and failing to follow proper policies and procedures of the Pasco Sheriff’s Office
* One “reliable source” quoted by the article abandoned an extra duty detail without permission and went to a female subordinate’s personal residence while she was off duty without invitation
Sadly, one of these supposed “reliable sources” admitted in the article that he did not follow proper policies and procedures by not documenting prolific offender checks, though we are unable to verify this information. This failure to follow policy and procedure is exactly why the “reliable source” is no longer with the Pasco Sheriff’s Office, along with several other policy violations.
It is dismaying to see the media demand accountability in law enforcement, something the Pasco Sheriff’s Office wholly supports and endorses as evidenced by our use of Body Worn Cameras as a small example, only to then use those who were held accountable as “reliable sources” with no consideration of their past actions or what led them to be held accountable. 
Let us be abundantly clear, the Intelligence-Led Policing philosophy works and has worked around the Tampa Bay area, as the same philosophy is used by numerous law enforcement agencies. It works because it reduced residential burglaries in Pasco County by 74.4% since it was implemented in 2011 and it works because property crimes are down 35.6% in that same time period. In fact, ILP was started in England in the 1990s and is hardly a new phenomenon or approach to law enforcement.
Contrary to the report, ILP is not a futuristic, predictive model where people are arrested for crimes they have not yet committed. Instead, the system is based SOLELY on an individual’s criminal history. Multiple studies have shown that 6% of the population commit 60% of the crime and that is what this model focuses on. 
Let us, again, be profusely clear that this model is based SOLELY on an individual’s criminal history. It is nameless, faceless, ageless, genderless and removes ALL identifying factors of an individual, EXCEPT for their criminal history. This philosophy removes any chance of bias in law enforcement, which is something that should be celebrated.
For example, an individual who was arrested 15 times in the last three years is more likely to commit an additional crime, as evidenced by a documented propensity for committing crime, than someone who has never been arrested. As such, the Pasco Sheriff’s Office visits and provides resources to the person who was arrested 15 times to attempt to break the cycle of recidivism and better protect our community from crime. 
This is not a judgment on an individual with a criminal history but, instead, a fact that cannot be ignored as we attempt to positively impact the criminal environment in Pasco County and keep our community safe.
That is what the ILP philosophy is, not a futuristic, crime predictive tool that arrests an individual for crimes not yet committed, despite the media report’s best assertion that the theory was taken straight out of fictional Hollywood movies like “Minority Report.”
Additionally, this media outlet appears to labor under the notion that both our ILP Analysts and STAR deputies exist exclusively to coordinate aspects of the Top 5 lists and Prolific Offender lists. In reality, these aspects are but a small subset of each parties’ job functions. As was provided to this outlet numerous times, ILP analysts work on nearly every case the Pasco Sheriff’s Office investigates, with the underlying, yet ever-present, goal of bettering the lives of the citizens of Pasco County through the reduction of crime. This includes cases such as the recovery of missing individuals and drug arrests leading to 435,000 lethal doses of fentanyl being removed from Pasco’s streets. Furthermore, STAR deputies are integral components of the search and successful safe recovery of missing Pasco children, both in and out of our county, among other daily activities wholly unrelated to making contact with prolific offenders. 
We continue to be open and transparent with our community and those that we serve, which is why we want to ensure you, our citizens, had this information. 
It is sad that there are media outlets attempting to drive division between a community and law enforcement by relying on previous employees who were held accountable for their actions as well as individuals with lengthy criminal histories and ignoring the more than 30 pages of well-researched facts that the Pasco Sheriff’s Office provided. 
It is also sad that this same media outlet refuses to report on the good work of our Behavioral Health Intervention Team and the fact that our men and women recently took 435,000 lethal doses of fentanyl off of Pasco’s streets. This is selection bias at its worst and the reason we connect with our community directly on social media.
We stand with our community and we are deeply grateful that you continue to stand with us.
As we told the media outlet, we will not apologize for continuing to keep our community safe.
Thank you for your continued support.",Pasco Sheriff's Office,https://www.facebook.com/pascosheriff/posts/3372128789491974
"[""Kathleen McGrory"",""Neil Bedi and Romy Ellenbogen""]",2021-07-27,2021-07-27,2021-01-19,2021-07-27,https://www.tampabay.com/resizer//ySdZ3a__tL_HMfcyp6t-LQT5a2k=/1140x641/smart/cloudfront-us-east-1.images.arcpublishing.com/tbt/XOUJXYLT6NDDXOOXNCWPJFO7K4.jpg,2015-09-01,0,en,,www.tampabay.com,"[""Kate Perkins""]","The school district shares student data with the Sheriff’s Office, which uses it to identify potential future criminals.

Denouncing the program as promoting “racial bias” and further feeding the “school-to-prison pipeline,” a U.S. congressman Tuesday called for a federal investigation into the Pasco school district’s practice of sharing student data with law enforcement.

“This use of student records goes against the letter and the spirit of (the federal student privacy law) and risks subjecting students, especially Black and Latino students, to excessive law enforcement interactions and stigmatization,” said U.S. Rep. Robert C. Scott, a Virginia Democrat and the chairman of the House Committee on Education and Labor, in a letter to the acting federal education secretary.

“Further, instead of helping at risk students, pre-criminal categorization merely makes more concrete the schools-to-prison pipeline that is a result of institutional bias,” he added.

Scott’s scathing three-page letter heavily referenced an investigative series in the Tampa Bay Times that found the district had shared data on grades, attendance and student discipline with the Pasco Sheriff’s Office.

The Sheriff’s Office then used the information to make a list of schoolchildren who might “fall into a life of crime.”

The Times found the program had played out largely in secret. Neither students nor their parents are told about the designation. And when first asked about the program by a reporter, schools Superintendent Kurt Browning said he was unaware school-district data was being used that way.

More than 400 kids were on the list late last year, the Sheriff’s Office said.

A statement from the school district late Tuesday said officials looked forward to hearing from the U.S. Department of Education.

“Their knowledge of our agreements with the Sheriff’s Office appears to be based on recent news stories, which do not provide a full picture,” the statement said. “Those agreements include safeguards for the proper use of student information and are designed to provide supports to students who are at risk.”

School board member Cynthia Armstrong declined to comment. The other four board members did not return reporters’ calls.

Separately, the Pasco Sheriff’s Office said in a statement that it continues to stand by the program and that the initiative had been “categorically misrepresented by the Tampa Bay Times.”

“The At-Risk Youth program, which follows recommendations and requirements made as part of Florida’s response to the Marjory Stoneman Douglas tragedy, is an important part of keeping children safe,” it said, referencing the 2018 shooting at a Parkland high school that left 17 dead. “Importantly, the Marjory Stoneman Douglas Commission identified silos and the lack of information sharing as key components that led to this tragedy.”

The federal education department declined to comment Tuesday, the last day of President Donald Trump’s administration.

In addition to grades and school attendance records, the Sheriff’s Office uses child-welfare histories from the state Department of Children and Families to determine which kids are likely to become criminals. Under the program, any child who gets a D or an F grade or has been the victim of child abuse could be subjected to scrutiny by law enforcement.

The agency has said it uses the list to provide mentoring and resources to at-risk kids.

But national experts have questioned the justification for mining students’ confidential records. And groups including the Brennan Center for Justice and the Future of Privacy Forum have raised legal and ethical concerns about the initiative.

Separately, the Pasco County Council PTA has called on the school district to review its data-sharing agreements, and tens of thousands of people have signed a petition urging Browning to end the program outright.

Scott echoed many of the same concerns in his letter, saying the program in Pasco raised “serious questions about the ethics of law enforcement agencies identifying law-abiding children for enhanced policing.”

“Of additional concern is the racial bias that necessarily feeds this or any similar system,” he added, noting that any law enforcement system that used school discipline data to identify kids as potential criminals was not only illegal, but unfair.

Scott also referenced the Times’ reporting on a separate Pasco intelligence program that uses people’s criminal histories and social networks to predict whether they will break the law in the future.

The Times found that the agency had monitored and harassed individuals it believed were likely to be future offenders. At least 1 in 10 were younger than 18.

“These abuses highlight the real dangers of this form of policing, especially as applied to students,” the congressman wrote.

In addition to requesting an investigation into the Pasco program, Scott asked the education department to “take steps to ensure that all school districts are appropriately managing student information.”

Full statement from the Pasco Sheriff’s Office:

The Pasco Sheriff’s Office continues to stand by this program, which has been categorically misrepresented by the Tampa Bay Times despite voluminous responses detailing the Intelligence-Led Policing philosophy, which is utilized across the Tampa Bay Area by numerous law enforcement agencies, and how it works and functions within our agency.

Specifically, the At-Risk Youth program, which follows recommendations and requirements made as part of Florida’s response to the Marjory Stoneman Douglas tragedy, is an important part of keeping children safe. Importantly, the Marjory Stoneman Douglas Commission identified silos and the lack of information sharing as key components that led to this tragedy.

This At-Risk Youth program breaks down silos. If the Tampa Bay Times has any concerns about this program, they should also share their concerns regarding the recommendations made by the Marjory Stoneman Douglas Commission.

None of the information received by the Sheriff’s Office is based on race, gender or any other demographic information.

The Pasco Sheriff’s Office reiterates our invitation for any individual or group who wishes to learn more about the At-Risk Youth program, and how it differs vastly from the Pasco Schools’ Early Warning program, to contact the Pasco Sheriff’s Office for the factual information on this program instead of relying on inaccurate and salacious reporting.

As such, the Pasco Sheriff’s Office has reached out to Congressman Scott’s staff to provide factual information on this program but has yet to hear back.
",Congressman urges probe of Pasco school data program,https://www.tampabay.com/investigations/2021/01/19/congressman-urges-probe-of-pasco-school-data-program/
"[""Romy Ellenbogen and Kathleen McGrory""]",2021-07-27,2021-07-27,2021-03-11,2021-07-27,https://www.tampabay.com/resizer//_XIXRgQoE9BVH6j8SxLGg_JXOcs=/1140x641/smart/cloudfront-us-east-1.images.arcpublishing.com/tbt/CKTVCYBNW5GF3H7K6KLPM5BPSM.jpg,2015-09-01,0,en,,www.tampabay.com,"[""Kate Perkins""]","A national public interest firm is representing the plaintiffs, who allege their First, Fourth and Fourteenth Amendment rights were infringed upon.

Four Pasco County residents are suing Sheriff Chris Nocco in federal court, alleging his intelligence program violated their constitutional rights.

The plaintiffs, three of whom were featured in a recent Tampa Bay Times investigation into the Sheriff’s Office’s intelligence arm, say they were harassed, fined and even arrested by overzealous deputies who overstepped their bounds.

They want a judge to put an end to the program, which targets people the Sheriff’s Office deems likely to commit future crimes and their friends and family members.

“The goal here is to shut this program down and to make sure it stops, both for these clients and everybody in Pasco County,” said Robert Johnson, an attorney with the Institute for Justice, the national public interest law firm representing the plaintiffs.

The Sheriff’s Office said Wednesday it had not been made aware of the lawsuit and declined to comment. “We look forward to defending any lawsuits in which we may be named in the proper venue and will not be party to litigation via the media,” spokeswoman Amanda Hunter said.

The agency has previously said it stands behind its intelligence program and credited it with a reduction in burglaries, larcenies and auto thefts over the last decade. The decline mirrors those in nearby police jurisdictions.

Although Nocco launched the program shortly after being named sheriff in 2011, the details of how it works were not known to the public until the Times began reporting on it last year.

The news organization found that the Sheriff’s Office uses arrest histories and information from police reports — including whether people have witnessed or been the victim of a crime — to determine which residents are most likely to break the law.

Deputies then make repeated visits to those individuals’ homes, even when there is no warrant or evidence of a crime.

During some visits, the Times reported, deputies surrounded the targets’ homes in the middle of the night. On others, they wrote or threatened to write code enforcement citations for minor infractions like overgrown grass. Sometimes, they arrested family members of those being targeted.

One former deputy said the objective was to make peoples’ lives miserable.

Institute for Justice attorneys said their organization started looking into the situation after reading the Times’ report. The Virginia-based firm describes itself as libertarian and says it files lawsuits on behalf of individuals who are denied their constitutional rights.

“The behavior of the Pasco County Sheriff’s Office is outrageous,” attorney Ari Bargil said. “There’s no such thing as innocent until predicted guilty and that’s exactly how the program operates.”

Johnson said the Sheriff’s Office was violating the constitutional right to be secure in one’s own home. He said the agency was also violating residents’ due process rights because individuals who are targeted can’t challenge their status.

“Being on this list is almost like being on probation for future crimes that haven’t happened yet,” he said. “If you are on real probation, you get a hearing. It’s problematic that they put you on this future probation without any type of hearing.”

The four plaintiffs were either targets or the parents of people who were targeted.

Robert A. Jones, III, whose teenage son was targeted, said when he stopped complying with constant demands from deputies to search his home, he was arrested on charges of marijuana possession and child neglect. Both charges were dropped.

Jones was also issued several code enforcement citations and not told about them, he said. He was then arrested for failing to appear in court. His arrest record dashed his dream of going to law school, he said.

Jones hopes that the lawsuit will set a precedent so nobody else faces what he did.

“I think it’s everybody’s hope that they were put on this earth for a reason,” he said. If he hadn’t gone to jail, he added, “maybe nothing changes, maybe these guys get away with this forever.”

Tammy Heilman, whose teenage son was also a target, was arrested, too.

In September 2016, a deputy showed up to ask about a dirt bike he thought her son had purchased with stolen money. Heilman said she wouldn’t speak without her lawyer there.

The deputy stopped Heilman after she drove away and opened her car door, later alleging that neither she nor her 7-year-old daughter were wearing seat belts. Heilman says her daughter was wearing a seat belt, but she removed it when the door opened. Body-camera video of the interaction shows Heilman calling 9-1-1 for help.

A team of deputies arrived and forced Heilman out of the car. She was taken to jail and charged with resisting an officer, battery on a law enforcement officer and providing false information to deputies.

Two years later, she was arrested again, this time for opening her screen door into a deputy’s chest. After 76 days in jail, Heilman took a plea deal that allowed her to return home but made her a convicted felon.

“They need to stop,” she told the Times. “They have impacted a lot of people’s lives in a negative way.”

In her case, she said, the damage has been “beyond repair.”

The Institute for Justice isn’t the only organization working to address aspects of the Pasco intelligence program.

Earlier this week, the NAACP Legal Defense and Education Fund filed a public records request, seeking to learn more about a separate, but related initiative that uses confidential data from the Pasco school district and the state Department of Children and Families to identify kids who could fall into a life of crime.

The Sheriff’s Office has said it uses the information to offer mentoring and resources to local students. But critics say some of that data is biased against children of color and with disabilities. And they worry a program of this nature could contribute to the school-to-prison pipeline.

“There is no place for predictive policing technology in school settings,” said Clarence Okoh, a Legal Defense and Education Fund fellow.

The school district, which is being asked to provide the records, has received the request and plans to respond, a spokesperson said.",Lawsuit: Pasco intelligence program violated citizens’ rights,https://www.tampabay.com/investigations/2021/03/11/lawsuit-pasco-intelligence-program-violated-citizens-rights/
"[""Kathleen McGrory""]",2021-07-27,2021-07-27,2021-01-24,2021-07-27,https://www.tampabay.com/resizer//cO_17RyGWDwiA_U0WKWSma-psp8=/1140x641/smart/cloudfront-us-east-1.images.arcpublishing.com/tbt/DB4D5UWIKAI6TGNIIBWI6S7HAY.jpg,2015-09-01,0,en,,www.tampabay.com,"[""Kate Perkins""]","Critics of the agency’s intelligence programs called the letter ‘patronizing’ and ‘offensive,’ and raised continued concerns about civil rights

It starts like an offer of admission from a prestigious university.

“We are pleased to inform you that you have been selected…” it says.

But the four-page letter from the Pasco Sheriff’s Office goes on to tell recipients they will be facing enhanced police scrutiny under the agency’s controversial intelligence program.

“You may wonder why you were enrolled in this program,” the letter continues. “You were selected as a result of an evaluation of your recent criminal behavior using an unbiased, evidence-based risk assessment designed to identify prolific offenders in our community. As a result of this designation, we will go to great efforts to encourage change in your life through enhanced support and increased accountability.”

Last year, a Tampa Bay Times investigation revealed that the Sheriff’s Office creates lists of people it considers likely to break the law based on criminal histories, social networks and other unspecified intelligence. The agency sends deputies to their homes repeatedly, often without a search warrant or probable cause for an arrest.

Targets and their relatives, including four who are now suing the Sheriff’s Office in federal court, described the tactics as harassment and a violation of their constitutional rights. National policing experts drew comparisons to child abuse and surveillance that could be expected under an authoritarian regime.

The Times also found that the agency has a separate program that uses schoolchildren’s grades, attendance records and abuse histories to label them potential future criminals.

Earlier this year, Sheriff Chris Nocco and the Pasco County school district announced they would scale back some features of the school-data program. But the letter signals a broadening of the core program.

The Sheriff’s Office said the letter is part of a new intelligence effort aimed specifically at people whose criminal histories include drug offenses and violent crimes.

It was supposed to launch in mid-2020, but was delayed until December because of the pandemic, Sheriff’s Office spokeswoman Amanda Hunter said.

It includes several new features, including that people can be dropped from the program after two years without “criminal activity” and a phone number they can call with questions.

In an online video, Sheriff’s Office Captain Toni Roach says being selected is “good news” because participants will “have the opportunity to receive assistance from the Pasco Sheriff’s Office and several community partners.”

But critics of the agency’s intelligence efforts, including an alliance of local, state and national organizations known as People Against the Surveillance of Children and Overpolicing, or the PASCO Coalition, said the latest communication raises even more concerns.

“The letter is basically threatening and promising a certain level of harassment and oversight that is in line with the stories we are hearing from the community,” said Raniah Elgendi, of the Council of American-Islamic Relations-Florida.

“We know that is not what makes people or communities more safe, this heightened level of surveillance,” said Lauren Johnson, an assistant counsel at the NAACP Legal Defense Fund.

The Times found being named a Sheriff’s Office target could have serious consequences. Deputies showed up at homes at all hours of the day and night, writing tickets for violations like overgrown grass and making arrests for any reason they could find.

By 2020, some 1,000 people had been ensnared. About 100 were 18 years old or younger.

The new letter to so-called “prolific offenders” says its purpose is to communicate the agency’s “sincere desire” to help recipients “begin a new path.”

“We are committed to your success,” it says.

The letter notes that the Sheriff’s Office has partnered with Pasco County Human Services and provides contact information for 18 government agencies, health clinics and non-profit organizations.

But it also delivers a stern warning: “Our desire to help you will not hinder us from holding you fully accountable for your choices and actions.”

It then says the Sheriff’s Office will share recipients’ names and criminal histories with local, state and federal law enforcement agencies to ensure “the highest level of accountability” for any future crimes they commit.

Members of the PASCO Coalition questioned whether the Sheriff’s Office pressuring people to get services would actually help.

They also took issue with the language used in the letter.

“It is so incredibly patronizing and offensive on so many levels,” said Bacardi Jackson, Florida Managing Attorney at the Southern Poverty Law Center.

The coalition is continuing to demand changes to Pasco’s intelligence programs, its members said.

Times Staff Writer Kathryn Varn contributed to this report.",Pasco Sheriff’s Office letter targets residents for ‘increased accountability’,https://www.tampabay.com/investigations/2021/07/24/pasco-sheriffs-office-letter-targets-residents-for-increased-accountability/
"[""Matthew Guariglia""]",2021-07-27,2021-07-27,2021-03-22,2021-07-27,https://www.eff.org/files/2021/03/22/pasco_1.png,2015-09-01,0,en,,www.eff.org,"[""Kate Perkins""]","The group asked the Department of Education to look at children of color, racial disparities in its federal investigation.

In September 2020, the Tampa Bay Times revealed a destructive “data-driven” policing program run by the Pasco County, Florida Sheriff's Office. The program is misleadingly called “Intelligence-Led Policing” (ILP), but in reality, it's nothing more than targeted child harassment by police. Young people's school grades and absences, minor infractions, and even instances where they are a victim of crime are used to inform a bogus rubric and point system, based on a formula that intends to ""prevent future crimes""—essentially labeling youths as a potential future criminals.

Below is a page from the ILP’s pseudoscientific manual. Once a juvenile is tagged with this label, police show up at their home and harass their entire family. As one former deputy described the program to reporters, the objective was to “make their lives miserable until they move or sue.”

The fault lies not just with the Pasco County Sheriff’s Office, which built this system and uses it to hound youth. The system also functions with the help of public schools and child welfare agencies that collect data about kids for purposes of providing them with important educational and social services, and then hand this data over to the Sheriff. This is an egregious abuse of trust. As a reaction to this publicized relationship between the schools and the police, the Charles and Lynn Schusterman Family Philanthropies organization cut nearly in half the money it had intended to give Pasco County schools, pulling the remaining $1.7 million. The organization explained, reasonably, that the program was contradictory to their values. 

It’s not hard to see why an organization would pull grant funding. The program, as well as many other data-driven or predictive policing models, creates a self-fulfilling feedback loop from which it is nearly impossible for a child or their family to escape. Anyone, when put under a microscope by police, might accumulate citations, fines, and even arrests. And that’s what this program does: it takes families, often ones that are already struggling, and puts them under the aggressive eye of police who expect their lives to become even more miserable. 

Some of the stories that emerged from news reporting illustrate the harms that getting stuck in an enforcement loop can have on people. After one 15-year-old was arrested for stealing bicycles out of a garage, the algorithm continuously dispatched police to harass him and his family. Over the span of five months, police went to his home 21 times. They also showed up at his gym and his parent’s place of work. The Tampa Bay Times revealed that since 2015, the sheriff's office has made more than 12,500 similar preemptive visits to people. 

These visits often resulted in other, unrelated fines and arrests that further victimized families and added to the likelihood that they would be visited and harassed again. In one incident, the mother of a targeted teenager was issued a $2,500 fine for having chickens in the backyard. In another incident, a father was arrested because his 17-year-old was smoking a cigarette. These behaviors occur in all neighborhoods, across all economic strata—but only marginalized people, who live under near constant police scrutiny, face penalization.

The Sheriff’s Office and the school district have claimed that the program is intended to identify at-risk youth in need of state intervention or mentorship. But in at least five places in the Sheriff’s ILP manual (including the one below), the program’s actual stated purpose is to identify potential future offenders. 

The ILP manual lists characteristics that the program considers as so-called “criminogenic risk factors”; these are the factors unscientifically believed to lead to future criminality. The characteristics include being a victim of a crime, unspecified “low intelligence,” “antisocial parents,” and being “socio-economically deprived.” The ILP manual explains the program’s purpose is to identify youth “destined to a life of crime.” 

This is absurd. No one is destined to a life of crime. Having bad grades or economic struggles does not make a person into a criminal suspect, or make them deserving of police harassment. 

In response to the program, a number of civil liberties groups have stepped up to try to stop the data sharing between the school district and the police, and end the ILP program. Color of Change launched a petition allowing people to tell the Superintendent of schools to stop sharing data with the police. The Institute for Justice also launched a lawsuit against the Sheriff's Office, asserting the First, Fourth, and Fourteenth Amendment rights of those that have been victimized by the program. We anticipate more advocacy to come against this program. 

In response to recent news about the program, Florida Republican Congressmember Matt Gaetz called on Governor DeSantis to remove the sheriff, Chris Nocco, from his post. 

Florida state senator Audrey Gibson has also introduced SB 808, which seeks to place a few limits on this kind of Intelligence-Led Policing. Most importantly, it would require notice to a person that they are listed and an opportunity to appeal that listing. The bill would also require departments that use these programs to adopt guidelines to address data processing, program goals, and number and length of visits. Further, the bill would mandate documentation of visits, including audio and video (presumably from police body worn cameras), and records on the demographics of visited youths.  Although this bill may mitigate some of the program’s more egregious potential abuses, it’s not nearly enough to prevent all of the harms ILP generates. Predictive policing programs that rob people of their presumption of innocence, and open them up to harassment and surveillance, should be banned. 

EFF has been an outspoken critic of police use of gang databases, which often have subjective and racist criteria for inclusion and which open up an individual to unfair police harassment and surveillance. Once put on the list, people often can be harassed for years without any knowledge of how to get remove themselves. Such opaque conditions exist in Pasco County as well. 

EFF is working with a coalition of local, statewide, and national organizations that are trying to dismantle this harmful ILP program. Attempts to predict crime and sniff out future criminals are not new; they’ve been fodder for science fiction writers, criminologists, and detectives for over a century. But then and now, no one cannot predict crime, they can only create targets through excessive surveillance. “Data” and “intelligence” too often are buzzwords that imply a police initiative is objective and immune from human biases. But when fallible and biased individuals, including school administrators and police, determine who is and is not a future criminal based on exam grades or supposedly “antisocial” behavior, the “intelligence” system will only serve to replicate pre-existing racial and class hierarchies. ",Pasco County’s Sheriff Must End Its Targeted Child Harassment Program,https://www.eff.org/deeplinks/2021/03/pasco-countys-sheriff-must-end-its-targeted-child-harassment-program
"[""Todd Feathers""]",2021-08-17,2021-08-17,2021-07-19,2021-08-17,https://video-images.vice.com/articles/60f1b788daac5e0094c30815/lede/1626704180119-gettyimages-1257497678.jpeg?image-resize-opts=Y3JvcD0xeHc6MC44NDI1eGg7MHh3LDB4aCZyZXNpemU9MTIwMDoqJnJlc2l6ZT0xMjAwOio,2016-04-01,0,en,,www.vice.com,"[""Kate Perkins""]","The right angle formed by Belmont and North Central Park avenues forms an invisible borderline in Chicago. 

To the south and east, the residents are primarily Black and brown. To the north and west, the neighborhoods are almost all majority white.

The two avenues mark another stark separation that follows the lines of racial segregation throughout Chicago: In nearly every community of color in the city, the Chicago Police Department (CPD) has installed ShotSpotter , a system that uses hidden microphone sensors to detect the sound of gunshots, generate real-time alerts, and trigger armed police responses to the location. In most of the white neighborhoods, there are no sensors at all, according to Motherboard’s analysis of ShotSpotter data obtained through a public records request.

Chicago is not alone. Cities and police departments are loath to disclose the locations of their ShotSpotter sensors, but through public records requests Motherboard also obtained years of data from Kansas City, Missouri; Cleveland, Ohio; and Atlanta, Georgia showing where ShotSpotter sensors generated alerts—a proxy for the general location of the sensors. 

In all four cities, the data shows that the sensors are also placed almost exclusively in majority Black and brown neighborhoods, based on population data from the U.S. Census.

In Chicago, the technology's reliability is coming under increasing scrutiny. Community members and civil rights activists say false ShotSpotter alerts bring a flood of unnecessary police into their neighborhoods, and even accurate alerts can create dangerous situations. In March, one such alert initiated a police response that eventually led to the killing of 13-year-old Adam Toledo, who was unarmed when Chicago police shot him. 

“The system is telling police that every time they go out in response to a ShotSpotter alert they should assume that anybody in the vicinity is armed and they’ve just fired a weapon,” Jonathan Manes, an attorney with the MacArthur Justice Center at the Northwestern Pritzker School of Law who has studied ShotSpotter in the city, told Motherboard. “The system is telling police officers that anybody in the area is a mortal threat. Following up on those alerts is creating a dangerous situation, and it’s happening 61 times a day in the city of Chicago.”

The Chicago Police Department, Mayor Lori Lightfoot’s office, and Alderman Chris Taliaferro, who chairs the city council’s public safety committee, did not respond to interview requests or questions.

“In general, police department customers determine coverage areas with assistance from ShotSpotter by analyzing historical gunfire and homicide data to assess areas most in need of gunshot detection,” Sam Klepper, senior vice president for marketing and product strategy at ShotSpotter, wrote to Motherboard in response to questions.

“We believe all residents that live in communities experiencing persistent gunfire deserve a rapid police response that gunshot detection enables, regardless of race or geographic location,” the company added. “While gun violence can unfortunately happen anywhere at any time, cities lack sufficient funds to cover an entire city with gunshot detection technology, so they most commonly deploy sensors in neighborhoods with the highest levels of gun violence to make the greatest impact.""

Kansas City, MO

In 2012, the Kansas City Area Transportation Authority received a federal grant to expand one of its express bus lines. Some of those funds were used to install ShotSpotter, in cooperation with police, in locations that “provided the highest amount of coverage to the areas most densely and heavily traveled by the public transit lines,” the Kansas City Police Department Sergeant Jake Becchina told Motherboard in an email.

What that meant in practice is that ShotSpotter sensors were installed in a 3.5-square mile swath—about 1 percent of the city’s footprint—that primarily includes neighborhoods where white residents make up as little as 3.5 percent of the population, according to Census data.

“We would love to have the resources to blanket the entire city with gunshot detection,” Becchina wrote to Motherboard. “ShotSpotter alerts give our officers more detailed information about what’s happening than with 911 calls alone ... so they are better able to bring a dangerous situation safely under control more quickly and efficiently.”

He added that 70 percent of the time ShotSpotter generates an alert, nobody calls 911 to report gunshots, according to an analysis performed in 2016.

That could be because residents choose not to call 911, or because no gunfire actually occurred and ShotSpotter misidentified a sound, as some studies suggest the technology does quite frequently.

“I think I speak for a lot of people when I say [ShotSpotter is] not welcome technology,” Henry Service, a Kansas City lawyer and community activist, told Motherboard. “We don’t want to be treated like a surveilled community, a community that’s under siege. The kind of policing we want is the kind of policing you get in the suburbs or affluent communities.”

As in many cities that have installed ShotSpotter, the initial deployment in Kansas City covered a small geographic area and was funded by a grant. ShotSpotter costs between $65,000 and $95,000 per square mile annually, plus installation costs, making it a prodigious investment.

Kansas City officials have said at various times since 2012 that they want to expand ShotSpotter to cover more of the city, potentially even more affluent sections, but so far, that hasn’t happened.

Atlanta, GA

The Atlanta Police Department (APD) installed 100 sensors in the city in 2018, funded entirely by grants from the Atlanta Police Foundation and Georgia Power, a utility company. The city stopped using the sensors in 2020, when funding ran out.

When local media first reported the ShotSpotter program, APD would not disclose the exact location of the sensors but said they would initially be located only in several West Atlanta neighborhoods, which happen to be overwhelmingly non-white. Alerts in the map below in the city’s outer neighborhoods may be the result of errors in the data. APD did not respond to a request for comment.

Unlike the ShotSpotter data Motherboard obtained from other cities, the Atlanta data also includes fields for when officers arrived at the scene of an alert and when they departed. The system generated 2,925 shots-fired alerts during its time in use. The median amount of time APD officers spent on scene for those alerts was nine minutes, suggesting that in a large proportion of the cases officers found little to work with at the scenes to which ShotSpotter dispatched them.

Cleveland, OH

Cleveland is one of the newest cities to have installed ShotSpotter, beginning its program in November 2020. The system—funded entirely during its two-year pilot phase by a $375,000 grant from the Cleveland Police Foundation—has been deployed in a small section of Southeast Cleveland that city officials have described as a high-crime area.

Those neighborhoods also happen to be among the least white in the city and those hit hardest by economic recession and the pandemic, according to local community organizers.

“These are communities that lack resources and have been targeted by city officials because they claim that they are highly concentrated areas of violence,” LaTonya Goldsby, president of Black Lives Matter Cleveland, told Motherboard. “You can see the disenfranchisement. There has been no reinvestment within that side of town. I would say it’s been a continued depletion of resources.”

The Cleveland Division of Police selected the Shotspotter locations by analyzing three years of crime data within its police district four, which encompasses many of the Blackest, most impoverished parts of the city. The area the division chose for Shotspotter accounted for 37 percent of all shots fired calls, 37 percent of felonious assaults, and 45% of all homicides within district four, Sergeant Jennifer Ciaccia wrote to Motherboard in an email.

“The technology appears to be extremely accurate, allowing officers to respond to locations of gunfire based on technology instead of community calls which can be more subjective,” she wrote.

Cleveland’s mayor and several city councilors did not respond to requests for comment.

After the Cleveland City Council—which has not allowed public input at its meetings since the 1930s—approved the ShotSpotter plans, Cleveland activists including Mary Drummer, a campaign director for AI for the People, wrote to councilors to ask that the money allocated for ShotSpotter be put to other uses in the affected communities.

“There’s worry that It’s going to lead to police harassment of folks that are just minding their business,” Drummer told Motherboard. “It’s really interesting, especially after the pandemic, how there’s never enough money to fund the community and get them the services they need, but there’s always enough money for the police.”",Gunshot-Detecting Tech Is Summoning Armed Police to Black Neighborhoods,https://www.vice.com/en/article/88nd3z/gunshot-detecting-tech-is-summoning-armed-police-to-black-neighborhoods
"[""Gary Craig""]",2021-08-17,2021-08-17,2018-01-11,2021-08-17,https://www.gannett-cdn.com/-mm-/621ecae1af5a774ce3224cd456d8cd5868144a41/c=0-18-2167-1242/local/-/media/2018/01/11/Rochester/Rochester/636512706800040447-TY-011118-REVERSE-SILVON-SIMMONS.jpg?auto=webp&format=pjpg&width=1200,2016-04-01,0,en,,www.democratandchronicle.com,"[""Kate Perkins""]","In a courtroom full of blue, with dozens of uniformed Rochester police officers watching, a judge Thursday reversed the criminal conviction of a man once accused of attempting to murder a city police officer.

County Court Judge Christopher Ciaccio decided that a key piece of the prosecution's case — the city's crime-fighting tool known as ShotSpotter — was not reliable enough. While the tool, which attempts to alert police to gunshots, may be viable in cases with other corroborating evidence of a crime, it became the linchpin of the case against Silvon Simmons, Ciaccio ruled.

""This case did not have buttressing evidence,"" Ciaccio said.

In late October a jury acquitted Simmons of the 2016 attempted murder of a police officer, Joseph Ferrigno, who was chasing him on foot. The jury did convict Simmons of the criminal possession of a weapon that was found near Simmons, whom Ferrigno shot three times. Ferrigno alleged that he returned fire after Simmons shot at him.

The case in some ways was a microcosm of the strained relations between police and some lower-income neighborhoods in the city. Activists pushing for tougher oversight of police portrayed the incident as a rogue cop shooting an innocent man; Simmons ran from police after being mistaken for someone else the police were looking for.

Rochester police, meanwhile, saw the shooting episode as a reminder of the hazards faced daily by cops in a city where illegal guns are legion and people willing to use them are plentiful.

But what became the centerpiece of the evidence against Simmons, as well as the centerpiece of Ciaccio's decision to reverse the weapons possession conviction, were the questions of the ShotSpotter evidence, which provided audio support of Ferrigno's version of the shooting.

Simmons was scheduled to be sentenced Thursday, but Ciaccio sided with Simmons' defense, who asked that the lone weapons possession conviction be reversed because of the ShotSpotter evidence.

ShotSpotter is contracted by localities across the country, including Rochester. The company sets up microphones with transmitters attached, and they're located at different elevated spots around high-crime areas. ShotSpotter officials then alert police to possible shooting locations.

Assistant Public Defender Elizabeth Riley, one of Simmons' attorneys, argued that ShotSpotter had denied the defense evidence that could have raised more questions about the shooting. The city contracts with ShotSpotter for about $130,000 annually. Riley in court portrayed the relationship as one in which ""a multi-million corporation preserved what (evidence) the customer wanted.""

Assistant District Attorney Julie Hahn, one of the prosecutors in the case, said that ShotSpotter had reviewed its audio from the shooting, and found nothing else worthy of relaying to the prosecution or defense. The audio included five audible bursts; Ferrigno fired four times and said Simmons fired at him first with a single shot.

Hahn said Ferrigno has not been able to return to work because of the trauma of the incidence, and his family has suffered too. Ferrigno was cleared of any wrongdoing in an internal investigation.

""There's no consideration whatsoever given the officer,"" she said after Ciaccio's ruling.

After the ruling, Simmons continued to maintain his innocence.

""I just thank God,"" he said. "" ... I still got shot for nothing.""

ShotSpotter evidence

At the trial, Ferrigno and another officer who was the first on the scene after the shooting testified about the incident. The defense highlighted inconsistencies in the testimony, which the prosecution said were minor.

However, Ciaccio said, the jury clearly ""discredited"" the testimony; otherwise it likely would have convicted Simmons of the more serious charge. And, over four days of deliberations, the jury returned multiple times to listen to the ShotSpotter audio, and its apparent five audio bursts.

The case was originally with another judge, who denied a defense request for a broad subpoena of ShotSpotter records. ShotSpotter provided eight seconds of audio, obtained from different sensors around Immel Street, where the shooting occurred.

Initially the automated system did not recognize the sounds as gunfire, deciding it was instead likely an overhead helicopter. After the city requested that ShotSpotter review the audio, company officials decided there were at first three, then four, then five shots.

That subjective determination raised questions about the reliability of the ShotSpotter evidence, Ciaccio said. And, Riley contended, the lack of audio before or after those eight seconds hindered the defense. There may well have been other audio bursts that could have shown there were other noises picked up by the sensors, Riley said.

Hahn countered that ShotSpotter officials thoroughly reviewed the audio evidence and would have provided proof of any other sounds, had there been others.

Hahn said that Ciaccio's decision may be appealed. There is also the possibility of a retrial on the weapons possession charge, but that would likely have to go forward without the ShotSpotter evidence.

Defense attorneys tried unsuccessfully to have the ShotSpotter evidence disallowed at the trial. The Innocence Project even joined in, filing its first brief raising questions about ShotSpotter.

The Innocence Project, a nonprofit organization that works to free the wrongly convicted, highlighted in its brief what it said was the subjective nature of ShotSpotter evidence.

""While this technology may have advanced to the point where its data can be used as an investigative tool, it is not sophisticated enough to generate data that is reliable enough to be admitted as 'scientific' evidence in a criminal trial,"" the Innocence Project's Dana Delger wrote in her brief.

Ciaccio said that the science behind ShotSpotter may be reliable enough when it is partnered with other evidence. But that was not the case with the Simmons' shooting, he said.
""The ShotSpotter testimony was crucial,"" Ciaccio said.
",Judge tosses conviction of man accused of shooting at cop,https://www.democratandchronicle.com/story/news/2018/01/11/judge-reverses-conviction-man-who-shot-officer/1017531001/
"[""Gary Craig""]",2021-08-17,2021-08-17,2017-11-17,2021-08-17,https://www.gannett-cdn.com/-mm-/698ae489868a9ad6cf87a3e9153a8d03f887ac3c/c=39-0-1326-727/local/-/media/2017/11/14/Rochester/Rochester/636462794672778987-IRP-Screenshot-with-ALL.jpg?auto=webp&format=pjpg&width=1200,2016-04-01,0,en,,www.democratandchronicle.com,"[""Kate Perkins""]","For Rochester police, the gunfire detection system known as ShotSpotter has been a valuable tool, alerting them to possible shootings and speeding up the police response time.

But, more than that, the system — which tries to locate gunfire through audio sensors placed in high-crime areas in the city — has also become part of courtroom testimony, with prosecutors using the recordings as evidence to bolster their allegations of where and how many shots were fired in criminal incidents.

However, the latter use — evidentiary support for prosecutions — is coming under increasing attack, as critics say the system's use is being stretched beyond its scientifc and analytical limits. The New York City-based Innocence Project, which has helped exonerate nearly 200 wrongly convicted people, recently filed a legal brief in a Rochester criminal case, challenging the reliability of ShotSpotter when it is used for more than a gunfire alert system.

“At the end of the day this is a machine that basically can tell you that there was a loud sound and then a human has to tell you whether it was gunfire or not,” Dana Delger, an attorney for the Innocence Project, said of ShotSpotter in a telephone interview.

In Rochester, ShotSpotter has been used as proof in several high-profile trials, most notably the trials of the men accused of the shooting in front of the Boys & Girls Club on Genesee Street that left three dead, and the murder of Rochester Police Officer Daryl Pierson by Thomas Johnson III.

The ShotSpotter-captured audio can give a jury a realistic picture of the events of a crime, said Monroe County First Assistant District Attorney Perry Duckles.

""ShotSpotter provides a captured audio of a point in time that we’re trying to explore at any given trial,"" said Duckles. ""When it captures that sound it brings a jury there. It allows them to hear what actually happened the night in question.""

With the Johnson trial, prosecutors were able to synchronize the ShotSpotter audio with surveillance videos, giving the jury not only a visual of the fatal shooting of Pierson but also the accompanying sounds of the gunshots.

But some critics see a system that is evolving in court beyond its original purpose of an alert mechanism.

“There’s a few things that are problematic,"" said Monroe County Assistant Public Defender Katie Higgins, who challenged the use of ShotSpotter audio in a recent criminal trial. ""One is that it was designed to be an investigative tool for the police, to alert them to possible gunfire and allow them to respond to see if there are civilian witnesses or other evidence of gunfire.

""But it was not designed to be used as actual primary evidence (in a trial),"" she said.

ShotSpotter needs to be subjected to more rigorous testing, from outside unbiased sources, to ensure its accuracy in courts, the critics say.

""While this technology may have advanced to the point where its data can be used as an investigative tool, it is not sophisticated enough to generate data that is reliable enough to be admitted as 'scientific' evidence in a criminal trial,"" the Innocence Project's Delger wrote in her brief filed in the Rochester case of Silvon Simmons, who was accused of trying to fatally shoot a police officer in 2016.

Alerting to crime

When the city of Rochester first contracted with California-based ShotSpotter in 2006, the company placed its audio sensors — advertised as ""gunfire specific acoustic-sensing technology"" — in several city areas with high crime rates.

Currently, the sensors cover more than seven square miles of the city, according to Paul Greene, the company's manager of forensics services. Fifteen to 20 sensors are used to cover a single square mile, Greene said in recent trial testimony.

Rochester now pays $130,000 annually to ShotSpotter, a cost that also includes the testimony at trials.

The sensors are microphones with transmitters attached, and they're located at different elevated spots around the city, ranging from utility poles to privately owned buildings. (ShotSpotter does not publicly release the locations.)

The sounds are transmitted to the company computers, which then studies the audio impulses and the likely distance from the monitoring sensors (at least three sensors need to register the sound for a determination of location). The computers subject the transmitted pulses to algorithms to try to determine the location and source, whether gunfire or something else, such as firecrackers or a backfiring car.

Those equations then offer a location for a likely shooting incident.

An analyst reviews the audio for ""classification verification"" — essentially deciding whether the computer was likely correct if it determined the sound was gunfire.

From the sound to notification to a 911 system and police of possible gunfire can take only a minute.

""It's not just a recording at that point,"" Duckles said. ""It's actually a scientific analysis of the recording.""

Police or prosecutors cannot make a sweeping request for long swaths of audio, said Special Assistant District Attorney Julie Hahn, who heads the office's major felony bureau and was one of the prosecutors in the Silvon Simmons case. Instead, the company provides the very narrow audio window when shots were fired, which is typically only seconds in duration.

""They will only provide us the pulses, the shots, which occurred with (a particular) incident,"" she said. Otherwise, Hahn said, the audio could be a privacy breach, possibly capturing snippets of loud street-level conversations or other sounds not connected to an investigation.

The company safeguards its information, and the proprietary relationship with its contracting municipalities gives ShotSpotter a different position than crime laboratories or medical examiner offices whose personnel might testify in a criminal trial about, for instance, forensics or likely causes of death.

A crime lab, for example, is typically a publicly funded operation, and its scientific processes are often an open book for prosecutors, defense lawyers, or forensics experts to explore and dissect for reliability. ShotSpotter officials contend that the algorithms are, like the Colonel Sanders fried chicken recipe, a proprietary company secret.

For defense lawyers, the relationship with the city and ShotSpotter is problematic, blocking them from a review that could show, for example, whether the sensors have been checked or serviced.

ShotSpotter says its research shows that the system is accurate 80 percent of the time within a 25-meter range from a shooting. Critics say that more peer review testing is needed to verify the system's accuracy.

A police shooting

On April 1, 2016, the ShotSpotter sensors picked up several audio bursts from a northwest Rochester neighborhood. Unable through its algorithms to detect a specific location, the system did not alert 911 or police. The system also thought the sounds to be the whirring blades of a helicopter, and not gunfire.

However, that night Officer Joseph Ferrigno chased Silvon Simmons, behind an Immel Street home. Ferrigno said Simmons shot at him once, then Ferrigno fired four shots, hitting Simmons three times.

Simmons was charged with attempted murder of a police officer and criminal possession of a weapon.

Police notified ShotSpotter of the shooting, and the company revisited the audio from the scene. The analysts at first thought there were three shots, then changed to number to four, then five. Analysts found the fifth shot after a prosecution request to review the audio again, prosecutors say.

Simmons' attorneys unsuccessfully sought to have the evidence withheld at trial, claiming that the changes from ShotSpotter officials showed a questionable coziness with authorities and the police version of the shooting. Prosecutors said the audio, once revisited, revealed the clear sounds of five shots.

The evidence was allowed. The jury heard the audio and acquitted Simmons of attempted murder but convicted him of criminal possession of a weapon. The jury may have decided that Simmons had a gun that accidentally discharged, attorneys surmised after the verdicts.

Attorneys agree that the ShotSpotter audio and testimony that there were five shots were crucial at Simmons' trial.

Because the system relies so heavily on a human review of the computerized data, there is a distinct possibility of ""cognitive bias"" — a determination that can be influenced by factors other than facts, the Innocence Project argued in the brief it filed in the Simmons' case.

In its brief, the Innocence Project highlighted the Rensselaer County trial of a man who fired shots at a police officer. In that case, one ShotSpotter analyst heard four gunshots in the captured audio file; another analyst heard three and said the fourth sound was likely a car backfiring.

The prosecution theory was that three shots were fired. The analyst who agreed with that number overrode the analyst's report of four shots and testified of his conclusion of three shots at the trial.

""This episode shows not only the subjective nature of the determination that a sound is actually the sound of gunfire, but also shows how ... (as here) information from the police may affect the conclusion of the analyst that a particular sound is gunfire,"" the Innocence Project argued.

Prosecutors say that they typically use ShotSpotter proof as supporting evidence in cases where there is no question a shooting occurred, such as the Genesee Street mass shooting, the murder of Officer Pierson, or the Immel Street shooting that led to Simmons' arrest and trial.

Investigative tool

The worth of ShotSpotter can be tough to argue, with its quick alerts to police of a detected shooting incident. Witnesses have been found and criminal cases solved because of the rapid ShotSpotter alerts.

While some cities — including Charlotte and Detroit — have abandoned the system, deciding the occasional false-positive alerts were too problematic, many cities, like Rochester, have decided the system is a valuable crime-fighting mechanism.

Almost 90 cities in the United States and elsewhere now use the system, according to the company.

""It's just a great tool to have with its analytics as far as being able to narrow down a (shooting) location,"" said Lt. Jeremy Lindauer.

Eric Piza, an associate professor at John Jay College of Criminal Justice, said that ShotSpotter has been valuable in cities where residents in high-crime neighborhoods have grown inured to the sounds of gunfire and don't always call police.

Piza, who previously worked as a crime analyst with the Newark, New Jersey, Police Department, said that when asked if a locality should use ShotSpotter, his standard answer is, ""It depends.""

""I think the first important question to ask is what are you trying to get out of this technology,"" Piza said. "" ... The empirical research that has been done on ShotSpotter suggests it might not affect crime reduction.""

Piza said he has yet to see research that studies ShotSpotter's reliability when used for specific evidentiary purposes in trials.

ShotSpotter Chief Executive Officer Ralph Clark said the courtroom challenges to ShotSpotter evidence are to be expected, similar to challenges by defense attorneys to most forensics-type evidence.

The challenges are ""normal court procedural motions, ones we see repeated time and time again whereby ultimately the evidence was ruled admissible and we testify as experts,"" Clark said.

Still, those who question the evidentiary use of ShotSpotter data say that judges should subject the company's systems to the same scientific analysis that has been required for the admission of evidence ranging from fingerprints to DNA. And that would require the company to open its system to outside review.

At Simmons' trial, ShotSpotter forensics service manager Greene said of the company, ""We're not required to have peer review but we're open to it.""

Assistant Public Defender Higgins said she hopes that day comes.

""It needs to be ... generally accepted within a wider scientific community,"" she said.",Is shot spotter reliable enough? Critics question human equation behind technology,https://www.democratandchronicle.com/story/news/2017/11/17/shot-spotter-technology-relshot-spotter-technology-coming-under-increased-scrutiny-judicial-communit/844335001/
"[""Adi Ashkenazy""]",2021-08-23,2021-08-23,2019-07-18,2021-08-23,https://skylightcyber.com/2019/07/18/cylance-i-kill-you/cylance-i-kill-you-small.gif,2019-07-18,0,en,,skylightcyber.com,"[""Kate Perkins""]","Read about our Journey of dissecting the brain of a leading AI based Endpoint Protection Product, culminating in the creation of a universal bypass

Update: 07/Sep/2019
We had the honour to present our findings in today’s BSides Sydney (Slides).
We took this opportunity to make some of the yet unpublished materials public.
We can now reveal that the undisclosed game we’ve used is “Rocket League”, but many others work just as well (we’ve tried Fortnite, for example).
Some more goodies include the “special sauce” - the list of strings that appears in Rocket League’s executable and are part of Cylance’s Model. Just append these into any malicious executable to make Cylance believe it’s benign.
If you’re just interested in trying it out locally, download this version of Mimikatz with the appended strings.

As of today, the bypass is still exploitable on the home edition (Cylance SmartAV). The vendor has told us the enterprise edition (CylancePROTECT) has been fixed, but we were unable to verify that. If you have access to the enterprise edition and can confirm the fix, please let us know in the comments box at the bottom of the page.

TL;DR
AI applications in security are clear and potentially useful, however AI based products offer a new and unique attack surface. Namely, if you could truly understand how a certain model works, and the type of features it uses to reach a decision, you would have the potential to fool it consistently, creating a universal bypass.

By carefully analyzing the engine and model of Cylance’s AI based antivirus product, we identify a peculiar bias towards a specific game. Combining an analysis of the feature extraction process, its heavy reliance on strings, and its strong bias for this specific game, we are capable of crafting a simple and rather amusing bypass. Namely, by appending a selected list of strings to a malicious file, we are capable of changing its score significantly, avoiding detection. This method proved successful for 100% of the top 10 Malware for May 2019, and close to 90% for a larger sample of 384 malware.

Read the full post to understand the research process itself, the inner workings of an advanced AI based EPP and how we found the universal bypass.

Another Brave New World
Once every few years, the cyber security world is blessed with the birth of a baby silver bullet. It starts small, with a few enthusiastic parent companies hailing the newborn prince as our savior, telling the stories of its invincible power. A few years and millions of marketing dollars later, it grows and becomes an absolute powerhouse. The mere whisper of its name adds trust to your product, gets you appearances in the most influential conferences, and helps seal that much-needed funding round.

With time, the silver appears to be just coating that soon starts wearing off and some brave souls start seeing it for what it is — another tool, hopefully an effective one, in the never-ending process which is cyber security.

Such is the story of many “silver bullets” we have seen over the years, and inevitably such will be the story of AI and machine learning.

AI has been touted as the silver bullet to end them all with significant marketing force — after all, if we can teach a machine to think like a human analyst, only with the addition of big data and almost infinite processing power, then surely, we will be able to create an invincible mechanism. A brain so powerful that it could not be fooled by any other mechanism.

Right…

In this post we will show how we can reverse the model of an AI based EPP product, and find a bias enabling a universal bypass. We chose Cylance for practical reasons, namely, it is publicly available and widely regarded as a leading vendor in the field.

However, we believe that the process presented in this post can be translated to other pure AI products as well.

AI for Millennials
AI is an extremely important and fascinating technological field with profound implications for human society. However, we are not AI experts, heck, we’re not even power users. With that disclaimer in mind, let’s try to understand how AI works conceptually, so we can hypothesise later on how we may fool it.

In the context of endpoint protection, we are faced with a classification problem. Given a file, we need to classify it as either malicious or benign.

There are many approaches to this, but generally speaking, you are trying to train an artificial brain to identify certain properties of the subject and then apply some form of mathematical model to calculate whether what you are looking at is a certain object.

Let’s assume for example, that we are interested in having a machine classify objects as either birds or human beings.

A specific AI model may look at certain attributes of the object like weight, height, whether it has a beak, wings etc. to make a decision. By exposing a model to numerous samples of birds and human beings, the differences will start showing. For example, most human beings that we are aware of do not have beaks or wings. Thus, if something possesses either of those attributes, there’s a high likelihood that it is a bird. This is of course an oversimplification of a truly beautiful field of knowledge, but it will suffice for now.

By applying the same approach to classification of files as malicious or benign, we get clear and substantial benefits:
- Prediction by design — a well-trained model should have the ability to identify a malicious file it has never seen and has no prior knowledge of.
- Infrequent updates — a model is trained once and can last years without updates.
- Lower resource consumption — AI vendors claim that the nature of their technology leads to lower CPU, memory and disk consumption.

Overall, AI should help you detect more threats, earlier, while incurring a lower management and computing resource overhead for your organization. Can I have two please?

Data Nerd vs. Hacker Mindset
Hackers will always try to find the most economical way to get their way.

Going back to the birds and human beings classification problem discussed above — if we understand that the AI model relies heavily on the existence of a beak and wings to make a decision, we can go buy ourselves a goofy bird costume.

If, on the flip side, the model looks at brain weight to body weight ratio, we’ll need a completely different trick.

Whatever the important attributes may be, if we identify them, we have a good chance of consistently defeating the artificial brain — that is, make malicious files look benign through some sort of treatment.

A Chatty Log
All right, time to roll our sleeves and start our research, exciting!

As a first stage we would like to create a process to determine the classification of a given file, as determined by the Cylance engine. This would allow us to understand at a later stage if we were capable of fooling it.

By activating verbose logging we can capture high-resolution information that Cylance is kind enough to provide. We always say that a verbose software is like a drunk stranger in a bar, it will tell you about all of his problems. Here is an excerpt from the log file, indicating a detection of a malicious file (Mimikatz with a single modified byte):

AnalyzeFile hashed C:\Users\Administrator\Desktop\mimikatz_with_slight_modification.exe 143020851E35E3234DBCC879759322E8AD4D6D3E89EAE1F662BF8EA9B9898D05
LocalAnalyzeItem LocalInfinity.ComputeScore begin
LocalAnalyzeItem, C:\Users\Administrator\Desktop\mimikatz_with_slight_modification.exe score -852 detector execution_control
Detected as 'Unsafe'! path:'C:\Users\Administrator\Desktop\mimikatz_with_slight_modification.exe' hash:143020851E35E3234DBCC879759322E8AD4D6D3E89EAE1F662BF8EA9B9898D05 

We can see that the engine has a scoring mechanism and our modified Mimikatz was scored -852. By empirically testing various good and bad files we later determined that the score can range from -1000 for the most malicious files, and +1000 for the most benign of files.

Good job Cylance, you identified a mutated Mimikatz, already putting you well beyond 50% of the endpoint protection products out there (don’t believe us? Compare original Mimikatz with one-byte modified Mimikatz).

Hold tight though, we are just getting started.

Diving In
We now had a clear objective — understand Cylance’s scoring mechanism, so we can later bypass it.

To do so, we started by reverse engineering the code, which was of course obfuscated, but constructed in a clear way, making it possible to follow.

We also found some publicly available information of the inner workings of the product from patent submissions and public talks. One of these resources describes the engine as being an ‘Ensemble’ (which is a group of models) and we did find a class named ‘EnsembleReader’. It is used to load the ensemble from a resource embedded in one of the DLL files. The extracted model is encrypted, however, the algorithm and quite original key are rather clear:

 public class EnsembleReader : ILogAccess, IEnsembleHeader, IDisposable
  {
    protected const int RandomHeaderSize = 3072;
    protected const string KeyAndIv = ""I am decrypting Cylance's intellectual property."";
    protected Stream _stream;
    protected byte[] _activeKey;
    protected byte[] _activeIV;
    protected bool _loadSectionData;

By following the trail of how the model information is used, we reached the ‘SampleScore2PE’ assembly. Its name suggests it scores PE files, which naturally piqued our interest. We looked at the exposed interfaces of that assembly and found the following gem:

public ISampleScore Create(string inputModelsPath)
{
    return (ISampleScore) new SampleScoring2PE(inputModelsPath);
}

Before discovering this interface we were already planning on using the old school way of revealing the secrets of the model file — use the key to decrypt the model and painstakingly analyse it for days or weeks. Instead, we realized that we can just build our own tiny .NET executable and link against these assemblies much like Cylance.Engine.Core does. We then called the Create() function with a path to the model file we’ve extracted earlier (still encrypted, BTW), and we got an object that exposes a ComputeScore() function.

Can this get any better (it does actually)?

SampleScoreFactory2PE factory = new SampleScoreFactory2PE();
SampleScoring2PE scorer = factory.Create(""test_model.bin"") as SampleScoring2PE;
Stream test_file = File.Open(""mimikatz_with_slight_modification.exe"", FileMode.Open);
Dictionary<string, object> extraData;
double score = scorer.ComputeScore(test_file, out extraData);

By executing the code above, we received a score of -0.852764 as output for our modified Mimikatz file, which looks awfully similar to the -852 we noticed in the log files earlier.

This may not seem like much, but this is actually a very solid basis for our research. We now have an easy process to test PE files against Cylance’s scoring engine, and more importantly, we have a starting point for dynamically debugging the scoring process.

Features Galore
We used a combination of static and dynamic analysis techniques to study the scoring process. The beginning of the classification journey starts with examining and measuring different features of the target object. Going back to our bird vs. humans example, this is the part where you would take measurements, check the existence of wings and beak, and quantify other aspects of the subject being classified.

Cylance uses a combination of code and data from the model itself to produce the feature vector.

The PE file is first extensively parsed to produce a vast amount of different properties of the file. Some are simple, such as the number and names of sections, while others are more complex observations that require a bit of processing to produce. For example, testing if the PE has a correct checksum field, counting the amount of instructions in the entrypoint and the number of imports related to process injection.

The next major step in the classification process is to turn these extracted properties into a feature vector (AKA: feature extraction). As there are thousands of features (7,000 to be precise), we did not bother enumerating all of them. Instead, we focused on the general process that Cylance uses to transform plain file properties into a feature vector.

There are thousands of lines of code handling this transformation, but the overall logic is the same: the engine takes an input property and compares it against a known value or a list of values. One comparison for examples compares the TimeDateStamp field from the File Header of the PE file against a list of 3523 different ranges of timestamps. Depending on what range the timestamp falls into, the engine executes a certain action.

That action is really just a sequence of instructions to increment or decrement values of the feature vector. Each action can affect one or more values, and the list of instructions is stored in the model’s data.

Let’s analyse one example:
if ( ! this . method_28 ( this . imagePEFile_0 . ImageNTHeader . FileHeader . TimeDateStamp , 2 , 3523 ) ) this . method_14 ( 2866581 , 0 ) ;

In this snippet, we first call method_28 which will try to search for the extracted TimeDateStamp property in 3523 different time-date ranges. Each range has a corresponding action attached to it. If the property is not found in any of the time-date ranges, method_14 will be called which will trigger an action, designated for instances where this property is not found to be in any of the “known” ranges.

At the end of this very long process, after executing countless actions, we end up with a vector containing 7000 feature values. This feature vector is the extract of the PE file, and it alone will determine the score and classification of the file.

The next phase is to apply the model to the extracted feature vector. The process starts with normalization and additional post-processing of the feature vector, transforming it into a format that is usable mathematically (we won’t go into full details here).

Then, the engine uses 3 different matrices that are part of the model’s data to transform the feature vector into a single value, which is the final score of the file. As discussed, we are not machine learning experts, but we have seen academic papers suggesting it is possible to approximate a neural network using matrix multiplication, and it all seems to add up: in order to improve performance, Cylance likely created an approximation of their model in the form of several matrix multiplications. After each multiplication, the engine applies an activation function (tanh / sigmoid). This seems to be a very common technique used in neural networks to introduce non-linearity into the model in order to allow computation of non-trivial problems.

How were these matrices calculated in the first place though?
We can’t be sure, but by combining what we’ve learned from analysing the product, open source information, and the little we know of how AI is used in other industries, we can draw a plausible explanation of the process that was used to create these matrices.

Cylance probably started by collecting a huge repository of malicious and benign files. They then generated a very large feature vector for each, much larger than the final 7000 features.
Using statistical models, they reduced the list of features down to the most meaningful 7000 (AKA: feature selection).
Then, they created sets of deep neural networks with varying configurations such as the number of nodes, layers, activation functions etc, and trained the models on the repository of files. Using an iterative Darwinistic process, they continued removing and exploring configurations, until those that exhibited the best properties remained (e.g. high accuracy, low false positive rate).

The next step would have been to approximate the model using matrix multiplication, the essence of which are the matrices we see in the model data.

Going back to the code, we can see that the first matrix multiplication uses the 7000 feature vector as input and outputs 256 2nd order features. These are then fed into another layer of neural network, represented as another matrix, yielding an additional set of 256 3rd order features. The final layer of the neural network is approximated using the last matrix multiplication which results in a single number, representing the final score of the file.

These matrices and associated feature extraction processes are stored in the model file and shipped with the product we tested against. This is the “model data” we were referring to earlier, and it represents the essence of the Cylance engine itself.

Paranoid Centroid
The end product of the lengthy process of file scoring is a number in the range of -1 to 1, indicating how malicious the file is. As always, real life challenges make things more complicated: after scoring is completed using the described method, another mechanism comes into play with an override power over the earlier model.

We can only speculate as to why this mechanism was introduced, but we believe that Cylance’s team encountered some false positives and false negatives in the main model.
They could have probably adapted or improved their model but maybe they had time pressure, so the R&D team had to come up with something quick that would make the problem go away.

Whatever the drama behind the scenes may have been, they introduced an additional mechanism that is designed to target specific families of executables with the power to override the decision made by the previous model. That mechanism is called “Centroids” and it is commonly used for clustering objects. Its use in Cylance’s case is similar to a list of exceptions: when the model classifies a file, we use the centroids to check whether it is white or black listed.

To accomplish that, the engine uses a different feature vector, which uses a modified set of feature values. It normalises the values of those features by centering each around zero, with values ranging between -3 and +3, and then calculates the Euclidean distance between the resulting vector and some known, pre-calculated ones. Effectively, it tries to find if the executable is very similar to ones that were added to the model’s white/black list.

If the executable is exactly the same, the feature vectors are equal and the distance is 0. If some features are different though, the distance could grow, and if grows beyond a given threshold, it is not considered in the same cluster as the white or black listed centroid.

Crossroads
With a fair bit of knowledge of how the model works in hand, we hypothesized as to how we can actually circumvent and confuse the engine.

Our first hypothesis was to try to make a malicious PE look like one of the files in the whitelist. That is, force the relevant features into the right distance from a white listed centroid. We quickly realized that this technique has little chance to work as this mechanism relies on thousands of features, some of which are extremely hard to modify.

Our second hypothesis was that perhaps we could find some bias in the model itself: a small set of features that have a significant effect on the outcome. If we could change the malicious binary so that these features resemble good files, maybe the model will be fooled.

We took another long look at the list of features and tried to estimate the work required to take a malicious file and modify it. It felt overwhelmingly difficult, as there were thousands upon thousands of features. That is, until we came across the following lines of code:

for ( int index = 0 ; index < this . imagePEFile_0 . Strings . Length ; ++ index ) { if ( ! this . method_26 ( this . imagePEFile_0 . Strings [ index ] . S , 95088 , 854069 , 0 ) ) this . method_14 ( this . list_0 [ 0 ] , 15166118410741992125UL , 2847678 , 0 ) ; }

These lines traverse through the entire list of strings found in the file, calculate their hash (using Murmur64), and then searches for them in a large DB containing string hashes (all part of the model’s data). For each string that is found, an action is triggered, affecting the feature vector.

Lift-Off
The latest discovery led us to believe that there might be some inherent bias in the model itself: many features that were included in the feature set (and remember: that’s the only value that matters) are a direct result of what strings exist within the executable. We can now differentiate between two types of features: ones that are based on strings, and ones that are based on some other property of the executable.

The string features are easier to manipulate and adding them to an executable should not be too difficult. Other properties can be much harder to manipulate. For example, reducing the amount of sections is rather hard, and requires work which is very specific to the executable itself.

So strings are a good direction, but which are our beak and wings? What strings would normally make someone look at a file and say “yep, looks benign to me”.
We were able to decrypt and parse the model’s data and retrieve the list of hashes of strings and associated actions. However, we didn’t have the actual strings themselves. This is when we had one of those eureka moments, without the bathtub.

By re-examining the centroids mechanism, we could observe the families of executables that Cylance’s team whitelisted. Perhaps a whitelist entry has been created at some time, after which a full re-train of the model was performed to correctly classify that family? If so, perhaps the model will be more biased towards strings taken from these types of executables.

We looked at the list which included only a few dozen centroids. Each centroid definition carried a name for identification and one of them stood out as it was a name of an online game. A quick conversation with one of the kids verified that it’s a well-known one. At the very least, it was popular enough to cause headaches and trigger special treatment from Cylance.

We purchased the game, and extracted all the strings contained in the main executable using a simple “strings” command, resulting in approximately 5 MB of strings. We then tried to find out exactly how the strings are parsed in order to better understand how we can make the parser pick them up. That is when we had our second eureka/lazy moment: let’s try the most naive solution and slap the strings onto the end of the file. That wouldn’t possibly ever work, would it?

We used the Mimikatz version that received a score of -852 and executed the following command:

copy /b mimikatz.exe+strings.txt modified_mimikatz.exe

We then fed it back into the scoring mechanism and had our OMG² moment — Score is now 0.9998944… (= 999). This is almost a perfect score.

Did we just find a potential shortcut to kill the model? Is this the cheap bird costume we were after?

We went ahead and tested our solution with additional known malicious files and hacking tools and got consistent results — the score changed from a strong negative to a strong positive. We also confirmed that no other mechanisms were testing the behaviour of the files dynamically and blocking them, by executing these modified versions successfully on VMs running Cylance.

Our conclusion was that we managed to find a universal bypass. That is, a simple, passive method we can apply to almost any malicious executable to turn it into a FUD (fully undetected).

We have lots of experience in bypassing endpoint protection products, and we were always able to take an executable and a specific antivirus product and modify it in a way that will make it pass under the radar, but it was always very specific to the executable and required time and expertise. There are packers that can turn an executable into a FUD, but the process always involves staging which complicates things and suffers from compatibility issues. With this approach, we were able to apply the same simple solution to any executable to turn it into a FUD. All we had to do is append a specific set of known strings.

Fine Tuning
After our initial success, we were interested in narrowing down the list of strings we use, by filtering out strings that were not part of Cylance’s model.

On one hand we parsed the relevant tables from Cylance’s model to have the full list of string hash values considered by the model. On the other hand, we calculated the hash for all the strings from the game. Combining these two lists together we managed to reduce the size of the added strings (“special sauce”) to a mere 60 KB. We have confirmed that reducing the size of the list did not affect our universal bypass. By appending those 60KBs to any executable we can change its score drastically, although as we soon found out, some were still detected as malicious, albeit with significantly better scores.

Exam Time
Our first tests were against what we would consider the usual suspects — Mimikatz, ProcessHacker, Meterpreter etc, and proved successful. It was time to up the game and look at a wider test group.

We started with a list of the top ten malware as of May 2019, published by the Center for Internet Security.

The results were staggering:

As can be clearly seen, almost all of these samples have changed from the most evil file on the planet, to your friendly neighborhood file. Again, the only treatment applied to these files, is the addition of the “special sauce” as a simple concatenation. Widening further our test group, we downloaded a list of 384 malicious files from online repositories and ran the test, receiving the following results:

Average score before secret sauce: -920
Average score after secret Sauce: 630
Average delta: 1550 (out of a maximum of 2000)
Percentage of files bypassing detection: 83.59%

We also realized that if we add the secret sauce multiple times, we can improve the score even further. With this technique we achieved an average score of 750, and a whooping 88.54% of our malicious files were now marked as benign.

Impact and Final Thoughts
We are always amused to see the shock on people’s faces when you tell them that the new security toy they spent millions of dollars buying and integrating can be bypassed. The same goes for new silver bullets, like AI based security. We are anything but surprised with the results, and we are confident that the same type of process can be applied to other pure AI vendors to achieve similar results.

Why?

Vendors too often approach the security problem with a one punch solution. Hackers are not wooden dummies, they fight back, and you have to be ready for the counter-punch, constantly innovating and increasing the cost of attack.

The concept of a static model that lasts for years without update may hold theoretically, but it fails in the arena.

Granted, it is harder to find a bias in an AI model than to bypass a simple AV signature, but the cost of fixing a broken model is equally expensive.

We believe that the solution lies in a hybrid approach. Using AI/ML primarily for the unknown, but verifying with tried and tested techniques used in the legacy world. This is really just another implementation of the defense in depth concept, applied to the endpoint protection world.

This means that the promise of a pure AI product may not be realized for EPPs, and vendors will have to maintain and update multiple systems of detection.

The promise of low resource consumption, with rare update cycles does not hold true for such a hybrid product, but it does provide a superior protective capability.

Till the next silver bullet…","Cylance, I Kill You!",https://skylightcyber.com/2019/07/18/cylance-i-kill-you/
"[""Kim Zetter""]",2021-08-23,2021-08-23,2019-07-18,2021-08-23,"https://video-images.vice.com/articles/5d3086d9f078ec0008738d60/lede/1563463617364-image3.jpeg?crop=1xw:0.9995xh;center,center&resize=640:*",2019-07-18,0,en,,www.vice.com,"[""Kate Perkins""]","By taking strings from an online gaming program and appending them to malicious files, researchers were able to trick Cylance’s AI-based antivirus engine into thinking programs like WannaCry and other malware are benign.

Artificial intelligence has been touted by some in the security community as the silver bullet in malware detection. Its proponents say it’s superior to traditional antivirus since it can catch new variants and never-before-seen malware—think zero-day exploits—that are the Achilles heel of antivirus. One of its biggest proponents is the security firm BlackBerry Cylance, which has staked its business model on the artificial intelligence engine in its endpoint PROTECT detection system, which the company says has the ability to detect new malicious files two years before their authors even create them.

But researchers in Australia say they’ve found a way to subvert the machine-learning algorithm in PROTECT and cause it to falsely tag already known malware as “goodware.” The method doesn’t involve altering the malicious code, as hackers generally do to evade detection. Instead, the researchers developed a “global bypass” method that works with almost any malware to fool the Cylance engine. It involves simply taking strings from a non-malicious file and appending them to a malicious one, tricking the system into thinking the malicious file is benign.

The benign strings they used came from an online gaming program, which they have declined to name publicly so that Cylance will have a chance to fix the problem before hackers exploit it.

“As far as I know, this is a world-first, proven global attack on the ML [machine learning] mechanism of a security company,” says Adi Ashkenazy, CEO of the Sydney-based company Skylight Cyber , who conducted the research with CTO Shahar Zini. “After around four years of super hype [about AI], I think this is a humbling example of how the approach provides a new attack surface that was not possible with legacy [antivirus software].”

The method works because Cylance’s machine-learning algorithm has a bias toward the benign file that causes it to ignore any malicious code and features in a malicious file if it also sees strings from the benign file attached to a malicious file—essentially overriding the correct conclusion the detection engine should otherwise make. The trick works even if the Cylance engine previously concluded the same file was malicious, before the benign strings were appended to it.

The researchers tested their attack against the WannaCry ransomware that crippled hospitals and businesses around the world in 2017, as well as the more recent Samsam ransomware, the popular Mimikatz hacking tool, and hundreds of other known malicious files—adding the same benign strings from the gaming program to each malicious file—and in nearly all cases, they were able to trick the Cylance engine.

Martijn Grooten, editor of Virus Bulletin, which conducts tests and reviews of malware detection programs, called the reverse-engineering research impressive and technically interesting, but wasn’t surprised by the findings.

“This is how AI works. If you make it look like benign files, then you can do this,” Grooten told Motherboard. “It mostly shows that you can’t rely on AI on its own…. AI isn’t a silver bullet…. I suspect it’ll get better at this kind of thing over time.”

A machine learning expert Motherboard spoke to agrees.

“Usually you try to work with machine learning to cover … things which are widely unknown or you cannot do manually,” said the expert, who asked to remain anonymous because his company doesn’t authorize him to speak with media. “And it usually works pretty well, until you have some corner cases where you can’t just make the model [work].”

Though he doesn’t fault Cylance for making a mistake, he does fault the company for hyping the AI in their marketing when the system contains a bias that essentially undermines the AI.

“Their crime is not that they coded AI poorly. Their crime is calling what they did AI,” he told Motherboard.

Cylance ranks about eight among the top ten endpoint security companies, after Symantec, Kaspersky and TrendMicro. But the company’s business is growing rapidly; last year it obtained $120 million in funding and this year was acquired by BlackBerry in a $1.4 billion deal.

Cylance’s PROTECT isn’t the only security product that uses artificial intelligence. Other firms like Symantec, Crowdstrike, and Darktrace use it too, but Ashkenazy and Zini didn’t test those systems and it’s not clear they would suffer from the same bias, since they’re architected differently and don’t rely as heavily on machine learning to detect malicious files as the Cylance system does.

“One of [Cylance’s] selling points… they say no more running after signatures and updates. We train the model once, and … you won’t have to train the model again for a couple of years. It’s very compelling, if it actually works,” Ashkenazy said.

But to fix the problem he and his colleague found in the Cylance engine, the company will have to retrain the system, which could be a “costly and complex process” Ashkenazy said.

Artificial intelligence has several advantages over traditional antivirus. In traditional systems, the vendor has to analyze each new file and push out new signatures or heuristics to their scanners to detect it. (Signatures look for specific strings of code or data that are unique to a piece of malware; heuristics look at the activity the code is engaged in to spot actions that are characteristic of malware.) But, according to Cylance, its engine doesn’t require an update every time new malware, or variants of existing malware, are discovered. Machine-learning detection systems are supposed to recognize not only known malicious files and activity but also spot new ones.

In a test conducted by SELabs and commissioned by Cylance, a version of its 2015 software had the ability to detect variants of the Cerber ransomware and other malicious programs that didn’t appear in the wild until 2016 and 2018.

To determine if a file is malicious or benign, the Cylance engine looks at 4 million different features or data points, according to Ryan Permeh, founder and chief scientist of Cylance. These include things like the size of the file, structural elements present, and entropy (the level of randomness), etc. Cylance programmers then “train” the engine by showing it about a billion malicious and benign files and tweak the system to hone its detection. But during training, the system also examines the files for patterns to see how malware variants evolve over time to anticipate how new malware might look—essentially “predicting” what malware authors will do before they do it. Models do get retrained, Permeh says, but only about every six months, and users only have to update their software if they want the latest features and performance improvements.

But none of this training and testing matter if the algorithm has a bias that is also training it to ignore what it learns from that other training. That’s essentially what the Skylight Cyber researchers discovered.

They purchased a copy of the Cylance program and reverse-engineered it to figure out what features or data points the agent was looking at to determine if a file is benign or malicious and they also studied how these features are weighed to arrive at the score the program gives each files.

The Cylance system analyzes each file based on these data points, and assigns a score to the file that ranges between -1,000 to 1,000 (with -1,000 being a file with the most or worst malicious features or data points in it). Scores are visible in the program’s log file.

When they saw how many features the program analyzes, the researchers worried initially that it would take them weeks or months to find the ones that carried the most weight in the algorithm’s decision process. That is, until they discovered that Cylance also had whitelisted certain families of executable files to avoid triggering false positives on legitimate software.

Suspecting the machine learning might be biased toward code in those whitelisted files, they extracted strings from an online gaming program Cylance had whitelisted and appended it to malicious files. The Cylance engine tagged the files benign and moved their scores from high negative numbers to high positive ones. The score for Mimikatz went from -799 to 998. WannaCry went from -1000 to 545. The researchers liken it to donning a mask with a beak and having a facial recognition system identify you as a bird, ignoring all other characteristics that indicate you’re just a person wearing an artificial beak.

They tested the top ten malware programs cited by the Center for Internet Security, then broadened their test to include 384 additional malicious files taken from online repositories of malware. The average score before they appended the benign strings from the whitelisted gaming program was -0.92. After adding the strings, the average score was 0.63. About 84 percent of the files bypassed detection once they added the gaming strings, though some files still got tagged malicious, but with significantly changed scores than before.

They didn’t just run the files against the static Cylance program - they executed the malicious files on a virtual machine with Cylance PROTECT running on it, to see if it would catch the malicious files in action. The theory was that even if the product was tricked by the strings, the malicious action of the file would still be detected by Cylance, but it wasn’t.

Ashkenazy said the use of whitelisting in an AI program is odd, but understands why Cylance did it, if its engine was creating false positives on those programs. The real problem, he said, was giving the whitelisted programs more weight in the algorithm’s scoring, causing them to override a decision the algorithm would normally reach if a file didn’t have the benign strings appended to it. He also said that not using backup signatures or heuristics to doublecheck the algorithm’s conclusion, and relying on the AI instead, caused the failures.

Permeh, who is also the architect of Cylance’s machine-learning engine, said they do use signatures and hard-coded heuristics in their product as well and don’t entirely rely on the machine-learning, but the AI does take precedence in detection.

He acknowledged to Motherboard the potential for the kind of bypass the researchers found, however.

“Generally speaking, in all of these AI scenarios, models are probabilistic. When you train one, you learn what’s good and what’s bad…. By training for what is a good file, we learn attributes of that… [and] it’s entirely possible that we overestimated the goodness of that,” he told Motherboard in a phone call. ""One of the interesting parts of being basically the first to take an AI-first approach, is that we’re still learning. We invest a lot in adverse research, but this is still an evolution.”

Contrary to what Ashkenazy said, Permeh doesn’t think it will take long to retrain the algorithm to fix the issue once he knows the details of the global bypass. Ashkenazy didn’t contact Cylance before contacting Motherboard to disclose the issue.

But Ashkenazy thinks the issue will take more time to fix than Permeh believes.

“The bias towards games and those features is there for a reason,” Ashkenazy said. “They were getting false positives for games, so retraining without sacrificing accuracy or false positive rate can’t be that simple.”

In the end, Ashkenzy doesn’t think Cylance is at fault for using machine learning, just for hyping it and relying on it so heavily for detection.

“I actually think they did a decent job applying current AI technology to security,” he told Motherboard. “It just has inherent flaws, like the possibility of having an exploitable bias which becomes a global bypass with a costly fix.”",Researchers Easily Trick Cylance's AI-Based Antivirus Into Thinking Malware Is 'Goodware',https://www.vice.com/amp/en/article/9kxp83/researchers-easily-trick-cylances-ai-based-antivirus-into-thinking-malware-is-goodware
"[""Our Mission"",""To Protect Every Computer"",""User"",""Thing Under The Sun.""]",2021-08-23,2021-08-25,2019-07-18,2021-08-25,https://blogs.blackberry.com/content/dam/blogs-blackberry-com/images/blogs/2019/07/Cylance-Blog-2.png,2019-07-18,0,en,,blogs.blackberry.com,"[""Kate Perkins""]","BlackBerry® Cylance® is aware that a bypass has been publicly disclosed by security researchers. We have verified there is an issue with CylancePROTECT® which can be leveraged to bypass the anti-malware component of the product.

Our research and development teams have identified a solution and will release a hotfix automatically to all customers running current versions in the next few days.

More information will be provided as soon as it is available.

***UPDATE - SUNDAY 7/21/2019: Resolution for BlackBerry Cylance Bypass

BlackBerry Cylance has verified the issue was not a universal bypass as reported, but rather a technique that allowed for one of the anti-malware components of the product to be bypassed in certain circumstances. The issue has been resolved for cloud-based scoring and a new agent will be rolled out to endpoints in the next few days. MORE INFO HERE.",BlackBerry Cylance Guidance on Bypass Disclosure,https://blogs.blackberry.com/en/2019/07/blackberry-cylance-guidance-on-potential-bypass
"[""The Guardian""]",2021-08-26,2021-08-26,2021-06-11,2021-08-26,https://i.guim.co.uk/img/media/ccea7cac8b526274893ac78a5831bfcc8ad5c504/0_429_4282_2569/master/4282.jpg?width=445&quality=45&auto=format&fit=max&dpr=2&s=242e00f2d245d44073d5bd6c0bc8a2ce,2017-01-04,57,en,,www.theguardian.com,"[""Anonymouse""]","Luke Henriques-Gomes
Robodebt: court approves $1.8bn settlement for victims of government’s ‘shameful’ failure
A “shameful chapter” in public administration has led to the federal court approving a settlement worth $1.8bn between the commonwealth and victims of the Coalition’s robodebt scheme.

Justice Bernard Murphy in Friday’s judgment criticised the federal government’s “massive failure”, noting the court had heard “heart-wrenching” stories of pain and anguish from victims of the Centrelink debt recovery program.

The judge said it should have been “obvious” to government ministers and senior public servants that the debt-raising method central to the scheme was flawed. He said the evidence showed it was unlawful.

The robodebt scheme, which ran between 2015 and November 2019, saw the government unlawfully raise $1.76bn in debts against 443,000 people, the court heard.

Murphy said the government had pursued about 381,000 people, unlawfully recovering $751m, including through private debt collectors. He noted that one mother had linked her son’s suicide to the debt recovery program.

Nathan Kearney at home in Brisbane
'Robodebt-related trauma': the victims still paying for Australia's unlawful welfare crackdown
Read more
Gordon Legal brought a class action on behalf of all victims last year after a court ruling in 2019 in response to a Victoria Legal Aid challenge paved the way for a wider case.

In response to the class action, the government has agreed to repay at least 381,000 people $751m and wipe all debts – worth $1.76bn – that were raised using the unlawful method of “income averaging” tax office data to check welfare payments.

Friday’s settlement “gives legal effect” to this pledge and also adds $112m in interest, which will be shared between 394,000 victims, depending on the size of their debt and how long they were without their money.

“The proceeding has exposed a shameful chapter in the administration of the commonwealth social security system and a massive failure of public administration,” Murphy said.

But the judge did not think there was evidence that proved the government knew the scheme was “unlawful” when it was established.

“I am reminded of the aphorism that, given a choice between a stuff-up, even a massive one, and a conspiracy, one should usually choose a stuff-up,” Murphy said.

About 200,000 people originally included in the class action will not receive any benefit from the settlement.

Murphy said those people’s debts had eventually been substantiated using their own payslips or other evidence, meaning they were valid and they had owed the money.

He said they would have needed to show that their debts were “tainted with illegality” to be owed compensation, a claim he said had “weak prospects of success”.

In approving the settlement, Murphy also said a more serious negligence claim originally brought by Gordon Legal would have been unlikely to succeed. The government did not accept legal liability in settling the case.

Some 680 people who objected to the settlement will be allowed to opt-out. The court heard earlier this year that some believed the interest payments were insufficient and the government had not been held accountable for its mistakes.

Murphy said the objections showed cases of “financial hardship, anxiety and distress, including suicidal ideation and in some cases suicide” that they said they suffered from the scheme. They also felt shame from being branded “welfare cheats”.

“It is plain enough that many group members continue to feel a great deal of anguish, upset and anger at the way in which they or their loved ones were treated,” Murphy said.

The judge said the government “ought to have ensured that it had a proper legal basis” to raise debts, noting many welfare recipients were “marginalised or vulnerable and ill-equipped” to challenge an overpayment.

“The proceeding revealed that the commonwealth completely failed in fulfilling that obligation,” he said. “Its failure was particularly acute given that many people who faced demands for repayment of unlawfully asserted debts could ill afford to repay those amounts.”

Andrew Grech, a Gordon Legal partner, said the firm and its clients were delighted by the outcome.

“We hope that this outcome brings peace of mind and some certainty to all class action members and acts as a strong deterrent against similar callous welfare practices for both present and future governments,” he said.

Bill Shorten, who announced the Gordon Legal class action in 2019, said a robodebt royal commission was now “inevitable”.

'It should not have taken so long': robodebt took a huge toll – there must be real accountability
Read more
“You can’t make a $2bn compliance fail, and no one’s lost their job, no one’s accountable,” he told Guardian Australia.

Shorten said the court’s suggestion the scheme was more likely a “stuff up” than “conspiracy” meant the government had been “shamefully stupid, not shamefully bad”.

“But when are you so recklessly stupid that it becomes bad?” he said.

Greens senator Rachel Siewert, who has long campaigned against the robodebt scheme, also called for a royal commission. “Robodebt cost lives, it has ruined many many more and has been the cause of immeasurable pain and anguish,” she said.

Guardian Australia revealed in March last year that the government was drawing up plans to repay victims of the scheme because legal advice showed it would otherwise lose in court.

Murphy approved $8.4m to be deducted from the settlement for Gordon Legal’s costs to date but wanted more evidence before agreeing to a further $4.2m for fees for distributing compensation.

",Robodebt: court approves $1.8bn settlement for victims of government’s ‘shameful’ failure,https://www.theguardian.com/australia-news/2021/jun/11/robodebt-court-approves-18bn-settlement-for-victims-of-governments-shameful-failure
"[""The Guardian""]",2021-08-26,2021-08-26,2019-11-27,2021-08-26,https://i.guim.co.uk/img/media/bf10db343ec886be8d71488c85d47b76e1ba0005/0_177_2362_1418/master/2362.jpg?width=445&quality=45&auto=format&fit=max&dpr=2&s=72c01765027eaf570e59758ad65b9c01,2017-01-04,57,en,,www.theguardian.com,"[""Anonymous""]","Government admits robodebt was unlawful as it settles legal challenge
The federal government has settled a landmark challenge against its robodebt program – conceding a $2,500 debt raised against Deanna Amato was not lawful because it relied on income averaging.

In orders made by consent on Wednesday, the federal court declared the debt was “not validly made”, that an order to garnishee Amato’s tax return was also invalid and there was no basis to add a 10% penalty to the debt.

Rowan McRae, the executive director of civil justice access at Victorian Legal Aid, said the case has “helped to clarify the unlawfulness of the robodebt system for hundreds of thousands of Australians in the same situation, who received or paid off a robodebt based only on averaging”.

Centrelink and Medicare offices in Armidale
Robodebt: government abandons key part of debt recovery scheme in major overhaul
Read more
Last week the federal government abandoned sole reliance on income averaging to calculate debts, dismantling a central plank of the robodebt program’s automation which has seen tens of thousands of welfare recipients overcharged for alleged debts.

Welfare groups, legal centres, Senate inquiries and a former administrative appeals tribunal senior member, Terry Carney, have all warned that income averaging is not a proper basis to claim a debt.

The same argument is being pursued in a class action by Gordon Legal challenging the legality of the entire program.

Amato, a 34-year-old local government employee, found out Centrelink claimed she owed a debt in January after it sent repeated notices to an old address then garnished her $1,700 tax refund.

Amato disputed the debt in the federal court, prompting the commonwealth to reduce her debt to $1.48 in September, but the case continued even after the radical overhaul to the system announced by government services minister Stuart Robert.

In a statement explaining the consent orders, the court noted the initial debt of $2,900 was calculated based on Australian Taxation Office income data averaged across fortnightly periods as if this were Amato’s actual income in each period.

The court said the conclusion Amato had received social security benefits she was not entitled to was “not open on the material before the decision-maker” because there was “no probative material” that the average reflected Amato’s actual income.

Francine Wheeler
'Pay the money back': robodebt, the Coalition's backflip and how it ‘hounded' welfare recipients
Read more
“In the circumstances, there was no material before the decision-maker capable of supporting the conclusion that a debt had arisen … The conclusion that a debt had arisen was therefore irrational, in the requisite legal sense.”

The court ordered the commonwealth to pay Amato’s costs and $92 interest.

Amato said she was delighted at the “amazing” news. “You can feel so small and helpless next to the government, but I am so glad that the unfair and ultimately unlawful aspects of this system have been brought to light,” she said in a statement.

“I had my money refunded to me, but I hope that others who have paid dodgy debts will also have a way to get their money back.

“I feel a weight lifted off my shoulders. I’ve proven my innocence, but also proven that there are reasons why you need all the facts before you can demand debt payments from people.”

McRae said while last week’s changes are “significant” they came “too late for many people who have experienced distress and hardship, or who have already paid robodebts through tax garnishee action and debt collectors”.

“Today’s result shows the federal government has accepted what advocates have been saying for years – using only income averaging to raise debts is both inaccurate and inconsistent with the Social Security Act.”

The shadow attorney general Mark Dreyfus said the judgment “simply confirms that we have got a Commonwealth government that has been extorting money from Australian citizens with no basis for doing so”.

“It’s an extraordinary scandal, robodebt,” he told the ABC. “I just think that the prime minister owes the Australian people an apology for this extraordinary program.”

On Wednesday the Senate passed a motion ordering the government to produce legal advice explaining why it made the changes to the system, with Greens senator Rachel Siewert demanding to know “the legal status of this program”.
",Government admits robodebt was unlawful as it settles legal challenge,https://www.theguardian.com/australia-news/2019/nov/27/government-admits-robodebt-was-unlawful-as-it-settles-legal-challenge
"[""The Guardian""]",2021-08-26,2021-08-26,2020-11-21,2021-08-26,https://i.guim.co.uk/img/media/0ff69d2cdc1f8298e4d7e52bc3f9eadb90a805b5/189_128_3354_2012/master/3354.jpg?width=700&quality=45&auto=format&fit=max&dpr=2&s=d3f75450c6d9aa02d7491afeb6e87f91,2017-01-04,57,en,,www.theguardian.com,"[""Anonymous""]","'Robodebt-related trauma': the victims still paying for Australia's unlawful welfare crackdown
Nathan Kearney says he lost two and half years of his life to robodebt. He’s still seeing a counsellor about it.

Four years ago he was living in Brisbane, seeing and playing gigs, working several different casual jobs and generally enjoying his 20s.

Then the debts came, first for about $2,000, and a year later for $4,500. It was the same story: you have underreported your income, you owe us this much money, please provide us your old payslips. He couldn’t.

Overwhelmed by the idea he now owed the government more than $6,000, Kearney moved back in with his parents in East Gippsland.

I wanted to know why those ministers felt that it was appropriate to use this illegal system and to target the most vulnerable
He was 27, dreams on hold, while he worked 50 hours a week in a country town he didn’t like just to regain a sense of financial stability.

“I feel like I got put back a couple of years in life because of this,” Kearney says. “And I would be closer to where I want to be at 31 years old if it hadn’t been for robodebt.”

On Monday the government reached a settlement with Gordon Legal, a law firm running a class action on behalf of hundreds of thousands of people caught up in the Coalition’s government welfare debt recovery program.

The settlement was for an eye-watering $1.2bn but almost all of the money came from the government’s May announcement to repay and wipe debts raised using the unlawful “income-averaging” of ATO pay data.

In fact, the only new figure was $111m in compensation that will be shared between about 430,000 victims.

The sums will vary significantly, depending on how much debt people had paid and how long they’d been without their money. Legal costs, which will be deducted from the compensation, are yet to be determined.

Many people have flooded victims’ group Facebook pages, as well as Gordon Legal’s own page, to express frustration at the compensation figure, which they feel does not reflect the pain or suffering the four-year program caused.

Moreover, some are angry that a scandal they see as being punctuated by continual cover-up and obfuscation never made it to a courtroom.

“I wanted to know why those ministers felt that it was appropriate to use this illegal system and to target the most vulnerable people,” Kearney says. “I wanted somebody to ask them to their faces: ‘Why did you think that it was OK to take money from the poorest people without giving them a chance to argue their case?’”

Others, like Jennifer Miller, whose son Rhys Cauzzo took his life when he was 28, say they intend to object to the settlement, which will need to be approved by the court. “There has been no accountability whatsoever,” Miller says.

Cauzzo lived with depression and anxiety but Miller believes the financial pressure that came from two Centrelink debts tipped him over the edge on Australia Day 2017.

Along with Kath Madgwick, whose son also took his life after receiving a Centrelink debt, Miller has been campaigning against the robodebt scheme for three years.

Like Kearney, she is insistent the case should have gone to court. “This isn’t over,” she says.

Gordon Legal emphasised this week that the settlement – an “excellent outcome” for clients and group members – should be viewed in its full context.

The robodebt settlement leaves many victims out in the cold once again
Read more
“When you think about the totality of what’s been achieved since the proceedings were commenced, that really amounts to more than $1.2bn,” said a partner at the law firm, Andrew Grech.

It was the Amato case brought by Victoria Legal Aid that established the legal precedent that ruled that robodebt’s “income averaging” unlawful.

But in the months afterwards, the government simply stonewalled. It said nothing about refunds and claimed only a “small cohort” of people had been affected.

Indeed, the refund decision in May, which Guardian Australia revealed two months earlier, was prompted by the need for a strategy to respond to the Gordon Legal class action.

In the lead-up to Monday’s announcement, it was clear that a settlement was looking likely.

And the $111m figure was said, from the government’s perspective, to represent the interest owed, rather than compensation for stress or anguish.

The government services minister, Stuart Robert, later publicly confirmed the government’s view of the compensation, noting it was for “the most part, for interest payments for money held”.

The commonwealth “has not accepted or admitted any liability in the matter”, Robert noted.

Robodebt: this is my list of who should be held to account
First Dog on the Moon
First Dog on the Moon
Read more
Though victims longed for their day in court, the two-week trial promised to be a fairly dry affair based mostly on documentary evidence and legal arguments.

The government did not plan to call any witnesses, so there was little prospect the former human services minister, Alan Tudge, or top departmental officials would need to take the stand.

In addition, the government’s prospects advice from earlier in the year said while it was likely a court would order refunds, plus interest, Gordon Legal’s negligence claim was unlikely to succeed.

Still, for some victims, the government’s lawyers would have been there in the “virtual” courtroom, defending their case.

Gordon Legal’s statement of claim, for example, alleged Centrelink was well aware of the distress the program had caused victims.

Nathan Kearney outside his home
Kearney outside his home in Brisbane. Photograph: David Kelly/The Guardian
Kearney says it was the second debt that caused his mental health to plummet (he lives with chronic depression). He also fell out with some friends, though he notes they’ve since patched things up.

Because he continued to contest his debts, Centrelink eventually sent them off to a private debt collector who would call him three times a day.

Centrelink also garnisheed nearly $3,000 from his tax return.

“Once they [the debt collectors] started calling, they … put you into a certain headspace, which is like a shame and guilt spiral,” he says. “Whether or not you deserve to feel the shame or guilt, it’s still there within me.

“Sometimes they’d call and I’d tell them, like, ‘I can’t deal with this any more. I’ve been thinking about taking my life,’ and things like that. It didn’t change anything.”

Perhaps as many as 100,000 people were also left out of the refunds because they provided payslips or bank statements after being hit with an initially unlawful debt. It was then recalculated and a debt was substantiated.

And Gordon Legal, which had initially argued these debts were “tainted”, dropped that claim in the settlement.

Centrelink office sign
More welfare debts under scrutiny after tribunal rulings cast doubt on Services Australia methods
Read more
Some of those people expressed anger and confusion at that outcome in conversations with Guardian Australia this week.

Other refund recipients, like the man who told the Guardian his debt had been a factor in his marriage breakdown, may only get the sense of justice they seek from a royal commission, as proposed by Labor and the Greens.

“I’ve been seeing somebody to actively to talk through, I guess, ‘robodebt-related trauma’ is the way that they put it, and slowly coming to terms with it,” says Kearney, who is now back in Brisbane.

He used to fear another debt might arrive at any moment.

“Now, with the settlement, it does feel like, ‘Everything’s gone back to normal, they’ve made penance … and everything goes good again,’” he says. “Whereas I think a lot of us are still dealing with the impacts of what they did years ago, even if we got our refunds.”
",'Robodebt-related trauma': the victims still paying for Australia's unlawful welfare crackdown,https://www.theguardian.com/australia-news/2020/nov/21/robodebt-related-trauma-the-victims-still-paying-for-australias-unlawful-welfare-crackdown
"[""Thomas Claburn""]",2021-09-16,2021-09-16,2021-08-20,2021-09-16,"{ ""$undefined"": true }",2020,0,en,,www.theregister.com,"[""Roman Lutz""]","A committee at the University of Texas in Austin has advised against using AI software to oversee students' online tests, citing the psychological toll on students and the financial toll on academic institutions.

Acknowledging that some form of online proctoring is necessary to discourage academic misconduct, the committee concluded, ""we strongly recommend against the use of AI-based software like Proctorio and ProctorU.""

The Report of the Academic Integrity Committee about Online Testing and Assessment, spotted by Megan Menchaca, education reporter for the Austin-American Statesman, is said to have been included in a university official's recent message to faculty.

AI-based software to watch over remote students as they take online tests – ""academic surveillance software"" to detractors – has flourished during the COVID-19 pandemic. Large numbers of students have been studying remotely and schools believe they need a way to prevent cheating.

But the software that's been deployed has been widely criticized by students and privacy advocates. The concern centers around the inability to audit the software source code and the possibility that these systems rely on flawed algorithms and biased or arbitrary signals to label students cheaters.

Critics also worry that the software can't account for varied student living conditions and is vulnerable to racial bias – eg, motion tracking that produces different results with different skin tones – and cognitive bias such as gaze tracking that flags ADHD behaviors as suspicious.

Such criticism last year led UC Berkeley [PDF] and Baruch College in New York to stop using remote proctoring products. In February, the University of Illinois at Urbana-Champaign said it will drop Proctorio after this summer due to ""significant accessibility concerns.""
When in doubt, sue

Amid this backlash, proctoring software maker Proctorio sued critics, alleging last year that Ian Linkletter, a learning technology specialist at the University of British Columbia (UBC) in Vancouver, Canada, violated US copyright law by linking to the company's publicly viewable videos. That case remains ongoing in Canada, and has forced Linkletter to appeal for funds to defend himself through the costly legal process.

Proctorio also last year filed a Digital Millennium Copyright Act (DMCA) takedown complaint against Miami University computer science student Erik Johnson seeking the removal of posts on Twitter that were critical of the company. Twitter removed the posts and later restored them.

The firm's legal crusade prompted pushback from the Electronic Frontier Foundation, which said the company should not be able ""to abuse copyright law to undermine their critics.""

    Uni revealed it killed off its PhD-applicant screening AI – just as its inventors gave a lecture about the tech
    'Meritless': Exam software maker under fire for suing teacher who tweeted links to biz's unlisted YouTube vids
    Twitter uses HackerOne bounties to find biases in its image-cropping AI model
    Zoom incompatible with GDPR, claims data protection watchdog for the German city of Hamburg

The UT Austin committee began working on its report after student councils in the spring of 2021 asked the university to get rid of AI proctoring software, which was used widely during the 2020-2021 academic year.

The committee asked student leaders and faculty to provide information about how the software was employed and decided that it just wasn't worth it.

""The invasive nature of the tools as well as the warnings that the tools may send to the screen during the exam cause high levels of anxiety,"" the report says.

""Although these tools were used extensively by faculty in academic year 2020-2021, only 27 cases were referred to the Student Conduct and Academic Integrity office as potential violations of academic integrity, and of these only 13 were upheld. Thus, the psychological (and financial) costs of the tool do not seem to be worth the small benefit of using it.""
Trust the teachers

The report goes on to suggest alternative methods of watching over students during tests, such as Zoom for small groups, and other academic software like Canvas Quizzes, Gradescope, and Panopto. It also recommends that instructors consider rethinking how they assess student progress in order to reduce online test anxiety.

The University of Texas at Austin, Proctorio, and ProctorU did not respond to requests for comment.

In an email to The Register, Linkletter – still awaiting a ruling on his effort to dismiss Proctorio's copyright complaint under Canada's anti-SLAPP statute, the Protection of Public Participation Act – said what stands out to him from the UT Austin report is the finding that Proctorio just isn't worth it.

""Every institution should be taking a hard look at whether Proctorio is worth the 'psychological cost' mentioned in the report, let alone the expense,"" he said.

""Over half of the 27 students accused had their academic integrity cases tossed. Thousands of students were surveilled, at great expense, for what? How much faculty and staff time was wasted? How much unnecessary heartbreak caused?

""Students understand that surveillance is wrong. They know how the technology works. There is no technical explanation that will reduce the harm being done – it simply needs to stop.

""The only way institutions can demonstrate they are listening to students is to stop using academic surveillance software."" ®
Updated to add

In a statement emailed to The Register Jarrod Morgan, Chief Strategy Officer of Meazure Learning, the parent company of ProctorU, took issue with the UT report and said that his firm gave up on AI proctoring several months ago.

“After our decision and announcement earlier this year to discontinue all Al-only exam monitoring, every single exam from every single test-taker using ProctorU is reviewed by a trained, live proctor,” said Morgan.

“As ProctorU announced at the time, it’s only proctoring if a human does it and, more importantly, it’s the only way to be sure the process is accurate, fair and consistent. Some exam monitoring companies may rely on AI software to monitor exams and score students, ProctorU is not one of them and it is important to understand the differences between true proctoring with a trained live proctor and the less-expensive and in our opinion, unacceptable option of ‘monitoring’ software such as that offered by Proctorio.”","Using 'AI-based software like Proctorio and ProctorU' to monitor online exams is a really bad idea, says uni panel",https://www.theregister.com/2021/08/20/ai_proctoring_software/
"[""Andy Brown""]",2021-10-02,2021-10-02,2021-08-09,2021-10-02,https://www.nme.com/wp-content/uploads/2021/07/steam-deck-library.jpg,2021-08-03,0,en,,www.nme.com,"[""Kate Perkins""]","150 people have been fired from Xsolla – a game payment company used by Steam, the Epic Games Store and other game developers – via a controversial email from CEO and founder Aleksandr Agapitov.

Agapitov contacted 150 fired employees through email on August 3, stating that “my big data team analyzed your activities” and accused them of being “unengaged and unproductive employees”.

The email starts by saying:

“You received this email because my big data team analyzed your activities in Jira, Confluence, Gmail, chats, documents, dashboards and tagged you as unengaged and unproductive employees. In other words, you were not always present at the workplace when you worked remotely.”

Agapitov also states, “Many of you might be shocked, but I truly believe that Xsolla is not for you” and that employees would receive help through separate HR companies. The email ends by listing the names of everyone who has been fired, and says “If you want to stay in contact with me, please write me a long letter about all your observations, injustice, and gratitude”.

All of the fired employees were based in Xsolla’s office in Perm, Russia. In a follow-up press conference reported by App2Top, Agapitov claimed that the layoffs were due to the company no longer showing 40 per cent growth.

After the press conference, Agapitov posted a Tweet which Game World Observer reports as translating to “Work your fucking ass off or get your fucking ass out”.

Xsolla has been accused of spying on employees to gather the metrics they were judged on – HR expert Alyona Vladimirskaya has argued that monitoring productivity like this is “extremely ineffective” and encourages fired employees to sue.

Xsolla provides payment services for large companies in the games industry, including Valve, Twitch, Epic Games and Ubisoft.

In other news, Darksiders III is coming to the Nintendo Switch next month.",Game payment company Xsolla fires 150 employees via controversial email,https://www.nme.com/news/gaming-news/game-payment-company-xsolla-fires-150-employees-via-controversial-email-3014850
"[""Sam Desatoff""]",2021-10-02,2021-10-02,2021-08-09,2021-10-02,https://deifoexkvnt11.cloudfront.net/assets/article/2021/08/09/xsolla-horizontal_feature.jpg,2021-08-03,0,en,,gamedaily.biz,"[""Kate Perkins""]","The news raises a number of ethical questions that are especially pertinent in a pandemic-stricken working environment.

Xsolla, the Russia-based games industry payment solution provider, has reportedly laid off upwards of 150 employees following a productivity audit of the company. Originally reported by Game World Observer, the layoffs were a result of an AI-based big data analysis of Xsolla. According to the Observer’s report, the magnitude of the layoffs--which mostly affected employees in company’s Perm offices--was determined after an examination of workers’ productivity across various workspace software, including Gmail, Jira, and Confluence, in addition to others.

Although unfortunate--especially considering the ongoing COVID-19 pandemic--the layoffs themselves aren’t the ultimate controversy here. Rather, it’s the way Xsolla CEO Aleksandr Agapitov handled the situation. In a company email, which has been translated from Russian, Agapitov used language that could be flippant and insulting. In particular, he specifically labeled fired employees as “unengaged and unproductive,” stating that “I truly believe Xsolla isn not for you.”

Following the email last week, Agapitov took to Twitter and posted a statement that roughly equates to “get the fuck in, or get the fuck out,” according to various translations.

Further complicating the situation is the fact that many of the employees appear to have been unaware that their activities were being tracked by Xsolla’s data collection AI. It’s a personal breach that Renee Gittins, executive director of the International Game Developers Association (IGDA), calls frightening and unethical.

“Layoffs are not uncommon, particularly in a turbulent industry like games,” Gittins told GameDaily. “Choosing who to let go of and how to communicate downsizing can be a difficult task. Unfortunately, Agapitov's termination emails and surrounding commentary lacked empathy and respect for those whose lives have been affected. Terminating such a large amount of people based on activity in software that they were not aware was being tracked is not only concerning from a business perspective but also a cultural and ethical one.”

Gittins said that evaluating a worker’s productivity based on software interaction and without their knowledge is deeply inappropriate and morally wrong. The nature of creative work, she explained, is impossible for AI software to track, and professionals in such a field will often approach their jobs in unique and unconventional ways.

“Such an evaluation could harm those with nonstandard approaches and with different neurodiversities, even if their overall output matches their peers. Tracking such values and then terminating people based on them is sure to create distrust and anxiety within the culture of Xsolla's workforce.”

In this way, Gittins expects Agapitov and Xsolla’s actions to perpetuate distrust, resulting in even more of the undesired behavior that the company cited in the first place, a cycle that is likely to sow low morality among the workforce. Further, the fact that AI software was used to make such rash decisions could speak to a greater problem of disorganization and inefficiency within Xsolla as a whole.

“It is common business sense to only measure and promote KPIs that are of direct benefit to the business behaviors you desire,” Gittins said. “People are very likely to behave in a way to maximize those values even if those values do not result in business returns and profits. It would not surprise me if Xsolla sees tickets for tasks too small to warrant them, drafts in Google Documents that would be more efficiently created on paper or whiteboards, and other behaviors to keep up these tracked metrics that may overall hurt productivity.”

In the fallout of the layoffs, some are sure to question what legal recourse, if any, former Xsolla employees might have. The answer is largely dependent on a number of variables, including evidence gathered by those affected, Russian employment law, and other stipulations. These are all outside the purview of attorney Richard Hoeg, who practices in Michigan, USA, but there is some conjecture that can still be made in this situation.

“The biggest issue I can see here from afar could possibly be data collection and privacy,” Hoeg told GameDaily. “In general, employers have the right to ensure productivity (certainly on their own equipment), but it's possible that trying to maintain that control on private computers (with whatever scans/analysis is necessary) could trip some data rules. They also would likely have had to make sure the employees/contractors knew that’s what they were doing.”

Hoeg’s speculation is predicated on the fact that much of the games industry shifted to remote work at the onset of the pandemic. As such, many workers are utilizing their own computers and hardware to perform work duties, raising even more ethical questions regarding Xsolla’s use of data collection software.

It’s unquestionably a gray area, and Agapitov himself noted that much of the layoffs affected employees working remotely:

“In other words, you were not always present at the workplace when you worked remotely,” he wrote in the email.

In all, the layoffs paint a grim picture of how Agapitov values his employees. The pandemic has not been easy for anybody, but it appears that Xsolla expected work to continue as normal. Using AI-based tracking software raises a host of ethical concerns, and perhaps Agapitov needs to address such questions publicly.",Xsolla fires 150 employees following AI-based productivity audit,https://gamedaily.biz/article/2127/xsolla-fires-150-employees-following-ai-based-productivity-audit
"[""Chris Wallace""]",2021-10-02,2021-10-02,2021-08-06,2021-10-02,https://www.mcvuk.com/wp-content/uploads/xsolla_horizontal-1.png,2021-08-03,0,en,,www.mcvuk.com,"[""Kate Perkins""]","Xsolla fires 150 employees based on big data analysis of their activity – “Many of you might be shocked, but I truly believe that Xsolla is not for you.”

Payment services company Xsolla has reportedly fired 150 of its employees, with workers in the company’s office in Perm, Russia being terminated based on big data analysis of their activity (via Game World Observer).

Making the situation worse, Xsolla CEO and founder Aleksandr Agapitov sent an email to the affected employees explaining the decision, revealing that they had been let go because they had been tagged as “unengaged and unproductive employees.”

A translated version of the email reads as follows:

“You received this email because my big data team analyzed your activities in Jira, Confluence, Gmail, chats, documents, dashboards and tagged you as unengaged and unproductive employees. In other words, you were not always present at the workplace when you worked remotely.

“Many of you might be shocked, but I truly believe that Xsolla is not for you. Nadia and her care team partnered with seven leading HR agencies, as we will help you find a good place, where you will earn more and work even less. Sasha will help you get a recommendation, including the one from myself. And Natalia will read you your rights.

“Once again, thank you for your contribution. If you want to stay in contact with me, please write me a long letter about all your observations, injustice, and gratitude.”

This prompted immediate and predictable backlash: both for the layoffs themselves and for the tone of the email. According to ProPerm.ru, the company is investigating to find the employee who leaked the email.

Following the layoffs, Agapitov held a press conference in which he explained that the mass layoffs were caused by the fact that the company has stopped showing 40% growth. Agapitov provided further details, including that the total number of laid-off employees could total 40% of the company’s headcount across all of its offices.

Following the press conference, Agapitov incited further controversy with a Tweet that roughly translates to “Work your fucking ass off or get your fucking ass out.”

Speaking with Forbes Russia, Agapitov revealed that 60 of the affected employees might stay with the company following discussions with their managers, while those who have been let go will keep their medical insurance and receive medical pay equal to four to six monthly salaries.",Xsolla fires 150 employees based on big data analysis of their activity,https://www.mcvuk.com/business-news/xsolla-fires-150-employees-based-on-big-data-analysis-of-their-activity-many-of-you-might-be-shocked-but-i-truly-believe-that-xsolla-is-not-for-you/
"[""Joel Hruska""]",2021-10-02,2021-10-02,2021-09-22,2021-10-02,https://www.extremetech.com/wp-content/uploads/2019/09/Rivian_Amazon-cityscape-truck.jpg,2021-09-20,0,en,,www.extremetech.com,"[""Kate Perkins""]","Concerns about artificial intelligence and its impact on work are not new, but as more companies deploy these solutions we’re seeing decided snags in the process. One point many of these conversations take for granted is that AI-powered tools work. What happens if they don’t?

The pandemic has fueled an explosion in semiconductor sales and a significant rise in the number of employees who are kept under surveillance by their employers. In some cases, people aren’t just being watched — they’re being graded. This might not be a problem if the AI tools in question were robust enough to do the job, but all available evidence suggests they very much are not.

A new story at Motherboard details the results of Amazon’s latest push to introduce AI technology in the workplace. Last February, Amazon began installing cameras from the fleet camera company Netradyne, with the supposed goal of keeping drivers safe. Netradyne’s website pitches the company’s technology in exactly these terms, emphasizing that it can keep drivers focused on the road. The system tracks whether drivers maintain proper following distance, obey stop signs and street lights, and keep their attention on the road.

It’s hard to argue with the idea that people who drive for a living should be required to do these things. But according to the drivers actually delivering Amazon’s packages, the system is a nightmare. The problem isn’t that people are being forced to follow the law. The problem is that the Netradyne system isn’t very good at deciding when a driver is or isn’t breaking the law and Amazon offers no method for drivers to contest events.

“I have been ‘dinged’ for following too close when someone cuts me off,” one driver told Motherboard. “If I look into my mirrors to make sure I am safe to change lanes, it dings me for distraction because my face is turned to look into my mirror. I personally did not feel any more safe with a camera watching my every move.”

Another driver indicated the Netradyne AI system has a major problem with false stops. Apparently, the system has a bad habit of flagging yield signs as stop signs (and penalizing drivers for failing to stop), while simultaneously penalizing drivers if they stop at a stop sign and then pull forward slowly to look around a blind curve. Anyone who has driven for any length of time is aware that neighborhoods and businesses do not always maintain proper lines of sight. It can be dangerous to accelerate away from a stop sign without checking around a brush-obscured corner.

“Most false positives we get are stop sign violations,” he said. “Either we stop after the stop sign so we can see around a bush or a tree and it dings us for that, or it catches yield signs as stop signs. A few times, we’ve been in the country on a dirt road, where there’s no stop sign, but the camera flags a stop sign.”

A human driver who observes another human taking an intersection cautiously will reflexively scan the situation for context clues about why this is happening. Netradyne’s AI is incapable of this kind of evaluation. It only “sees” whether the vehicle is operating according to its own inflexible logic.

Amazon spokespeople insist that the Netradyne system has yielded positive results, with accidents down 48 percent, stop sign and signal violations down 77 percent, driving without a seatbelt reduced by 60 percent, following distance violations down 50 percent, and distracted driving decreased by 75 percent. These are impressive figures, to be sure. But they don’t actually tell us much and Amazon isn’t known for its honesty when dealing with the press.

I was the person who found the pee in the bottle. Trust me, it happened. https://t.co/U76UlDRWSO — James Bloodworth (@J_Bloodworth) March 25, 2021

For starters, we don’t know how this information was being gathered prior to the Netradyne system’s installation, so we don’t know how to compare the before-and-after figures. The 77 percent reduction in stop sign and signal violations may reflect the fact that Amazon’s delivery drivers are being more diligent, or it could indicate that drivers are avoiding false positives at stop signs by behaving in a less-safe manner that’s also less likely to cause a ding on their driving record.

Part of the problem is that these metrics are being used to determine how much Amazon’s delivery partners get paid. Too many Netradyne events can ruin a company’s score, reducing how much it earns from Amazon that month. There’s probably validity to the concept that this creates an incentive for a company to hire good drivers, but the ability of such metrics to achieve their goals is predicated on the idea that they’re measuring correctly in the first place.

Amazon is squeezing companies to make pro-safety changes while simultaneously pushing companies to adopt delivery schedules so punishing, some drivers carry plastic bottles in lieu of attempting to visit a restroom. Earlier this year, two Oregon companies effectively shut themselves down rather than continue hauling packages for Amazon. Investigative reports have repeatedly found that Amazon’s warehouse culture is a brutally difficult work environment, so it’s not surprising to see the company pushing the same model outwards in its business model.

Various delivery companies believe Amazon has instituted these practices so it can avoid paying them. Amazon insists it’s only trying to protect safety. According to Motherboard, various companies are telling drivers how to circumvent these tracking systems because turning them on means handing Amazon an excuse not to pay. Why aren’t drivers wearing seatbelts? Because Amazon insists on delivery schedules so demanding, drivers say they don’t have time to wear them. Regardless of what one believes about Amazon’s intentions, it’s obvious that its methods are having unintended consequences that work against the goal of improving driver safety records.

Perhaps the most maddening aspect of the entire situation is that there is no meaningful appeal process. While delivery companies can apparently submit feedback tickets for specific events, the Netradyne system logs hundreds of events per week and Amazon almost never overturns a previous decision. Companies mostly don’t try to contest these decisions because contesting them almost never works.

This kind of problem should ring more alarm bells than it probably will. For all the good AI can do in the right circumstances, tools like Netradyne are not ready for deployment if they generate false positives at this kind of rate. If Netradyne can’t offer a product that properly detects driver behavior in all circumstances, it has no business claiming otherwise.

It’s possible that Amazon really has seen the kind of safety improvements it claims, but increased safety is not the only variable that matters, here. A safety system whose improper detection systems cause drivers to act in an unsafe manner is definitionally less safe than one which does not. It’s all well and good to claim that the benefits represent a net improvement, but that does no good to the individuals who are unfairly penalized or even rendered unemployed because a random piece of software decided they were a problematic driver with zero human oversight or review.",AI Is Penalizing Amazon Delivery Drivers for Errors They Aren't Making,https://www.extremetech.com/extreme/327317-ai-is-penalizing-amazon-delivery-drivers-for-errors-they-arent-making?utm_source=email&utm_campaign=extremetech&utm_medium=title
"[""Julia Dressel"",""Hany Farid""]",2020-10-02,2021-10-02,2018-01-17,2021-10-02,https://www.science.org/pb-assets/images/logos/sciadv-logo-1620488349693.svg,2016-05-23,11,en,,www.science.org,"[""Kate Perkins""]","Abstract

Algorithms for predicting recidivism are commonly used to assess a criminal defendant’s likelihood of committing a crime. These predictions are used in pretrial, parole, and sentencing decisions. Proponents of these systems argue that big data and advanced machine learning make these analyses more accurate and less biased than humans. We show, however, that the widely used commercial risk assessment software COMPAS is no more accurate or fair than predictions made by people with little or no criminal justice expertise. In addition, despite COMPAS’s collection of 137 features, the same accuracy can be achieved with a simple linear classifier with only two features.

INTRODUCTION

We are the frequent subjects of predictive algorithms that determine music recommendations, product advertising, university admission, job placement, and bank loan qualification. In the criminal justice system, predictive algorithms have been used to predict where crimes will most likely occur, who is most likely to commit a violent crime, who is likely to fail to appear at their court hearing, and who is likely to reoffend at some point in the future (1).

One widely used criminal risk assessment tool, Correctional Offender Management Profiling for Alternative Sanctions (COMPAS; Northpointe, which rebranded itself to “equivant” in January 2017), has been used to assess more than 1 million offenders since it was developed in 1998. The recidivism prediction component of COMPAS—the recidivism risk scale—has been in use since 2000. This software predicts a defendant’s risk of committing a misdemeanor or felony within 2 years of assessment from 137 features about an individual and the individual’s past criminal record.

Although the data used by COMPAS do not include an individual’s race, other aspects of the data may be correlated to race that can lead to racial disparities in the predictions. In May 2016, writing for ProPublica, Angwin et al. (2) analyzed the efficacy of COMPAS on more than 7000 individuals arrested in Broward County, Florida between 2013 and 2014. This analysis indicated that the predictions were unreliable and racially biased. COMPAS’s overall accuracy for white defendants is 67.0%, only slightly higher than its accuracy of 63.8% for black defendants. The mistakes made by COMPAS, however, affected black and white defendants differently: Black defendants who did not recidivate were incorrectly predicted to reoffend at a rate of 44.9%, nearly twice as high as their white counterparts at 23.5%; and white defendants who did recidivate were incorrectly predicted to not reoffend at a rate of 47.7%, nearly twice as high as their black counterparts at 28.0%. In other words, COMPAS scores appeared to favor white defendants over black defendants by underpredicting recidivism for white and overpredicting recidivism for black defendants.

In response to this analysis, Northpointe argued that the ProPublica analysis overlooked other more standard measures of fairness that the COMPAS score satisfies (3) [see also the studies of Flores et al. (4) and Kleinberg et al. (5)]. Specifically, it is argued that the COMPAS score is not biased against blacks because the likelihood of recidivism among high-risk offenders is the same regardless of race (predictive parity), it can discriminate between recidivists and nonrecidivists equally well for white and black defendants as measured with the area under the curve of the receiver operating characteristic, AUC-ROC (accuracy equity), and the likelihood of recidivism for any given score is the same regardless of race (calibration). The disagreement amounts to different definitions of fairness. In an eloquent editorial, Corbett-Davies et al. (6) explain that it is impossible to simultaneously satisfy all of these definitions of fairness because black defendants have a higher overall recidivism rate (in the Broward County data set, black defendants recidivate at a rate of 51% as compared with 39% for white defendants, similar to the national averages).

While the debate over algorithmic fairness continues, we consider the more fundamental question of whether these algorithms are any better than untrained humans at predicting recidivism in a fair and accurate way. We describe the results of a study that shows that people from a popular online crowdsourcing marketplace—who, it can reasonably be assumed, have little to no expertise in criminal justice—are as accurate and fair as COMPAS at predicting recidivism. In addition, although Northpointe has not revealed the inner workings of their recidivism prediction algorithm, the accuracy of COMPAS on one data set can be explained with a simple classifier (7); we confirm their result here. In further agreement with Angelino et al. (7), we also show that although COMPAS may use up to 137 features to make a prediction, the same predictive accuracy can be achieved with only two features, and that more sophisticated classifiers do not improve prediction accuracy or fairness. Collectively, these results cast significant doubt on the entire effort of algorithmic recidivism prediction.

RESULTS

We compare the overall accuracy and bias in human assessment with the algorithmic assessment of COMPAS. Throughout, a positive prediction is one in which a defendant is predicted to recidivate, whereas a negative prediction is one in which they are predicted to not recidivate. We measure overall accuracy as the rate at which a defendant is correctly predicted to recidivate or not (that is, the combined true-positive and true-negative rates). We also report on false positives (a defendant is predicted to recidivate but they do not) and false negatives (a defendant is predicted to not recidivate but they do).

Human assessment

Participants saw a short description of a defendant that included the defendant’s sex, age, and previous criminal history, but not their race (see Materials and Methods). Participants predicted whether this person would recidivate within 2 years of their most recent crime. We used a total of 1000 defendant descriptions that were randomly divided into 20 subsets of 50 each. To make the task manageable, each participant was randomly assigned to see one of these 20 subsets. The mean and median accuracy for these predictions is 62.1 and 64.0%.

We compare these results with the performance of COMPAS on this subset of 1000 defendants. Because groups of 20 participants judged the same subset of 50 defendants, the individual judgments are not independent. However, because each participant judged only one subset of the defendants, the median accuracies of each subset can reasonably be assumed to be independent. Therefore, the participant performance on the 20 subsets can be directly compared to the COMPAS performance on the same 20 subsets. A one-sided t test reveals that the average of the 20 median participant accuracies of 62.8% [and a standard deviation (SD) of 4.8%] is, just barely, lower than the COMPAS accuracy of 65.2% (P = 0.045).

To determine whether there is “wisdom in the crowd” (7) (in our case, a small crowd of 20 per subset), participant responses were pooled within each subset using a majority rules criterion. This crowd-based approach yields a prediction accuracy of 67.0%. A one-sided t test reveals that COMPAS is not significantly better than the crowd (P = 0.85).

Prediction accuracy can also be assessed using the AUC-ROC. The AUC-ROC for our participants is 0.71 ± 0.03, nearly identical to COMPAS’s 0.70 ± 0.04.

Prediction accuracy can also be assessed using tools from signal detection theory in which accuracy is expressed in terms of sensitivity (d′) and bias (β). Higher values of d′ correspond to greater participant sensitivity. A value of d′ = 0 means that the participant has no information to make reliable identifications no matter what bias he or she might have. A value of β = 1.0 indicates no bias, a value of β > 1 indicates that participants are biased to classifying a defendant as not being at risk of recidivating, and β < 1 indicates that participants are biased to classifying a defendant as being at risk of recidivating. With a d′ of 0.86 and a β of 1.02, our participants are slightly more sensitive and slightly less biased than COMPAS with a d′ of 0.77 and a β of 1.08.

With considerably less information than COMPAS (only 7 features compared to COMPAS’s 137), a small crowd of nonexperts is as accurate as COMPAS at predicting recidivism. In addition, our participants’ and COMPAS’s predictions were in agreement for 692 of the 1000 defendants.

Fairness

We measure the fairness of our participants with respect to a defendant’s race based on the crowd predictions. Our participants’ accuracy on black defendants is 68.2% compared with 67.6% for white defendants. An unpaired t test reveals no significant difference across race (P = 0.87). This is similar to that of COMPAS that has an accuracy of 64.9% for black defendants and 65.7% for white defendants, which is also not significantly different (P = 0.80, unpaired t test). By this measure of fairness, our participants and COMPAS are fair to black and white defendants.

Our participants’ false-positive rate for black defendants is 37.1% compared with 27.2% for white defendants. An unpaired t test reveals a significant difference across race (P = 0.027). Our participants’ false-negative rate for black defendants is 29.2% compared with 40.3% for white defendants. An unpaired t test reveals a significant difference across race (P = 0.034). These discrepancies are similar to that of COMPAS that has a false-positive rate of 40.4% for black defendants and 25.4% for white defendants, which are significantly different (P = 0.002, unpaired t test). COMPAS’s false-negative rate for black defendants is 30.9% compared with 47.9% for white defendants, which are significantly different (P = 0.003, unpaired t test). By this measure of fairness, our participants and COMPAS are similarly unfair to black defendants, despite the fact that race is not explicitly specified. See Table 1 [columns (A) and (C)] and Fig. 1 for a summary of these results.

Prediction with race

In this second condition, a newly recruited set of 400 participants repeated the same study but with the defendant’s race included. We wondered whether including a defendant’s race would reduce or exaggerate the effect of any implicit, explicit, or institutional racial bias. In this condition, the mean and median accuracy on predicting whether a defendant would recidivate is 62.3 and 64.0%, nearly identical to the condition where race is not specified.

The crowd-based accuracy is 66.5%, slightly lower than the condition where race is not specified, but not significantly different (P = 0.66, paired t test). The crowd-based AUC-ROC is 0.71 ± 0.03 and the d′/β is 0.83/1.03, similar to the previous no-race condition [Table 1, columns (A) and (B)].

With respect to fairness, participant accuracy is not significantly different for black defendants (66.2%) compared with white defendants (67.6%; P = 0.65, unpaired t test). The false-positive rate for black defendants is 40.0% compared with 26.2% for white defendants. An unpaired t test reveals a significant difference across race (P = 0.001). The false-negative rate for black defendants is 30.1% compared with 42.1% for white defendants that, again, is significantly different (P = 0.030, unpaired t test). See Table 1 [column (B)] for a summary of these results.

In conclusion, there is no sufficient evidence to suggest that including race has a significant impact on overall accuracy or fairness. The exclusion of race does not necessarily lead to the elimination of racial disparities in human recidivism prediction.
Participant demographics

Our participants ranged in age from 18 to 74 (with one participant over the age of 75) and in education level from “less than high school degree” to “professional degree.” Neither age, gender, nor level of education had a significant effect on participant accuracy. There were not enough nonwhite participants to reliably measure any differences across participant race.

Algorithmic assessment

Because nonexperts are as accurate as the COMPAS software, we wondered about the sophistication of the COMPAS predictive algorithm. Northpointe’s COMPAS software incorporates 137 distinct features to predict recidivism. With an overall accuracy of around 65%, these predictions are not as accurate as we might want, particularly from the point of view of a defendant whose future lies in the balance.

Northpointe does not reveal the details of the inner workings of COMPAS—understandably so, given their commercial interests. We have, however, found that a simple linear predictor—logistic regression (LR) (see Materials and Methods)—provided with the same seven features as our participants (in the no-race condition), yields similar prediction accuracy as COMPAS. As compared to COMPAS’s overall accuracy of 65.4%, the LR classifier yields an overall testing accuracy of 66.6%. This predictor yields similar results to COMPAS in terms of predictive fairness [Table 2, (A) and (D) columns].

Despite using only 7 features as input, a standard linear predictor yields similar results to COMPAS’s predictor with 137 features. We can reasonably conclude that COMPAS is using nothing more sophisticated than a linear predictor or its equivalent.

To test whether performance was limited by the classifier or by the nature of the data, we trained a more powerful nonlinear support vector machine (NL-SVM) on the same data. Somewhat surprisingly, the NL-SVM yields nearly identical results to the linear classifier [Table 2, column (C)]. If the relatively low accuracy of the linear classifier was because the data are not linearly separable, then we would have expected the NL-SVM to perform better. The failure to do so suggests that the data are simply not separable, linearly, or otherwise.

Lastly, we wondered whether using an even smaller subset of the 7 features would be as accurate as using COMPAS’s 137 features. We trained and tested an LR classifier on all possible subsets of the seven features. A classifier based on only two features—age and total number of previous convictions—performs as well as COMPAS; see Table 2 [column (B)]. The importance of these two criteria is consistent with the conclusions of two meta-analysis studies that set out to determine, in part, which criteria are most predictive of recidivism (8, 9).

DISCUSSION

We have shown that commercial software that is widely used to predict recidivism is no more accurate or fair than the predictions of people with little to no criminal justice expertise who responded to an online survey. Given that our participants, our classifiers, and COMPAS all seemed to reach a performance ceiling of around 65% accuracy, it is important to consider whether any improvement is possible. We should note that our participants were each presented with the same data for each defendant and were not instructed on how to use these data in making a prediction. It remains to be seen whether their prediction accuracy would improve with the addition of guidelines that specify how much weight individual features should be given. For example, a large-scale meta-analysis of approaches to predicting recidivism of sexual offenders (10) found that actuarial measures, in which explicit data and explicit combination rules are used to combine the data into a single score, provide more accurate predictions than unstructured measures in which neither explicit data nor explicit combination rules are specified. It also remains to be seen whether the addition of dynamic risk factors (for example, pro-offending attitudes and socio-affective problems) would improve prediction accuracy as previously suggested (11, 12) (we note, however, that COMPAS does use some dynamic risk factors that do not appear to improve overall accuracy). Lastly, because pooling responses from multiple participants yields higher accuracy than individual responses, it remains to be seen whether a larger pool of participants will yield even higher accuracy, or whether participants with criminal justice expertise would outperform those without.

Although Northpointe does not reveal the details of their COMPAS software, we have shown that their prediction algorithm is equivalent to a simple linear classifier. In addition, despite the impressive collection of 137 features, it would appear that a linear classifier based on only 2 features—age and total number of previous convictions—is all that is required to yield the same prediction accuracy as COMPAS. This finding is supported by earlier work that showed how to create simple, transparent, and interpretable predictive rules for recidivism prediction (7, 14, 15). When applied to the same Broward County data set, the authors found that simple models afforded similar predictive accuracy as COMPAS.

The question of accurate prediction of recidivism is not limited to COMPAS. A review of nine different algorithmic approaches to predicting recidivism found that eight of the nine approaches failed to make accurate predictions (including COMPAS) (13). In addition, a meta-analysis of nine algorithmic approaches found only moderate levels of predictive accuracy across all approaches and concluded that these techniques should not be solely used for criminal justice decision-making, particularly in decisions of preventative detention (14).

Recidivism in this study, and for the purpose of evaluating COMPAS, is operationalized with rearrest that, of course, is not a direct measure of reoffending. As a result, differences in the arrest rate of black and white defendants complicate the direct comparison of false-positive and false-negative rates across race (black people, for example, are almost four times as likely as white people to be arrested for drug offenses).

When considering using software such as COMPAS in making decisions that will significantly affect the lives and well-being of criminal defendants, it is valuable to ask whether we would put these decisions in the hands of random people who respond to an online survey because, in the end, the results from these two approaches appear to be indistinguishable.

MATERIALS AND METHODS

Our analysis was based on a database of 2013–2014 pretrial defendants from Broward County, Florida (2). This database of 7214 defendants contains individual demographic information, criminal history, the COMPAS recidivism risk score, and each defendant’s arrest record within a 2-year period following the COMPAS scoring. COMPAS scores, ranging from 1 to 10, classify the risk of recidivism as low-risk (1 to 4), medium-risk (5 to 7), or high-risk (8 to 10).

Our algorithmic assessment was based on this full set of 7214 defendants. Our human assessment was based on a random subset of 1000 defendants, which was held fixed throughout all conditions. This subset yielded similar overall COMPAS accuracy, false-positive rate, and false-negative rate as the complete database (a positive prediction is one in which a defendant is predicted to recidivate; a negative prediction is one in which they are predicted to not recidivate). The COMPAS accuracy for this subset of 1000 defendants was 65.2%. The average COMPAS accuracy on 10,000 random subsets of size 1000 each was 65.4% with a 95% confidence interval of (62.6, 68.1).

Human assessment

A descriptive paragraph for each of 1000 defendants was generated:

The defendant is a [SEX] aged [AGE]. They have been charged with: [CRIME CHARGE]. This crime is classified as a [CRIMINAL DEGREE]. They have been convicted of [NON-JUVENILE PRIOR COUNT] prior crimes. They have [JUVENILE- FELONY COUNT] juvenile felony charges and [JUVENILE-MISDEMEANOR COUNT] juvenile misdemeanor charges on their record.

In a follow-up condition, the defendant’s race was included so that the first line of the above paragraph read, “The defendant is a [RACE] [SEX] aged [AGE].”

There were a total of 63 unique criminal charges including armed robbery, burglary, grand theft, prostitution, robbery, and sexual assault. The crime degree is either “misdemeanor” or “felony.” To ensure that our participants understood the nature of each crime, the above paragraph was followed by a short description of each criminal charge:

[CRIME CHARGE]: [CRIME DESCRIPTION]

After reading the defendant description, participants were then asked to respond either “yes” or “no” to the question “Do you think this person will commit another crime within 2 years?” The participants were required to answer each question and could not change their response once it was made. After each answer, the participants were given two forms of feedback: whether their response was correct and their average accuracy.

The 1000 defendants were randomly divided into 20 subsets of 50 each. Each participant was randomly assigned to see one of these 20 subsets. The participants saw the 50 defendants, one at a time, in random order. The participants were only allowed to complete a single subset of 50 defendants.

The participants were recruited through Amazon’s Mechanical Turk, an online crowdsourcing marketplace where people are paid to perform a wide variety of tasks (Institutional Review Board guidelines were followed for all participants). Our task was titled “Predicting Crime” with the description “Read a few sentences about an actual person and predict if they will commit a crime in the future.” The keywords for the task were “survey, research, and criminal justice.” The participants were paid $1.00 for completing the task and a $5.00 bonus if their overall accuracy on the task was greater than 65%. This bonus was intended to provide an incentive for participants to pay close attention to the task. To filter out participants who were not paying close attention, three catch trials were randomly added to the subset of 50 questions. These questions were formatted to look like all other questions but had easily identifiable correct answers. A participant’s response was eliminated from our analysis if any of these questions were answered incorrectly. The catch trial questions were (i) The state of California was the 31st state to join the Union. California’s nickname is: The Golden State. The state capital is Sacramento. California is bordered by three other states. Los Angeles is California’s most populous city, which is the country’s second largest city after New York City. Does the state of California have a nickname?; (ii) The first spaceflight that landed humans on the Moon was Apollo 11. These humans were: Neil Armstrong and Buzz Aldrin. Armstrong was the first person to step onto the lunar surface. This landing occurred in 1969. They collected 47.5 pounds (21.59 kg) of lunar material to bring back to Earth. Did the first spaceflight that landed humans on the Moon carry Buzz Aldrin?; and (iii) The Earth is the third planet from the Sun. The shape of Earth is approximately oblate spheroidal. It is the densest planet in the Solar System and the largest of the four terrestrial planets. During one orbit around the Sun, Earth rotates about its axis over 365 times. Earth is home to over 7.4 billion humans. Is Earth the fifth planet from the Sun?

Responses for the first (no-race) condition were collected from 462 participants, 62 of which were removed because of an incorrect response on a catch trial. Responses for the second (race) condition were collected from 449 participants, 49 of which were removed because of an incorrect response on a catch trial. In each condition, this yielded 20 participant responses for each of 20 subsets of 50 questions. Because of the random pairing of participants to a subset of 50 questions, we occasionally oversampled the required number of 20 participants. In these cases, we selected a random 20 participants and discarded any excess responses. Throughout, we used both paired and unpaired t tests (with 19 degrees of freedom) to analyze the performance of our participants and COMPAS.

Algorithmic assessment

Our algorithmic analysis used the same seven features as described in the previous section extracted from the records in the Broward County database. Unlike the human assessment that analyzed a subset of these defendants, the following algorithmic assessment was performed over the entire database.

We used two different classifiers: logistic regression (15) (a linear classifier) and a nonlinear SVM (16). The input to each classifier was seven features from 7214 defendants: age, sex, number of juvenile misdemeanors, number of juvenile felonies, number of prior (nonjuvenile) crimes, crime degree, and crime charge (see previous section). Each classifier was trained to predict recidivism from these seven features. Each classifier was trained 1000 times on a random 80% training and 20% testing split; we report the average testing accuracy and bootstrapped 95% confidence intervals for these classifiers.
Logistic regression is a linear classifier that, in a two-class classification (as in our case), computes a separating hyperplane to distinguish between recidivists and nonrecidivists. A nonlinear SVM uses a kernel function—in our case, a radial basis kernel—to project the initial seven-dimensional feature space to a higher dimensional space in which a linear hyperplane is used to distinguish between recidivists and nonrecidivists. The use of a kernel function amounts to computing a nonlinear separating surface in the original seven-dimensional feature space, allowing the classifier to capture more complex patterns between recidivists and nonrecidivists than is possible with linear classifiers.

Acknowledgments

We wish to thank M. Banks, M. Bravo, E. Cooper, L. Lax, and G. Wolford for helpful discussions. Funding: The authors acknowledge that they received no funding in support of this research. Author contributions: The authors contributed equally to all aspects of this work and manuscript preparation. Competing interests: The authors declare that they have no competing interests. Data and materials availability: All data associated with this research may be found at www.cs.dartmouth.edu/farid/downloads/publications/scienceadvances17. Additional data related to this paper may be requested from the authors.
","The accuracy, fairness, and limits of predicting recidivism",https://www.science.org/doi/10.1126/sciadv.aao5580
"[""Gabriela Galindo""]",2021-10-04,2021-10-04,2020-04-14,2021-10-04,https://www.brusselstimes.com/wp-content/uploads/2020/04/Screenshot-2020-04-14-at-14.59.23-1024x640.png,2020-04-14,0,en,,www.brusselstimes.com,"[""Kate Perkins""]","A deepfake video showing Belgium’s prime minister speaking of an urgent need to tackle the economic and climate crises has been put into circulation by Extinction Rebellion Belgium.

Published on Tuesday, the video is a modification of a previous address to the nation given by Premier Shophie Wilmès about the coronavirus pandemic.

“Coronavirus is an alarm bell we cannot ignore,” Wilmès is made to say in the video. “Pandemics are one of the consequences of a deeper ecological crisis.”

The fictional speech goes on to say that the most recent global epidemics, like SARS, Ebola, the Swine Flu and now Covid-19 are directly linked to the “exploitation and destruction by humans of our natural environment.”

The video, in which the use machine learning and artificial technology to reproduce Wilmès’ voice and likeness, is based of the full speech posted online which draws from dozens of scientific publications about the impacts that unbridled global warming could have on life on Earth.

“We have failed as policymakers to grasp the seriousness of the ecological collapse. But today, the coronavirus crisis is making us aware of the depth of change required of us: we must change our way of life, and we must change it now,” the speech reads.

The deepfake video comes amid efforts from the climate action group to keep the pressure high on leaders even amid the nationwide lockdown imposed by the current pandemic.

As part of an initiative dubbed “Rebellion on Lockdown” the group is planning to continue campaigning online until they are allowed to return to the streets again.

According to the Belga news agency, the video was sent to the cabinet of the prime minister as well as to the offices of the minister-presidents of Brussels, Wallonia and Flanders.",XR Belgium posts deepfake of Belgian premier linking Covid-19 with climate crisis,https://www.brusselstimes.com/news/belgium-all-news/politics/106320/xr-belgium-posts-deepfake-of-belgian-premier-linking-covid-19-with-climate-crisis/
"[""Alexander Puutio"",""David Alexandru Timis""]",2021-10-04,2021-10-04,2020-10-05,2021-10-04,https://assets.weforum.org/editor/responsive_large_webp_Wj3KSQ-K7-wlCMqru0obsr-QZV9kUrcCQi_dlfDswfI.webp,2020-04-13,0,en,,www.weforum.org,"[""Kate Perkins""]","The emerging threat of deepfakes could have an unprecedented impact on this election cycle, raising serious questions about the integrity of democratic elections, policy-making and our society at large.

A new ethical agenda for AI in political advertising and content on online platforms is required. Given the cross-border nature of the problem, the agenda must be backed by global consensus and action.

Communities and individuals can also take action directly by setting higher standards for how to create and interact with political content online.

In a few months the United States will elect its 46th President. While some worry about whether campaigning and casting votes can be done safely during the COVID-19 pandemic, another question is just as critical: how many votes will result via the manipulative influence of artificial intelligence?

Specifically, the emerging threat of deepfakes could have an unprecedented impact on this election cycle, raising serious questions about the integrity of elections, policy-making and our democratic society at large.

Understanding deepfakes

AI-powered deepfakes have the potential to bring troubling consequences for the US 2020 elections.

The technology that began as little more than a giggle-inducing gimmick for making homebrew mash-up videos has recently been supercharged by advances in AI.

Today, open sourced software like DeepFaceLab and Faceswap allow virtually anyone with time and access to cloud computing to deploy sophisticated machine learning processes and graphical rendering without any prior development.

More worryingly, the technology itself is improving at such a rapid pace where experts predict that deepfakes may soon be indistinguishable from real videos. The staggering results that AI can create today can be attributed to herculean leaps in a subfield called Generative Adversarial Networks. This technology enables neural networks to make the jump from mere perception to creation.

As one can expect with viral technology, the number of deepfake videos is growing exponentially as the continuing democratization of AI and cloud-computing make the underlying processes more and more accessible.

A new infodemic?
As we have seen during the COVID-19 pandemic, the contagious spread of misinformation rarely requires more than a semblance of authority accompanying the message, no matter how pernicious or objectively unsafe the content may be to the audience.

Given how easily deepfakes can combine fake narratives and information with fabricated sources of authority, they have an unprecedented potential to mislead, misinform and manipulate, giving ‘you won’t believe your eyes’ a wholly new meaning.

In fact, according to a recent report published by The Brookings Institute, deepfakes are well on their way to not only distort the democratic discourse but also to erode trust in public institutions at large.

How can deepfakes become electoral weapons?

How exactly could deepfakes be weaponized in an election? To begin with, malicious actors could forge evidence to fuel false accusation and fake narratives. For example, by introducing subtle changes to how a candidate delivers an otherwise authentic speech could be used to put character, fitness and mental health into question without most viewers knowing any better.

Deepfakes could also be used to create entirely new fictitious content, including controversial or hateful statements with the intention of playing upon political divisions, or even inciting violence.

Perhaps not surprisingly, deepfakes have already been leveraged in other countries to destabilize governments and political processes.

In Gabon, the military launched an ultimately unsuccessful coup after the release of an apparently fake video of leader Ali Bongo suggested that the President was no longer healthy enough to hold office.

In Malaysia, a video purporting to show the Economic Affairs Minister having sex has generated a considerable debate over whether the video was faked or not, which caused reputational damage for the Minister.

In Belgium, a political group released a deepfake of the Belgian Prime Minister giving a speech that linked the COVID-19 outbreak to environmental damage and called for drastic action on climate change.

The truth may win

As of today, we are woefully ill-equipped to deal with deepfakes.

According to the Pew Research Center, almost two-thirds of the US population say that fake content creates a great deal of confusion about the political reality. What is worse, even our best efforts to correct and fact check fake content could ultimately serve to only strengthen the spread of faked narrative instead.

For AI and democracy to coexist, we must urgently secure a common understanding of what is true and create a shared environment for facts from which our diverging opinions can safely emerge.

What is most desperately needed is a new ethical agenda for AI in political advertising and content on online platforms. Given the cross-border nature of the problem, the agenda must be backed by global consensus and action.

Initiatives like the World Economic Forum’s Responsible Use of Technology, which bring tech executives together to discuss the ethical use of their platforms, are a strong start.

On the more local level, legislatures have started to follow California’s initiative to ban deepfakes during elections and even Facebook has joined the fight with its own ban on certain forms of manipulated content and a challenge to create technologies to spot them.

The future: fact or fiction?

Still, more can be done.

We do not necessarily need a technology or regulatory paradigm change in order to disarm deepfakes. Instead, communities and individuals can also take action directly by setting higher standards for how we create and interact with political content online ourselves.

In fact, unless voters themselves stand up for facts and truth in online discourse, it will be all but impossible to drive meaningful change, simply because of the inherent subjectivity of online platforms that puts reality at a disadvantage.

Whether we want it or not deepfakes are here to stay. But November 2020 could mark the moment we take a collective stand against the threats AI poses before it’s too late.
",Deepfake democracy: Here's how modern elections could be decided by fake news,https://www.weforum.org/agenda/2020/10/deepfake-democracy-could-modern-elections-fall-prey-to-fiction/
"[""Jim Waterson""]",2021-10-04,2021-10-04,2020-05-30,2021-10-04,https://i.guim.co.uk/img/media/4a940b4e9ff074072b8b52695f3baac6031c2b1d/0_290_5254_3154/master/5254.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctYWdlLTIwMjAucG5n&enable=upscale&s=237b7550c307c3d2febbab5d5a9111ec,2020-05-28,0,en,,www.theguardian.com,"[""Kate Perkins""]","Dozens of journalists have been sacked after Microsoft decided to replace them with artificial intelligence software.

Staff who maintain the news homepages on Microsoft’s MSN website and its Edge browser – used by millions of Britons every day – have been told that they will be no longer be required because robots can now do their jobs.

Around 27 individuals employed by PA Media – formerly the Press Association – were told on Thursday that they would lose their jobs in a month’s time after Microsoft decided to stop employing humans to select, edit and curate news articles on its homepages.

Employees were told Microsoft’s decision to end the contract with PA Media was taken at short notice as part of a global shift away from humans in favour of automated updates for news.

One staff member who worked on the team said: “I spend all my time reading about how automation and AI is going to take all our jobs, and here I am – AI has taken my job.”

The individual added that the decision to replace humans with software was risky, as the existing staff were careful to stick to “very strict editorial guidelines” which ensured that users were not presented with violent or inappropriate content when opening their browser, of particular importance for younger users.

The team working on the Microsoft site did not report original stories but still exercised editorial control, selecting stories produced by other news organisations – including the Guardian – and editing content and headlines where appropriate to fit the format. The articles were then hosted on Microsoft’s website, with the tech company sharing advertising revenue with the original publishers.

Manual curation of news stories also ensured that headlines were clear and appropriate for the format, while encouraging a spread of political opinions and avoiding untrustworthy stories, while highlighting interesting articles from smaller outlets.

Some of the journalists now facing redundancy had longstanding experience in the industry, while for others it offered a foot in the door and a job in an industry which has seen wave after wave of cuts. They now face a tough challenge to get jobs elsewhere when the whole industry is looking to cut costs. Other teams around the world are expected to be affected by Microsoft’s decision to automate the curation of its news sites.

In common with other news organisations, PA Media is facing tough financial challenges and has had to furlough some staff and ask others to take pay cuts. The company has expanded outside its traditional news agency business, recently buying stock image business Alamy shortly before the pandemic devastated the media industry.

A spokesperson for the company said: “We are in the process of winding down the Microsoft team working at PA, and we are doing everything we can to support the individuals concerned. We are proud of the work we have done with Microsoft and know we delivered a high-quality service.”

A Microsoft spokesperson said: “Like all companies, we evaluate our business on a regular basis. This can result in increased investment in some places and, from time to time, re-deployment in others. These decisions are not the result of the current pandemic.”

Many tech companies are experimenting with uses for Artificial Intelligence in journalism, with the likes of Google funding investment in projects to understand its uses, although efforts to automate the writing of articles have not been adopted widely.",Microsoft sacks journalists to replace them with robots,https://www.theguardian.com/technology/2020/may/30/microsoft-sacks-journalists-to-replace-them-with-robots
"[""Tom Warren""]",2021-10-04,2021-10-04,2020-05-30,2021-10-04,https://cdn.vox-cdn.com/thumbor/vCMsNERYJI81L9t2rdMsnez4Euc=/0x146:2040x1214/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/10793349/acastro_180507_1777_microsoft_0002.jpg,2020-05-28,0,en,,www.theverge.com,"[""Kate Perkins""]","Microsoft is laying off dozens of journalists and editorial workers at its Microsoft News and MSN organizations. The layoffs are part of a bigger push by Microsoft to rely on artificial intelligence to pick news and content that’s presented on MSN.com, inside Microsoft’s Edge browser, and in the company’s various Microsoft News apps. Many of the affected workers are part of Microsoft’s SANE (search, ads, News, Edge) division, and are contracted as human editors to help pick stories.

“Like all companies, we evaluate our business on a regular basis,” says a Microsoft spokesperson in a statement. “This can result in increased investment in some places and, from time to time, re-deployment in others. These decisions are not the result of the current pandemic.”

While Microsoft says the layoffs aren’t directly related to the ongoing coronavirus pandemic, media businesses across the world have been hit hard by advertising revenues plummeting across TV, newspapers, online, and more.

Business Insider first reported the layoffs on Friday, and says that around 50 jobs are affected in the US. The Microsoft News job losses are also affecting international teams, and The Guardian reports that around 27 are being let go in the UK after Microsoft decided to stop employing humans to curate articles on its homepages.

Microsoft has been in the news business for more than 25 years, after launching MSN all the way back in 1995. At the launch of Microsoft News nearly two years ago, Microsoft revealed it had “more than 800 editors working from 50 locations around the world.”

Microsoft has gradually been moving towards AI for its Microsoft News work in recent months, and has been encouraging publishers and journalists to make use of AI, too. Microsoft has been using AI to scan for content and then process and filter it and even suggest photos for human editors to pair it with. Microsoft had been using human editors to curate top stories from a variety of sources to display on Microsoft News, MSN, and Microsoft Edge.",Microsoft lays off journalists to replace them with AI,https://www.theverge.com/2020/5/30/21275524/microsoft-news-msn-layoffs-artificial-intelligence-ai-replacements
"[""Dan Robitzski""]",2021-10-04,2021-10-04,2020-06-01,2021-10-04,https://wp-assets.futurism.com/2020/06/msn-fires-journalists-replaces-ai.jpg,2020-05-28,0,en,,futurism.com,"[""Kate Perkins""]","New Algorithm

Over the weekend, Microsoft announced that it’s laying off dozens of journalists, editors, and other workers at MSN and its other news divisions.

While media layoffs are tragically widespread at the moment, Microsoft said that the layoffs had nothing to do with the COVID-19 pandemic, The Verge reports. Instead, it’s part of the company’s push over the last few months to automate journalism: it plans to replace the laid-off workers with news-scanning artificial intelligence.

Algorithmic Curation

Many of the roughly 77 editors and journalists hit by the layoffs helped curate the news stories that appear on the homepage for Microsoft News, MSN, and Microsoft’s Edge browser, according to The Verge. Now, AI algorithms will scan the internet for news articles to highlight, taking the work of deciding which news is important out of human hands.

In recent months, Microsoft has increasingly urged reporters and editors to rely on AI for tasks like finding and distilling online content and images to use in articles, The Verge reports.

Layoff Season

While plummeting ad revenue and other financial downturns caused by the coronavirus pandemic have hit newsrooms hard, Microsoft says that’s not what motivated its layoffs.

“Like all companies, we evaluate our business on a regular basis,” a company spokesperson said, according to The Verge. “This can result in increased investment in some places and, from time to time, re-deployment in others. These decisions are not the result of the current pandemic.”","MSN Fires Journalists, Replaces Them With AI",https://futurism.com/the-byte/msn-fires-journalists-replaces-ai
"[""Mayank Kumar""]",2021-10-04,2021-10-04,2020-06-01,2021-10-04,https://images.financialexpress.com/2020/05/microsoft.jpg,2020-05-28,0,en,,www.financialexpress.com,"[""Kate Perkins""]","The team working on the Microsoft website did not publish original stories but instead exercised editorial control, choosing articles created by other news outlets and, where necessary, editing material and headlines to suit the format. The articles were then published on Microsoft’s website, with the original publishers sharing advertisement revenue with the software firm.

Microsoft is laying off hundreds of journalists and editorial staff at Microsoft News and MSN. The layoffs are part of Microsoft’s bigger drive to rely on artificial intelligence to select news and content from MSN.com, inside Microsoft’s Edge platform, and the numerous Microsoft News apps in the business. The employees working in the Microsoft’s SANE (search, advertising, News, Edge) division who are generally hired as editors to help select stories will deal with the fallout of the move by the company.

According to media reports, 50 employees in the United States and 21 in the United Kingdom will be immediately affected as they will now have to search for new opportunities.

“Like all companies, we evaluate our business on a regular basis. This can result in increased investment in some places and, from time to time, re-deployment in others. These decisions are not the result of the current pandemic,” a Microsoft spokesperson said in a statement.

Although Microsoft claims the layoffs are not directly linked to the ongoing coronavirus pandemic, advertising sales plummeting through TV, magazines, internet, and more have hit media companies all over the world hard. Microsoft also used human editors to curate top stories from a variety of outlets on Microsoft News, MSN, and Microsoft Edge.

Manual curation of news stories also ensured that headlines were consistent and fitting for the medium while promoting circulation of political views and avoiding untrustworthy reports while highlighting interesting articles from smaller outlets.

Many of the journalists now facing redundancy had long-standing business experience, while it offered a foot in the door and a position in a business that saw wave after wave of cuts for many. Now they are facing a difficult task of finding jobs elsewhere as the industry as a whole is trying to cut costs. Microsoft’s decision to automate the curation of its news pages is likely to impact other teams around the world.","Robot uprising begins? Microsoft fires journalists, replaces them with AI",https://www.financialexpress.com/industry/technology/robot-uprising-begins-microsoft-fires-journalists-replaces-them-with-ai/1977441/
"[""Kahekashan""]",2021-10-04,2021-10-04,2020-05-30,2021-10-04,https://assets.thehansindia.com/h-upload/2020/05/30/973045-microsoft.jpg,2020-05-28,0,en,,www.thehansindia.com,"[""Kate Perkins""]","Microsoft fired many journalists after it decided to replace them with artificial intelligence robots.

Staff who maintain the Microsoft's MSN website news homepages on and its Edge browser have been told that they will be no longer needed as robots can do their jobs.

Around 27 people employed by PA Media – earlier the Press Association – were told on Thursday that in a month they would lose their jobs after Microsoft decided to stop hiring humans to select, edit and curate news articles on its homepages.

One team member said: ""I spend all my time reading about how automation and AI are going to take all our jobs, and here I am – AI has taken my job.""

He further added that the decision to replace humans with software was risky, as the existing staff were keen to stick to ""very strict editorial guidelines"" which ensured users not to be presented with violent or unsuitable content when opening their browser, of particular importance for younger users.

The Microsoft team working on-site did not report original stories, but still implemented editorial control, selecting stories produced by other news organizations, including The Guardian, and editing content and headlines when appropriate for the format. The articles were then presented on the Microsoft website, and the company shared the advertising revenue with the original publishers.

Manual presentation of the news also ensured that the headlines were clear and format-appropriate while encouraging the dissemination of political views and avoiding unreliable stories, while also highlighting interesting articles from small-media house.

Some of the journalists' are now facing layoff had long experience in the industry, while for others they offered a foot in the door and a job in a sector that has seen wave after wave of cuts. Now they face a challenge of getting a job somewhere else when the whole industry is planning to cut costs. Other teams around across the globe are expected to be affected by Microsoft's decision to automate the curation of its news sites.

Like other news organizations, PA Media faces difficult financial challenges and had to suspend some staff members and ask others to accept pay cuts. The tech company had expanded out of its traditional newsagency business, recently buying the Alamy stock image business just before the pandemic devastated the media industry.

A company spokesperson said: ""We are in the process of winding down the Microsoft team working at PA, and we are doing everything we can to support the individuals concerned. We are proud of the work we have done with Microsoft and know we delivered a high-quality service.""

""Like all companies, we evaluate our business on a regular basis. This can result in increased investment in some places and, from time to time, re-deployment in others. These decisions are not the result of the current pandemic,"" said a Microsoft spokesperson.

Many tech companies are experimenting with Artificial Intelligence uses in journalism. As Google funding investment in projects to understand its uses, although automate the writing of articles have not been adopted extensively.",Microsoft fires journalists to replace them with AI robots,https://www.thehansindia.com/technology/tech-news/microsoft-fires-journalists-to-replace-them-with-ai-robots-625324
"[""Avinash A""]",2021-10-04,2021-10-04,2020-06-03,2021-10-04,https://techlog360.com/wp-content/uploads/2017/12/Microsoft-Filed-Lawsuit-Against-IP-Address-For-Illegally-Activating-Copies-of-Windows-And-Office.jpg,2020-05-28,0,en,,techlog360.com,"[""Kate Perkins""]","Now onwards you are going to see news stories generated by AI on homepages of the MSN website and Edge browser.

Microsoft has been fired dozens of journalists and editorial workers at its Microsoft News and MSN organizations — leveraging robot journalism to cut costs.

According to Microsoft, this move is a part of their business evaluation process. This can increase investment in some places and redeployment in others. They also clarified that the layoffs are not driven by the ongoing coronavirus pandemic.

Robot uprising in Microsoft has affected around 50 news contractors in the US and around 27 in the UK. One staff member who worked on the team said — “I spend all my time reading about how automation and AI is going to take all our jobs, and here I am – AI has taken my job.”

Microsoft launched MSN in 1995 and has been in the news business for more than 23 years and at the launch of Microsoft News, the company revealed that it had more than 800 editors working from 50 locations around the world.",Microsoft fires MSN journalists - replaces them with AI robots,https://techlog360.com/microsoft-fires-msn-journalists-replaces-them-with-ai-robots/
"[""Girish Shetti""]",2021-10-04,2021-10-04,2020-06-01,2021-10-04,https://techpluto.s3.amazonaws.com/wp-content/uploads/2019/07/18105954/Microsoft.jpg,2020-05-28,0,en,,www.techpluto.com,"[""Kate Perkins""]","1

Microsoft laying dozens of Journalists & COVID 19 is not the reason for this layoff

Tech giant Microsoft is laying off several journalists at its Microsoft News and MSN Organization (Microsoft owns MSN.com). However, this lay off has nothing to do with ongoing coronavirus crises, which is proving to be a major drag for the global economy. Microsoft has taken this decision in order to give a major push to artificial intelligence (AI) in picking news and content on its various platforms, which has now led to job cuts at MSN and other Microsoft subsidiaries that deal with news and content.

2

Unlike Twitter, Facebook won’t have any faceoff with Trump

While Twitter is currently caught in a spat with White House that is now looking like a never-ending saga, Facebook has chosen to play it safe. In his latest post posted on Facebook on late Friday, Mark Zuckerberg has given more than enough hints that his company won’t take any action against Trump’s FB posts on Minneapolis violence. Zuckerberg’s stand on the issue has already started earning him and his company barrage of bad names from the critics. Critics, on the other hand, have lauded Twitter for taking bold stance against the U.S President.

3

Youtube responds to Minneapolis violence with $1 Mn donation

Police authorities have largely been blamed for Minneapolis violence, which has again brought the raging issue of racism at the forefront. In order to control discriminatory practices by police, Youtube has announced that it will donate $1 Mn to the Centre for Police Equity, which is a non-profit organization aimed at reforming and discouraging discriminatory practices in the police department. The step is likely to do lot of good for the PR and image of Youtube, since in the past it has faced constant criticism for promoting racist contents on its platform.

4

SpaceX takes NASA Astronauts to International space station; ushers a new era for spaceflight

Almost 24 hours after lifting off from Florida, SpaceX successfully delivered and installed two NASA astronauts in International Space Station (ISS). The two NASA astronauts – Bob Behnken & Doug Harley – reached the ISS at approximately 1.25 P.M EDT. With SpaceX now becoming one of the first private space companies to launch astronauts into orbit & delivering it to ISS, the success of this operation has undeniably ushered a new era for spaceflight.

5

Zoom planning to launch strong encryption for paying customers

Plagued by privacy concerns, Zoom is planning to roll out strong encryption for its paying customers, the company official said on Friday. However, Zoom’s free customers have been left out from the plan and they will most likely not enjoy strong encryption.",Microsoft Fires Journalists and replaces them with AI – Top Trending News,https://www.techpluto.com/microsoft-fires-journalists-and-replaces-them-with-ai-top-trending-news/
"[""Jim Waterson""]",2021-10-04,2021-10-04,2020-06-09,2021-10-04,https://i.guim.co.uk/img/media/b9b5212dc70421c33f58ad00d1b0883f6e43fb91/0_0_2560_1536/master/2560.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctYWdlLTIwMjAucG5n&enable=upscale&s=9c12a9bade7551138417faef41a92b80,2020-06-06,0,en,,www.theguardian.com,"[""Kate Perkins""]","Microsoft’s decision to replace human journalists with robots has backfired, after the tech company’s artificial intelligence software illustrated a news story about racism with a photo of the wrong mixed-race member of the band Little Mix.

A week after the Guardian revealed plans to fire the human editors who run MSN.com and replace them with Microsoft’s artificial intelligence code, an early rollout of the software resulted in a story about the singer Jade Thirlwall’s personal reflections on racism being illustrated with a picture of her fellow band member Leigh-Anne Pinnock.

Thirlwall, who attended a recent Black Lives Matter protest in London, criticised MSN on Friday, saying she was sick of “ignorant” media making such mistakes. She posted on Instagram: “@MSN If you’re going to copy and paste articles from other accurate media outlets, you might want to make sure you’re using an image of the correct mixed race member of the group.”

“This shit happens to @leighannepinnock and I ALL THE TIME that it’s become a running joke,” she said. “It offends me that you couldn’t differentiate the two women of colour out of four members of a group … DO BETTER!”

What Thirlwall could not have known, according to sources at the company, is that the image was selected by Microsoft’s artificial intelligence software, which is already responsible for editing parts of the news site, which attracts hundreds of millions of readers worldwide.

An Instagram Stories post by Jade Thirlwall criticising the MSN news service. Photograph: Jade Thirlwall

Microsoft does not carry out original reporting but employs human editors to select, edit and repurpose articles from news outlets, including the Guardian. Articles are then hosted on Microsoft’s website and the tech company shares advertising revenue with the original publishers. At the end of last month, Microsoft decided to fire hundreds of journalists in the middle of a pandemic and fully replace them with the artificial intelligence software.

Asked why Microsoft was deploying software that cannot tell mixed-race individuals apart, whether apparent racist bias could seep into deployments of the company’s artificial intelligence software by leading corporations, and whether the company would reconsider plans to replace the human editors with robots, a spokesman for the tech company said: “As soon as we became aware of this issue, we immediately took action to resolve it and have replaced the incorrect image.”

In advance of the publication of this article, staff at MSN were told to expect a negative article in the Guardian about alleged racist bias in the artificial intelligence software that will soon take their jobs.

Because they are unable to stop the new robot editor selecting stories from external news sites such as the Guardian, the remaining human staff have been told to stay alert and delete a version of this article if the robot decides it is of interest and automatically publishes it on MSN.com. They have also been warned that even if they delete it, the robot editor may overrule them and attempt to publish it again.

Staff have already had to delete coverage criticising MSN for running the story about Little Mix with the wrong image after the AI software decided stories about the incident would interest MSN readers.

One staff member said Microsoft was deeply concerned about reputational damage to its AI product: “With all the anti-racism protests at the moment, now is not the time to be making mistakes.”",Microsoft's robot editor confuses mixed-race Little Mix singers,https://www.theguardian.com/technology/2020/jun/09/microsofts-robot-journalist-confused-by-mixed-race-little-mix-singers
"[""Shivali Best""]",2021-10-04,2021-10-04,2020-06-09,2021-10-04,https://i2-prod.mirror.co.uk/tech/article22162732.ece/ALTERNATES/s1200/0_JS212045133.jpg,2020-06-06,0,en,,www.mirror.co.uk,"[""Kate Perkins""]","Microsoft's AI editor posted a story about Ms Thirlwall’s experience with racism on the search site MSN.com, but mistakenly used a photo of her bandmate, Ms Pinnock

They’re two of the biggest pop stars in the world, but Little Mix’s Jade Thirlwall and Leigh-Anne Pinnock have been involved in a bizarre mix-up this week, thanks to Microsoft’s robot editor.

The AI editor posted a story about Ms Thirlwall’s experience with racism on the search site MSN.com, but mistakenly used a photo of her bandmate, Ms Pinnock.

Ms Thirlwall flagged the error on her Instagram Story, writing: “@MSN If you’re going to copy and paste articles from other accurate media outlets, you might want to make sure you’re using an image of the correct mixed race member of the group.”

She added that this isn’t the first time that the two singers have been confused for one another.

She added: “This s*** happens to @leighannepinnock and I ALL THE TIME that it’s become a running joke ... It offends me that you couldn’t differentiate the two women of colour out of four members of a group … DO BETTER!”

According to The Guardian, who first reported the error, the mistake was made by Microsoft’s robot editor, which MSN has been using over the last month.

The reason for the mistake remains unclear.

A spokesperson for Microsoft said: “Whilst removing bias and improving accuracy remain an area of focus for AI research, this mistake was not a result of these issues.

""In testing a new feature to select an alternate image, rather than defaulting to the first photo, a different image on the page of the original article was paired with the headline of the piece. This made it erroneously appear as though the headline was a caption for the picture.

""As soon as we became aware of this issue, we immediately took action to resolve it, replaced the incorrect image and turned off this new feature.”

Last month, Microsoft replaced almost 80 journalists and editorial workers with artificial intelligence systems.

Microsoft said: “Like all companies, we evaluate our business on a regular basis. This can result in increased investment in some places and, from time to time, re-deployment in others. These decisions are not the result of the current pandemic.”

However, the Little Mix-up suggests that Microsoft’s AI may not be as foolproof as the tech giant hopes…",Little Mix's Jade Thirlwall slams Microsoft after its AI confuses her with bandmate,https://www.mirror.co.uk/tech/little-mixs-jade-thirlwall-slams-22162740
